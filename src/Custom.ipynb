{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd428d9",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "##imports\n",
    "import time\n",
    "import numpy as np\n",
    "import sys\n",
    "import random\n",
    "import os\n",
    "import warnings\n",
    "import datetime\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "import torch.utils.data.distributed\n",
    "from easydict import EasyDict as edict\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append('./')\n",
    "\n",
    "from data_process.ttnet_dataloader import create_train_val_dataloader, create_test_dataloader\n",
    "from models.model_utils import create_model, load_pretrained_model, make_data_parallel, resume_model, get_num_parameters\n",
    "from models.model_utils import freeze_model\n",
    "from utils.train_utils import create_optimizer, create_lr_scheduler, get_saved_state, save_checkpoint\n",
    "from utils.train_utils import reduce_tensor, to_python_float\n",
    "from utils.misc import AverageMeter, ProgressMeter\n",
    "from utils.logger import Logger\n",
    "#from config.config import parse_configs\n",
    "from utils.misc import make_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07091e3f",
   "metadata": {},
   "source": [
    "###### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b81b98",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def parse_configs():\n",
    "    parser = argparse.ArgumentParser(description='TTNet Implementation')\n",
    "    parser.add_argument('--seed', type=int, default=2020, help='re-produce the results with seed random')\n",
    "    parser.add_argument('--saved_fn', type=str, default='ttnet', metavar='FN', help='The name using for saving logs, models,...')\n",
    "    ####################################################################\n",
    "    ##############     Model configs            ###################\n",
    "    ####################################################################\n",
    "    parser.add_argument('-a', '--arch', type=str, default='ttnet', metavar='ARCH', help='The name of the model architecture')\n",
    "    parser.add_argument('--dropout_p', type=float, default=0.5, metavar='P', help='The dropout probability of the model')\n",
    "    parser.add_argument('--multitask_learning', action='store_true', help='If true, the weights of different losses will be learnt (train).'\n",
    "                             'If false, a regular sum of different losses will be applied')\n",
    "    parser.add_argument('--no_local', action='store_true', help='If true, no local stage for ball detection.')\n",
    "    parser.add_argument('--no_event', action='store_true', help='If true, no event spotting detection.')\n",
    "    parser.add_argument('--no_seg', action='store_true', help='If true, no segmentation module.')\n",
    "    parser.add_argument('--pretrained_path', type=str, default=None, metavar='PATH', help='the path of the pretrained checkpoint')\n",
    "    parser.add_argument('--overwrite_global_2_local', action='store_true', help='If true, the weights of the local stage will be overwritten by the global stage.')\n",
    "\n",
    "    ####################################################################\n",
    "    ##############     Dataloader and Running configs            #######\n",
    "    ####################################################################\n",
    "    parser.add_argument('--working-dir', type=str, default='../../', metavar='PATH', help='the ROOT working directory')\n",
    "    parser.add_argument('--no-val', action='store_true', help='If true, use all data for training, no validation set')\n",
    "    parser.add_argument('--no-test', action='store_true', help='If true, dont evaluate the model on the test set')\n",
    "    parser.add_argument('--val-size', type=float, default=0.2, help='The size of validation set')\n",
    "    parser.add_argument('--smooth-labelling', action='store_true', help='If true, smoothly make the labels of event spotting')\n",
    "    parser.add_argument('--num_samples', type=int, default=None, help='Take a subset of the dataset to run and debug')\n",
    "    parser.add_argument('--num_workers', type=int, default=4, help='Number of threads for loading data')\n",
    "    parser.add_argument('--batch_size', type=int, default=8, help='mini-batch size (default: 8), this is the total'\n",
    "                             'batch size of all GPUs on the current node when using'\n",
    "                             'Data Parallel or Distributed Data Parallel')\n",
    "    parser.add_argument('--print_freq', type=int, default=50, metavar='N', help='print frequency (default: 50)')\n",
    "    parser.add_argument('--checkpoint_freq', type=int, default=2, metavar='N', help='frequency of saving checkpoints (default: 2)')\n",
    "    parser.add_argument('--sigma', type=float, default=1., metavar='SIGMA', help='standard deviation of the 1D Gaussian for the ball position target')\n",
    "    parser.add_argument('--thresh_ball_pos_mask', type=float, default=0.05, metavar='THRESH', help='the lower thresh for the 1D Gaussian of the ball position target')\n",
    "    ####################################################################\n",
    "    ##############     Training strategy            ###################\n",
    "    ####################################################################\n",
    "\n",
    "    parser.add_argument('--start_epoch', type=int, default=1, metavar='N', help='the starting epoch')\n",
    "    parser.add_argument('--num_epochs', type=int, default=30, metavar='N', help='number of total epochs to run')\n",
    "    parser.add_argument('--lr', type=float, default=1e-3, metavar='LR', help='initial learning rate')\n",
    "    parser.add_argument('--minimum_lr', type=float, default=1e-7, metavar='MIN_LR', help='minimum learning rate during training')\n",
    "    parser.add_argument('--momentum', type=float, default=0.9, metavar='M', help='momentum')\n",
    "    parser.add_argument('-wd', '--weight_decay', type=float, default=0., metavar='WD', help='weight decay (default: 1e-6)')\n",
    "    parser.add_argument('--optimizer_type', type=str, default='adam', metavar='OPTIMIZER', help='the type of optimizer, it can be sgd or adam')\n",
    "    parser.add_argument('--lr_type', type=str, default='plateau', metavar='SCHEDULER', help='the type of the learning rate scheduler (steplr or ReduceonPlateau)')\n",
    "    parser.add_argument('--lr_factor', type=float, default=0.5, metavar='FACTOR', help='reduce the learning rate with this factor')\n",
    "    parser.add_argument('--lr_step_size', type=int, default=5, metavar='STEP_SIZE', help='step_size of the learning rate when using steplr scheduler')\n",
    "    parser.add_argument('--lr_patience', type=int, default=3, metavar='N', help='patience of the learning rate when using ReduceoPlateau scheduler')\n",
    "    parser.add_argument('--earlystop_patience', type=int, default=None, metavar='N', help='Early stopping the training process if performance is not improved within this value')\n",
    "    parser.add_argument('--freeze_global', action='store_true', help='If true, no update/train weights for the global stage of ball detection.')\n",
    "    parser.add_argument('--freeze_local', action='store_true', help='If true, no update/train weights for the local stage of ball detection.')\n",
    "    parser.add_argument('--freeze_event', action='store_true', help='If true, no update/train weights for the event module.')\n",
    "    parser.add_argument('--freeze_seg', action='store_true', help='If true, no update/train weights for the segmentation module.')\n",
    "\n",
    "    ####################################################################\n",
    "    ##############     Loss weight            ###################\n",
    "    ####################################################################\n",
    "    parser.add_argument('--bce_weight', type=float, default=0.5, help='The weight of BCE loss in segmentation module, the dice_loss weight = 1- bce_weight')\n",
    "    parser.add_argument('--global_weight', type=float, default=1., help='The weight of loss of the global stage for ball detection')\n",
    "    parser.add_argument('--local_weight', type=float, default=1., help='The weight of loss of the local stage for ball detection')\n",
    "    parser.add_argument('--event_weight', type=float, default=1., help='The weight of loss of the event spotting module')\n",
    "    parser.add_argument('--seg_weight', type=float, default=1., help='The weight of BCE loss in segmentation module')\n",
    "\n",
    "    ####################################################################\n",
    "    ##############     Distributed Data Parallel            ############\n",
    "    ####################################################################\n",
    "    parser.add_argument('--world-size', default=-1, type=int, metavar='N', help='number of nodes for distributed training')\n",
    "    parser.add_argument('--rank', default=-1, type=int, metavar='N', help='node rank for distributed training')\n",
    "    parser.add_argument('--dist-url', default='tcp://127.0.0.1:29500', type=str, help='url used to set up distributed training')\n",
    "    parser.add_argument('--dist-backend', default='nccl', type=str, help='distributed backend')\n",
    "    parser.add_argument('--gpu_idx', default=0, type=int, help='GPU index to use.')\n",
    "    parser.add_argument('--no_cuda', action='store_true', help='If true, cuda is not used.')\n",
    "    parser.add_argument('--multiprocessing-distributed', action='store_true', help='Use multi-processing distributed training to launch '\n",
    "                             'N processes per node, which has N GPUs. This is the '\n",
    "                             'fastest way to use PyTorch for either single node or '\n",
    "                             'multi node data parallel training')\n",
    "    ####################################################################\n",
    "    ##############     Evaluation configurations     ###################\n",
    "    ####################################################################\n",
    "    parser.add_argument('--evaluate', action='store_true', help='only evaluate the model, not training')\n",
    "    parser.add_argument('--resume_path', type=str, default=None, metavar='PATH', help='the path of the resumed checkpoint')\n",
    "    parser.add_argument('--use_best_checkpoint', action='store_true', help='If true, choose the best model on val set, otherwise choose the last model')\n",
    "    parser.add_argument('--seg_thresh', type=float, default=0.5, help='threshold of the segmentation output')\n",
    "    parser.add_argument('--event_thresh', type=float, default=0.5, help='threshold of the event spotting output')\n",
    "    parser.add_argument('--save_test_output', action='store_true', help='If true, the image of testing phase will be saved')\n",
    "\n",
    "    ####################################################################\n",
    "    ##############     Demonstration configurations     ###################\n",
    "    ####################################################################\n",
    "    parser.add_argument('--video_path', type=str, default=None, metavar='PATH', help='the path of the video that needs to demo')\n",
    "    parser.add_argument('--output_format', type=str, default='text', metavar='PATH', help='the type of the demo output')\n",
    "    parser.add_argument('--show_image', action='store_true', help='If true, show the image during demostration')\n",
    "    parser.add_argument('--save_demo_output', action='store_true', help='If true, the image of demonstration phase will be saved')\n",
    "\n",
    "    configs = edict(vars(parser.parse_args([])))\n",
    "\n",
    "    return configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253d4e7c",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def config_process(configs):\n",
    "    ####################################################################\n",
    "    ############## Hardware configurations ############################\n",
    "    ####################################################################\n",
    "    configs.device = torch.device('cpu' if configs.no_cuda else 'cuda')\n",
    "    configs.ngpus_per_node = torch.cuda.device_count()\n",
    "\n",
    "    configs.pin_memory = True\n",
    "\n",
    "    ####################################################################\n",
    "    ##############     Data configs            ###################\n",
    "    ####################################################################\n",
    "    configs.dataset_dir = os.path.join(configs.working_dir, 'dataset')\n",
    "    configs.train_game_list = ['game_1']#, 'game_2', 'game_3', 'game_4', 'game_5']\n",
    "    configs.test_game_list = ['test_1']#, 'test_2', 'test_3', 'test_4', 'test_5', 'test_6', 'test_7']\n",
    "    configs.events_dict = {\n",
    "        'bounce': 0,\n",
    "        'net': 1,\n",
    "        'empty_event': 2\n",
    "    }\n",
    "    configs.events_weights_loss_dict = {\n",
    "        'bounce': 1.,\n",
    "        'net': 3.,\n",
    "    }\n",
    "    configs.events_weights_loss = (configs.events_weights_loss_dict['bounce'], configs.events_weights_loss_dict['net'])\n",
    "    configs.num_events = len(configs.events_weights_loss_dict)  # Just \"bounce\" and \"net hits\"\n",
    "    configs.num_frames_sequence = 9\n",
    "\n",
    "    configs.org_size = (1920, 1080)\n",
    "    configs.input_size = (320, 128)\n",
    "\n",
    "    configs.tasks = ['global', 'local', 'event', 'seg']\n",
    "    if configs.no_local:\n",
    "        if 'local' in configs.tasks:\n",
    "            configs.tasks.remove('local')\n",
    "        if 'event' in configs.tasks:\n",
    "            configs.tasks.remove('event')\n",
    "    if configs.no_event:\n",
    "        if 'event' in configs.tasks:\n",
    "            configs.tasks.remove('event')\n",
    "    if configs.no_seg:\n",
    "        if 'seg' in configs.tasks:\n",
    "            configs.tasks.remove('seg')\n",
    "\n",
    "    # Compose loss weight for tasks, normalize the weights later\n",
    "    loss_weight_dict = {\n",
    "        'global': configs.global_weight,\n",
    "        'local': configs.local_weight,\n",
    "        'event': configs.event_weight,\n",
    "        'seg': configs.seg_weight\n",
    "    }\n",
    "    configs.tasks_loss_weight = [loss_weight_dict[task] for task in configs.tasks]\n",
    "\n",
    "    configs.freeze_modules_list = []\n",
    "    if configs.freeze_global:\n",
    "        configs.freeze_modules_list.append('ball_global_stage')\n",
    "    if configs.freeze_local:\n",
    "        configs.freeze_modules_list.append('ball_local_stage')\n",
    "    if configs.freeze_event:\n",
    "        configs.freeze_modules_list.append('events_spotting')\n",
    "    if configs.freeze_seg:\n",
    "        configs.freeze_modules_list.append('segmentation')\n",
    "\n",
    "    ####################################################################\n",
    "    ############## logs, Checkpoints, and results dir ########################\n",
    "    ####################################################################\n",
    "    configs.checkpoints_dir = os.path.join(configs.working_dir, 'checkpoints', configs.saved_fn)\n",
    "    configs.logs_dir = os.path.join(configs.working_dir, 'logs', configs.saved_fn)\n",
    "    configs.use_best_checkpoint = True\n",
    "\n",
    "    if configs.use_best_checkpoint:\n",
    "        configs.saved_weight_name = os.path.join(configs.checkpoints_dir, '{}_best.pth'.format(configs.saved_fn))\n",
    "    else:\n",
    "        configs.saved_weight_name = os.path.join(configs.checkpoints_dir, '{}.pth'.format(configs.saved_fn))\n",
    "\n",
    "    configs.results_dir = os.path.join(configs.working_dir, 'results')\n",
    "\n",
    "    make_folder(configs.checkpoints_dir)\n",
    "    make_folder(configs.logs_dir)\n",
    "    make_folder(configs.results_dir)\n",
    "\n",
    "    if configs.save_test_output:\n",
    "        configs.saved_dir = os.path.join(configs.results_dir, configs.saved_fn)\n",
    "        make_folder(configs.saved_dir)\n",
    "\n",
    "    if configs.save_demo_output:\n",
    "        configs.save_demo_dir = os.path.join(configs.results_dir, 'demo', configs.saved_fn)\n",
    "        make_folder(configs.save_demo_dir)\n",
    "\n",
    "    return configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a633b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "PHASE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff491d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = parse_configs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d656cf9",
   "metadata": {
    "code_folding": [
     19,
     42
    ]
   },
   "outputs": [],
   "source": [
    "if PHASE == 1: \n",
    "    #Phase 1\n",
    "    configs.working_dir      = \"../\"\n",
    "    configs.saved_fn         = 'ttnet_1st_phase_opentt_custom'\n",
    "    configs.no_val           = True\n",
    "    configs.batch_size       = 24\n",
    "    configs.num_workers      = 8\n",
    "    configs.lr               = 0.001 \n",
    "    configs.lr_type          = 'step_lr' \n",
    "    configs.lr_step_size     = 10 \n",
    "    configs.lr_factor        = 0.1\n",
    "    configs.gpu_idx          = 0 \n",
    "    configs.global_weight    = 5. \n",
    "    configs.seg_weight       = 1.\n",
    "    configs.no_event         = True\n",
    "    configs.no_local         = True\n",
    "    configs.print_freq       = 500\n",
    "    configs.smooth_labelling = True\n",
    "\n",
    "if PHASE == 2:\n",
    "    #Phase 2\n",
    "    configs.working_dir                = \"../\"\n",
    "    configs.batch_size                 = 24\n",
    "    configs.num_workers                = 8\n",
    "    configs.saved_fn                   = 'ttnet_2nd_phase_wtt' \n",
    "    configs.no_val                     = True  \n",
    "    configs.lr                         = 0.001 \n",
    "    configs.lr_type                    = 'step_lr' \n",
    "    configs.lr_step_size               =  10 \n",
    "    configs.lr_factor                  = 0.1 \n",
    "    configs.gpu_idx                    = 0 \n",
    "    configs.global_weight              = 0. \n",
    "    configs.event_weight               = 2. \n",
    "    configs.local_weight               = 1. \n",
    "    configs.pretrained_path            = \"../../checkpoints/ttnet/ttnet_1st_phase_wtt_epoch_30.pth\"\n",
    "    configs.overwrite_global_2_local   = True\n",
    "    configs.freeze_global              = True\n",
    "    configs.smooth_labelling           = True\n",
    "    configs.freeze_seg                 = True\n",
    "    configs.sigma                      =  1.0\n",
    "    configs.print_freq                 =  500   \n",
    "    \n",
    "if PHASE == 3:\n",
    "    #Phase 3\n",
    "    configs.working_dir       = \"../\"\n",
    "    configs.saved_fn          = 'ttnet_3nd_phase' \n",
    "    configs.batch_size        = 24\n",
    "    configs.num_workers       = 8\n",
    "    \n",
    "    configs.no_val            = True  \n",
    "    configs.lr                = 0.0001 \n",
    "    configs.lr_type           = 'step_lr' \n",
    "    configs.lr_step_size      =  10 \n",
    "    configs.lr_factor         = 0.2 \n",
    "    configs.gpu_idx           = 0 \n",
    "    configs.global_weight     = 1. \n",
    "    configs.event_weight      = 1. \n",
    "    configs.local_weight      = 1. \n",
    "    configs.pretrained_path   = \"../../checkpoints/ttnet/ttnet_2nd_phase_epoch_30.pth\"\n",
    "    configs.smooth_labelling  = True\n",
    "#     configs.sigma             = 1.0\n",
    "    configs.print_freq        = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ad6140",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = config_process(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9370dba",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "configs.freeze_modules_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5ea5a4",
   "metadata": {
    "code_folding": [
     0,
     6
    ]
   },
   "outputs": [],
   "source": [
    "if configs.seed is not None:\n",
    "    random.seed(configs.seed)\n",
    "    np.random.seed(configs.seed)\n",
    "    torch.manual_seed(configs.seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "if configs.gpu_idx is not None:\n",
    "    warnings.warn('You have chosen a specific GPU. This will completely '\n",
    "                      'disable data parallelism.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8834d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs.distributed = configs.world_size > 1 or configs.multiprocessing_distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c5ecf3",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "if configs.gpu_idx is not None:\n",
    "    print(\"Use GPU: {} for training\".format(configs.gpu_idx))\n",
    "    configs.device = torch.device('cuda:{}'.format(configs.gpu_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c1d151",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs.is_master_node = (not configs.distributed) or (configs.distributed and (configs.rank % configs.ngpus_per_node == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4f8124",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs.is_master_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abfeb8d",
   "metadata": {
    "code_folding": [
     0,
     5
    ]
   },
   "outputs": [],
   "source": [
    "if configs.is_master_node:\n",
    "    logger = Logger(configs.logs_dir, configs.saved_fn)\n",
    "    logger.info('>>> Created a new logger')\n",
    "    logger.info('>>> configs: {}'.format(configs))\n",
    "    tb_writer = SummaryWriter(log_dir=os.path.join(configs.logs_dir, 'tensorboard'))\n",
    "else:\n",
    "    logger = None\n",
    "    tb_writer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f22e10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8571ad20",
   "metadata": {},
   "source": [
    "##### Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8224f73e",
   "metadata": {
    "code_folding": [
     0,
     11,
     21,
     40,
     77,
     103,
     134
    ]
   },
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.maxpool(self.relu(self.batchnorm(self.conv(x))))\n",
    "        return x\n",
    "class ConvBlock_without_Pooling(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvBlock_without_Pooling, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.batchnorm(self.conv(x)))\n",
    "        return x\n",
    "class DeconvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DeconvBlock, self).__init__()\n",
    "        middle_channels = int(in_channels / 4)\n",
    "        self.conv1 = nn.Conv2d(in_channels, middle_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(middle_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.batchnorm_tconv = nn.BatchNorm2d(middle_channels)\n",
    "        self.tconv = nn.ConvTranspose2d(middle_channels, middle_channels, kernel_size=3, stride=2, padding=1,\n",
    "                                        output_padding=1)\n",
    "        self.conv2 = nn.Conv2d(middle_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.batchnorm1(self.conv1(x)))\n",
    "        x = self.relu(self.batchnorm_tconv(self.tconv(x)))\n",
    "        x = self.relu(self.batchnorm2(self.conv2(x)))\n",
    "\n",
    "        return x\n",
    "class BallDetection(nn.Module):\n",
    "    def __init__(self, num_frames_sequence, dropout_p):\n",
    "        super(BallDetection, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_frames_sequence * 3, 64, kernel_size=1, stride=1, padding=0)\n",
    "        self.batchnorm = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.convblock1 = ConvBlock(in_channels=64, out_channels=64)\n",
    "        self.convblock2 = ConvBlock(in_channels=64, out_channels=64)\n",
    "        self.dropout2d = nn.Dropout2d(p=dropout_p)\n",
    "        self.convblock3 = ConvBlock(in_channels=64, out_channels=128)\n",
    "        self.convblock4 = ConvBlock(in_channels=128, out_channels=128)\n",
    "        self.convblock5 = ConvBlock(in_channels=128, out_channels=256)\n",
    "        self.convblock6 = ConvBlock(in_channels=256, out_channels=256)\n",
    "        self.fc1 = nn.Linear(in_features=2560, out_features=1792)\n",
    "        self.fc2 = nn.Linear(in_features=1792, out_features=896)\n",
    "        self.fc3 = nn.Linear(in_features=896, out_features=448)\n",
    "        self.dropout1d = nn.Dropout(p=dropout_p)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.batchnorm(self.conv1(x)))\n",
    "        out_block2 = self.convblock2(self.convblock1(x))\n",
    "        x = self.dropout2d(out_block2)\n",
    "        out_block3 = self.convblock3(x)\n",
    "        out_block4 = self.convblock4(out_block3)\n",
    "        x = self.dropout2d(out_block4)\n",
    "        out_block5 = self.convblock5(out_block4)\n",
    "        features = self.convblock6(out_block5)\n",
    "\n",
    "        x = self.dropout2d(features)\n",
    "        x = x.contiguous().view(x.size(0), -1)\n",
    "\n",
    "        x = self.dropout1d(self.relu(self.fc1(x)))\n",
    "        x = self.dropout1d(self.relu(self.fc2(x)))\n",
    "        out = self.sigmoid(self.fc3(x))\n",
    "\n",
    "        return out, features, out_block2, out_block3, out_block4, out_block5\n",
    "class EventsSpotting(nn.Module):\n",
    "    def __init__(self, dropout_p):\n",
    "        super(EventsSpotting, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(512, 64, kernel_size=1, stride=1, padding=0)\n",
    "        self.batchnorm = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout2d = nn.Dropout2d(p=dropout_p)\n",
    "        self.convblock = ConvBlock_without_Pooling(in_channels=64, out_channels=64)\n",
    "        self.fc1 = nn.Linear(in_features=640, out_features=512)\n",
    "        self.fc2 = nn.Linear(in_features=512, out_features=2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, global_features, local_features):\n",
    "        input_eventspotting = torch.cat((global_features, local_features), dim=1)\n",
    "        x = self.relu(self.batchnorm(self.conv1(input_eventspotting)))\n",
    "        x = self.dropout2d(x)\n",
    "        x = self.convblock(x)\n",
    "        x = self.dropout2d(x)\n",
    "        x = self.convblock(x)\n",
    "        x = self.dropout2d(x)\n",
    "\n",
    "        x = x.contiguous().view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        out = self.sigmoid(self.fc2(x))\n",
    "\n",
    "        return out\n",
    "class Segmentation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Segmentation, self).__init__()\n",
    "        self.deconvblock5 = DeconvBlock(in_channels=256, out_channels=128)\n",
    "        self.deconvblock4 = DeconvBlock(in_channels=128, out_channels=128)\n",
    "        self.deconvblock3 = DeconvBlock(in_channels=128, out_channels=64)\n",
    "        self.deconvblock2 = DeconvBlock(in_channels=64, out_channels=64)\n",
    "        self.tconv = nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=3, stride=2, padding=0,\n",
    "                                        output_padding=0)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv1 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=0)\n",
    "        self.conv2 = nn.Conv2d(32, 3, kernel_size=2, stride=1, padding=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, out_block2, out_block3, out_block4, out_block5):\n",
    "        x = self.deconvblock5(out_block5)\n",
    "        x = x + out_block4\n",
    "        x = self.deconvblock4(x)\n",
    "        x = x + out_block3\n",
    "        x = self.deconvblock3(x)\n",
    "\n",
    "        x = x + out_block2\n",
    "        x = self.deconvblock2(x)\n",
    "\n",
    "        x = self.relu(self.tconv(x))\n",
    "\n",
    "        x = self.relu(self.conv1(x))\n",
    "\n",
    "        out = self.sigmoid(self.conv2(x))\n",
    "\n",
    "        return out\n",
    "class TTNet(nn.Module):\n",
    "    def __init__(self, dropout_p, tasks, input_size, thresh_ball_pos_mask, num_frames_sequence,\n",
    "                 mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n",
    "        super(TTNet, self).__init__()\n",
    "        self.tasks = tasks\n",
    "        self.ball_local_stage, self.events_spotting, self.segmentation = None, None, None\n",
    "        self.ball_global_stage = BallDetection(num_frames_sequence=num_frames_sequence, dropout_p=dropout_p)\n",
    "        if 'local' in tasks:\n",
    "            self.ball_local_stage = BallDetection(num_frames_sequence=num_frames_sequence, dropout_p=dropout_p)\n",
    "        if 'event' in tasks:\n",
    "            self.events_spotting = EventsSpotting(dropout_p=dropout_p)\n",
    "        if 'seg' in tasks:\n",
    "            self.segmentation = Segmentation()\n",
    "        self.w_resize = input_size[0]\n",
    "        self.h_resize = input_size[1]\n",
    "        self.thresh_ball_pos_mask = thresh_ball_pos_mask\n",
    "        self.mean = torch.repeat_interleave(torch.tensor(mean).view(1, 3, 1, 1), repeats=9, dim=1)\n",
    "        self.std = torch.repeat_interleave(torch.tensor(std).view(1, 3, 1, 1), repeats=9, dim=1)\n",
    "\n",
    "    def forward(self, resize_batch_input, org_ball_pos_xy):\n",
    "        \"\"\"Forward propagation\n",
    "        :param resize_batch_input: (batch_size, 27, 128, 320)\n",
    "        :param org_ball_pos_xy: (batch_size, 2) --> Use it to get ground-truth for the local stage\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        pred_ball_local, pred_events, pred_seg, local_ball_pos_xy = None, None, None, None\n",
    "\n",
    "        # Normalize the input before compute forward propagation\n",
    "        pred_ball_global, global_features, out_block2, out_block3, out_block4, out_block5 = self.ball_global_stage(\n",
    "            self.__normalize__(resize_batch_input))\n",
    "        if self.ball_local_stage is not None:\n",
    "            # Based on the prediction of the global stage, crop the original images\n",
    "            input_ball_local, cropped_params = self.__crop_original_batch__(resize_batch_input, pred_ball_global)\n",
    "            # Get the ground truth of the ball for the local stage\n",
    "            local_ball_pos_xy = self.__get_groundtruth_local_ball_pos__(org_ball_pos_xy, cropped_params)\n",
    "            # Normalize the input before compute forward propagation\n",
    "            pred_ball_local, local_features, *_ = self.ball_local_stage(self.__normalize__(input_ball_local))\n",
    "            # Only consider the events spotting if the model has the local stage for ball detection\n",
    "            if self.events_spotting is not None:\n",
    "                pred_events = self.events_spotting(global_features, local_features)\n",
    "        if self.segmentation is not None:\n",
    "            pred_seg = self.segmentation(out_block2, out_block3, out_block4, out_block5)\n",
    "\n",
    "        return pred_ball_global, pred_ball_local, pred_events, pred_seg, local_ball_pos_xy\n",
    "\n",
    "    def run_demo(self, resize_batch_input):\n",
    "        \"\"\"Only for full 4 stages/modules in TTNet\"\"\"\n",
    "\n",
    "        # Normalize the input before compute forward propagation\n",
    "        pred_ball_global, global_features, out_block2, out_block3, out_block4, out_block5 = self.ball_global_stage(\n",
    "            self.__normalize__(resize_batch_input))\n",
    "        input_ball_local, cropped_params = self.__crop_original_batch__(resize_batch_input, pred_ball_global)\n",
    "        # Normalize the input before compute forward propagation\n",
    "        pred_ball_local, local_features, *_ = self.ball_local_stage(self.__normalize__(input_ball_local))\n",
    "        pred_events = self.events_spotting(global_features, local_features)\n",
    "        pred_seg = self.segmentation(out_block2, out_block3, out_block4, out_block5)\n",
    "\n",
    "        return pred_ball_global, pred_ball_local, pred_events, pred_seg\n",
    "\n",
    "    def __normalize__(self, x):\n",
    "        if not self.mean.is_cuda:\n",
    "            self.mean = self.mean.cuda()\n",
    "            self.std = self.std.cuda()\n",
    "\n",
    "        return (x / 255. - self.mean) / self.std\n",
    "\n",
    "    def __get_groundtruth_local_ball_pos__(self, org_ball_pos_xy, cropped_params):\n",
    "        local_ball_pos_xy = torch.zeros_like(org_ball_pos_xy)  # no grad for torch.zeros_like output\n",
    "\n",
    "        for idx, params in enumerate(cropped_params):\n",
    "            is_ball_detected, x_min, x_max, y_min, y_max, x_pad, y_pad = params\n",
    "\n",
    "            if is_ball_detected:\n",
    "                # Get the local ball position based on the crop image informaion\n",
    "                local_ball_pos_xy[idx, 0] = max(org_ball_pos_xy[idx, 0] - x_min + x_pad, -1)\n",
    "                local_ball_pos_xy[idx, 1] = max(org_ball_pos_xy[idx, 1] - y_min + y_pad, -1)\n",
    "                # If the ball is outside of the cropped image --> set position to -1, -1 --> No ball\n",
    "                if (local_ball_pos_xy[idx, 0] >= self.w_resize) or (local_ball_pos_xy[idx, 1] >= self.h_resize) or (\n",
    "                        local_ball_pos_xy[idx, 0] < 0) or (local_ball_pos_xy[idx, 1] < 0):\n",
    "                    local_ball_pos_xy[idx, 0] = -1\n",
    "                    local_ball_pos_xy[idx, 1] = -1\n",
    "            else:\n",
    "                local_ball_pos_xy[idx, 0] = -1\n",
    "                local_ball_pos_xy[idx, 1] = -1\n",
    "        return local_ball_pos_xy\n",
    "\n",
    "    def __crop_original_batch__(self, resize_batch_input, pred_ball_global):\n",
    "        \"\"\"Get input of the local stage by cropping the original images based on the predicted ball position\n",
    "            of the global stage\n",
    "        :param resize_batch_input: (batch_size, 27, 128, 320)\n",
    "        :param pred_ball_global: (batch_size, 448)\n",
    "        :param org_ball_pos_xy: (batch_size, 2)\n",
    "        :return: input_ball_local (batch_size, 27, 128, 320)\n",
    "        \"\"\"\n",
    "        # Process input for local stage based on output of the global one\n",
    "\n",
    "        batch_size = resize_batch_input.size(0)\n",
    "        h_original, w_original = 1080, 1920\n",
    "        h_ratio = h_original / self.h_resize\n",
    "        w_ratio = w_original / self.w_resize\n",
    "        pred_ball_global_mask = pred_ball_global.clone().detach()\n",
    "        pred_ball_global_mask[pred_ball_global_mask < self.thresh_ball_pos_mask] = 0.\n",
    "\n",
    "        # Crop the original images\n",
    "        input_ball_local = torch.zeros_like(resize_batch_input)  # same shape with resize_batch_input, no grad\n",
    "        original_batch_input = F.interpolate(resize_batch_input, (h_original, w_original))  # On GPU\n",
    "        cropped_params = []\n",
    "        for idx in range(batch_size):\n",
    "            pred_ball_pos_x = pred_ball_global_mask[idx, :self.w_resize]\n",
    "            pred_ball_pos_y = pred_ball_global_mask[idx, self.w_resize:]\n",
    "            # If the ball is not detected, we crop the center of the images, set ball_poss to [-1, -1]\n",
    "            if (torch.sum(pred_ball_pos_x) == 0.) or (torch.sum(pred_ball_pos_y) == 0.):\n",
    "                # Assume the ball is in the center image\n",
    "                x_center = int(self.w_resize / 2)\n",
    "                y_center = int(self.h_resize / 2)\n",
    "                is_ball_detected = False\n",
    "            else:\n",
    "                x_center = torch.argmax(pred_ball_pos_x)  # Upper part\n",
    "                y_center = torch.argmax(pred_ball_pos_y)  # Lower part\n",
    "                is_ball_detected = True\n",
    "\n",
    "            # Adjust ball position to the original size\n",
    "            x_center = int(x_center * w_ratio)\n",
    "            y_center = int(y_center * h_ratio)\n",
    "\n",
    "            x_min, x_max, y_min, y_max = self.__get_crop_params__(x_center, y_center, self.w_resize, self.h_resize,\n",
    "                                                                  w_original, h_original)\n",
    "            # Put image to the center\n",
    "            h_crop = y_max - y_min\n",
    "            w_crop = x_max - x_min\n",
    "            x_pad = 0\n",
    "            y_pad = 0\n",
    "            if (h_crop != self.h_resize) or (w_crop != self.w_resize):\n",
    "                x_pad = int((self.w_resize - w_crop) / 2)\n",
    "                y_pad = int((self.h_resize - h_crop) / 2)\n",
    "                input_ball_local[idx, :, y_pad:(y_pad + h_crop), x_pad:(x_pad + w_crop)] = original_batch_input[idx, :,\n",
    "                                                                                           y_min:y_max, x_min: x_max]\n",
    "            else:\n",
    "                input_ball_local[idx, :, :, :] = original_batch_input[idx, :, y_min:y_max, x_min: x_max]\n",
    "            cropped_params.append([is_ball_detected, x_min, x_max, y_min, y_max, x_pad, y_pad])\n",
    "\n",
    "        return input_ball_local, cropped_params\n",
    "\n",
    "    def __get_crop_params__(self, x_center, y_center, w_resize, h_resize, w_original, h_original):\n",
    "        x_min = max(0, x_center - int(w_resize / 2))\n",
    "        y_min = max(0, y_center - int(h_resize / 2))\n",
    "\n",
    "        x_max = min(w_original, x_min + w_resize)\n",
    "        y_max = min(h_original, y_min + h_resize)\n",
    "\n",
    "        return x_min, x_max, y_min, y_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1957fca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs.tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e59c56",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def check_ttnet(configs):\n",
    "    ttnet = TTNet(dropout_p=0.5, tasks=configs.tasks, input_size=(320, 128), thresh_ball_pos_mask=0.01,\n",
    "                  num_frames_sequence=9).cuda()\n",
    "    resize_batch_input = torch.rand((1, 27, 128, 320)).cuda()\n",
    "    org_ball_pos_xy = torch.rand((1, 2)).cuda()\n",
    "    start = time.time()\n",
    "    \n",
    "#     pred_ball_global, pred_ball_local, pred_events, local_ball_pos_xy = ttnet(resize_batch_input, org_ball_pos_xy)\n",
    "    pred_ball_global, pred_ball_local, pred_events, pred_seg, local_ball_pos_xy = ttnet(resize_batch_input, org_ball_pos_xy)\n",
    "    \n",
    "#     print(\"DEBUG Unbalaced loss: \", pred_ball_global.shape, pred_ball_local.shape, pred_events.shape, local_ball_pos_xy.shape)    \n",
    "        \n",
    "    if pred_ball_global is not None:\n",
    "        print('pred_ball_global: {}'.format(pred_ball_global.size()))\n",
    "    if pred_ball_local is not None:\n",
    "        print('pred_ball_local: {}'.format(pred_ball_local.size()))\n",
    "    if pred_events is not None:\n",
    "        print('pred_events: {}'.format(pred_events.size()))\n",
    "    if pred_seg is not None:\n",
    "        print('Seg_events: {}'.format(pred_seg.size()))\n",
    "#     print('local_ball_pos_xy: {}'.format(local_ball_pos_xy.size()))\n",
    "    return ttnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f556855",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = check_ttnet(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11225df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e48a4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_data_parallel(model, configs)\n",
    "\n",
    "model = freeze_model(model, configs.freeze_modules_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39069d49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15aaba0c",
   "metadata": {},
   "source": [
    "#### Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83586bb7",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Ball_Detection_Loss(nn.Module):\n",
    "    def __init__(self, w, h, epsilon=1e-9):\n",
    "        super(Ball_Detection_Loss, self).__init__()\n",
    "        self.w = w\n",
    "        self.h = h\n",
    "        self.epsilon = epsilon\n",
    "        self.criterion = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "\n",
    "    def forward(self, pred_ball_position, target_ball_position):\n",
    "        x_pred = pred_ball_position[:, :self.w]\n",
    "        y_pred = pred_ball_position[:, self.w:]\n",
    "\n",
    "        x_target = target_ball_position[:, :self.w]\n",
    "        y_target = target_ball_position[:, self.w:]\n",
    "\n",
    "        loss_ball_x = - torch.mean(x_target * torch.log(x_pred + self.epsilon) + (1 - x_target) * torch.log(1 - x_pred + self.epsilon))\n",
    "        loss_ball_y = - torch.mean(y_target * torch.log(y_pred + self.epsilon) + (1 - y_target) * torch.log(1 - y_pred + self.epsilon))\n",
    "#         loss_ball_x = self.criterion(x_target.log(), x_pred)\n",
    "#         loss_ball_y = self.criterion(y_target.log(), y_pred)\n",
    "        \n",
    "        return loss_ball_x + loss_ball_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0dfd2e",
   "metadata": {
    "code_folding": [
     0,
     12,
     20,
     28
    ]
   },
   "outputs": [],
   "source": [
    "class Events_Spotting_Loss(nn.Module):\n",
    "    def __init__(self, weights=(1, 3), num_events=2, epsilon=1e-9):\n",
    "        super(Events_Spotting_Loss, self).__init__()\n",
    "        self.weights = torch.tensor(weights).view(1, 2)\n",
    "        self.weights = self.weights / self.weights.sum()\n",
    "        self.num_events = num_events\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, pred_events, target_events):\n",
    "        self.weights = self.weights.cuda()\n",
    "        return - torch.mean(self.weights * (target_events * torch.log(pred_events + self.epsilon) + (1. - target_events) * torch.log(1 - pred_events + self.epsilon)))\n",
    "class DICE_Smotth_Loss(nn.Module):\n",
    "    def __init__(self, epsilon=1e-9):\n",
    "        super(DICE_Smotth_Loss, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, pred_seg, target_seg):\n",
    "        return 1. - ((torch.sum(2 * pred_seg * target_seg) + self.epsilon) / (torch.sum(pred_seg) + torch.sum(target_seg) + self.epsilon))\n",
    "class BCE_Loss(nn.Module):\n",
    "    def __init__(self, epsilon=1e-9):\n",
    "        super(BCE_Loss, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, pred_seg, target_seg):\n",
    "        return - torch.mean(target_seg * torch.log(pred_seg + self.epsilon) + (1 - target_seg) * torch.log(1 - pred_seg + self.epsilon))\n",
    "class Segmentation_Loss(nn.Module):\n",
    "    def __init__(self, bce_weight=0.5):\n",
    "        super(Segmentation_Loss, self).__init__()\n",
    "        self.bce_criterion = BCE_Loss(epsilon=1e-9)\n",
    "        self.dice_criterion = DICE_Smotth_Loss(epsilon=1e-9)\n",
    "        self.bce_weight = bce_weight\n",
    "\n",
    "    def forward(self, pred_seg, target_seg):\n",
    "        target_seg = target_seg.float()\n",
    "        loss_bce = self.bce_criterion(pred_seg, target_seg)\n",
    "        loss_dice = self.dice_criterion(pred_seg, target_seg)\n",
    "        loss_seg = (1 - self.bce_weight) * loss_dice + self.bce_weight * loss_bce\n",
    "        return loss_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668a03b0",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class DICE_Smotth_Loss(nn.Module):\n",
    "    def __init__(self, epsilon=1e-9):\n",
    "        super(DICE_Smotth_Loss, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, pred_seg, target_seg):\n",
    "        return 1. - ((torch.sum(2 * pred_seg * target_seg) + self.epsilon) / (torch.sum(pred_seg) + torch.sum(target_seg) + self.epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b896869",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class BCE_Loss(nn.Module):\n",
    "    def __init__(self, epsilon=1e-9):\n",
    "        super(BCE_Loss, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, pred_seg, target_seg):\n",
    "        return - torch.mean(target_seg * torch.log(pred_seg + self.epsilon) + (1 - target_seg) * torch.log(1 - pred_seg + self.epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f3ed05",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Segmentation_Loss(nn.Module):\n",
    "    def __init__(self, bce_weight=0.5):\n",
    "        super(Segmentation_Loss, self).__init__()\n",
    "        self.bce_criterion = BCE_Loss(epsilon=1e-9)\n",
    "        self.dice_criterion = DICE_Smotth_Loss(epsilon=1e-9)\n",
    "        self.bce_weight = bce_weight\n",
    "\n",
    "    def forward(self, pred_seg, target_seg):\n",
    "        target_seg = target_seg.float()\n",
    "        loss_bce = self.bce_criterion(pred_seg, target_seg)\n",
    "        loss_dice = self.dice_criterion(pred_seg, target_seg)\n",
    "        loss_seg = (1 - self.bce_weight) * loss_dice + self.bce_weight * loss_bce\n",
    "        return loss_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e2919d",
   "metadata": {
    "code_folding": [
     0,
     1,
     16
    ]
   },
   "outputs": [],
   "source": [
    "class Unbalance_Loss_Model(nn.Module):\n",
    "    def __init__(self, model, tasks_loss_weight, weights_events, input_size, sigma, thresh_ball_pos_mask, device):\n",
    "        super(Unbalance_Loss_Model, self).__init__()\n",
    "        self.model = model\n",
    "        self.tasks_loss_weight = torch.tensor(tasks_loss_weight)\n",
    "        self.tasks_loss_weight = self.tasks_loss_weight / self.tasks_loss_weight.sum()\n",
    "        self.num_events = len(tasks_loss_weight)\n",
    "        self.w = input_size[0]\n",
    "        self.h = input_size[1]\n",
    "        self.sigma = sigma\n",
    "        self.thresh_ball_pos_mask = thresh_ball_pos_mask\n",
    "        self.device = device\n",
    "        self.ball_loss_criterion = Ball_Detection_Loss(self.w, self.h)\n",
    "        self.event_loss_criterion = Events_Spotting_Loss(weights=weights_events, num_events=self.num_events)\n",
    "        self.seg_loss_criterion = Segmentation_Loss()\n",
    "\n",
    "    def forward(self, resize_batch_input, org_ball_pos_xy, global_ball_pos_xy, target_events, target_seg):\n",
    "        pred_ball_global, pred_ball_local, pred_events, pred_seg, local_ball_pos_xy = self.model(resize_batch_input,\n",
    "                                                                                                 org_ball_pos_xy)\n",
    "        # Create target for events spotting and ball position (local and global)\n",
    "        batch_size = pred_ball_global.size(0)\n",
    "        target_ball_global = torch.zeros_like(pred_ball_global)\n",
    "        task_idx = 0\n",
    "        for sample_idx in range(batch_size):\n",
    "            target_ball_global[sample_idx] = create_target_ball(global_ball_pos_xy[sample_idx], sigma=self.sigma,\n",
    "                                                                w=self.w, h=self.h,\n",
    "                                                                thresh_mask=self.thresh_ball_pos_mask,\n",
    "                                                                device=self.device)\n",
    "        global_ball_loss = self.ball_loss_criterion(pred_ball_global, target_ball_global)\n",
    "        total_loss = global_ball_loss * self.tasks_loss_weight[task_idx]\n",
    "\n",
    "        if pred_ball_local is not None:\n",
    "            task_idx += 1\n",
    "            target_ball_local = torch.zeros_like(pred_ball_local)\n",
    "            for sample_idx in range(batch_size):\n",
    "                target_ball_local[sample_idx] = create_target_ball(local_ball_pos_xy[sample_idx], sigma=self.sigma,\n",
    "                                                                   w=self.w, h=self.h,\n",
    "                                                                   thresh_mask=self.thresh_ball_pos_mask,\n",
    "                                                                   device=self.device)\n",
    "            local_ball_loss = self.ball_loss_criterion(pred_ball_local, target_ball_local)\n",
    "            total_loss += local_ball_loss * self.tasks_loss_weight[task_idx]\n",
    "\n",
    "        if pred_events is not None:\n",
    "            task_idx += 1\n",
    "            target_events = target_events.to(device=self.device)\n",
    "            event_loss = self.event_loss_criterion(pred_events, target_events)\n",
    "            total_loss += event_loss * self.tasks_loss_weight[task_idx]\n",
    "\n",
    "        if pred_seg is not None:\n",
    "            task_idx += 1\n",
    "            seg_loss = self.seg_loss_criterion(pred_seg, target_seg)\n",
    "            total_loss += seg_loss * self.tasks_loss_weight[task_idx]\n",
    "\n",
    "        return pred_ball_global, pred_ball_local, pred_events, pred_seg, local_ball_pos_xy, total_loss, None\n",
    "\n",
    "    def run_demo(self, resize_batch_input):\n",
    "        pred_ball_global, pred_ball_local, pred_events, pred_seg = self.model.run_demo(resize_batch_input)\n",
    "        return pred_ball_global, pred_ball_local, pred_events, pred_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301eb560",
   "metadata": {
    "code_folding": [
     0,
     7
    ]
   },
   "outputs": [],
   "source": [
    "class Multi_Task_Learning_Model(nn.Module):\n",
    "    \"\"\"\n",
    "    Original paper: \"Multi-task learning using uncertainty to weigh losses for scene geometry and semantics\" - CVPR 2018\n",
    "    url: https://arxiv.org/pdf/1705.07115.pdf\n",
    "    refer code: https://github.com/Hui-Li/multi-task-learning-example-PyTorch\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, tasks, num_events, weights_events, input_size, sigma, thresh_ball_pos_mask, device):\n",
    "        super(Multi_Task_Learning_Model, self).__init__()\n",
    "        self.model = model\n",
    "        self.tasks = tasks\n",
    "        self.num_tasks = len(tasks)\n",
    "        self.log_vars = nn.Parameter(torch.zeros((self.num_tasks)))\n",
    "        self.w = input_size[0]\n",
    "        self.h = input_size[1]\n",
    "        self.sigma = sigma\n",
    "        self.thresh_ball_pos_mask = thresh_ball_pos_mask\n",
    "        self.device = device\n",
    "        self.ball_loss_criterion = Ball_Detection_Loss(self.w, self.h)\n",
    "        self.event_loss_criterion = Events_Spotting_Loss(weights=weights_events, num_events=num_events)\n",
    "        self.seg_loss_criterion = Segmentation_Loss()\n",
    "\n",
    "    def forward(self, resize_batch_input, org_ball_pos_xy, global_ball_pos_xy, target_events, target_seg):\n",
    "        log_vars_idx = 0\n",
    "        pred_ball_global, pred_ball_local, pred_events, pred_seg, local_ball_pos_xy = self.model(resize_batch_input,\n",
    "                                                                                                 org_ball_pos_xy)\n",
    "        # Create target for events spotting and ball position (local and global)\n",
    "        batch_size = pred_ball_global.size(0)\n",
    "        target_ball_global = torch.zeros_like(pred_ball_global)\n",
    "        for sample_idx in range(batch_size):\n",
    "            target_ball_global[sample_idx] = create_target_ball(global_ball_pos_xy[sample_idx], sigma=self.sigma,\n",
    "                                                                w=self.w, h=self.h,\n",
    "                                                                thresh_mask=self.thresh_ball_pos_mask,\n",
    "                                                                device=self.device)\n",
    "        global_ball_loss = self.ball_loss_criterion(pred_ball_global, target_ball_global)\n",
    "        total_loss = global_ball_loss / (torch.exp(2 * self.log_vars[log_vars_idx])) + self.log_vars[log_vars_idx]\n",
    "\n",
    "        if pred_ball_local is not None:\n",
    "            log_vars_idx += 1\n",
    "            target_ball_local = torch.zeros_like(pred_ball_local)\n",
    "            for sample_idx in range(batch_size):\n",
    "                target_ball_local[sample_idx] = create_target_ball(local_ball_pos_xy[sample_idx], sigma=self.sigma,\n",
    "                                                                   w=self.w, h=self.h,\n",
    "                                                                   thresh_mask=self.thresh_ball_pos_mask,\n",
    "                                                                   device=self.device)\n",
    "            local_ball_loss = self.ball_loss_criterion(pred_ball_local, target_ball_local)\n",
    "            total_loss += local_ball_loss / (torch.exp(2 * self.log_vars[log_vars_idx])) + self.log_vars[log_vars_idx]\n",
    "\n",
    "        if pred_events is not None:\n",
    "            log_vars_idx += 1\n",
    "            target_events = target_events.to(device=self.device)\n",
    "            event_loss = self.event_loss_criterion(pred_events, target_events)\n",
    "            total_loss += event_loss / (2 * torch.exp(self.log_vars[log_vars_idx])) + self.log_vars[log_vars_idx]\n",
    "\n",
    "        if pred_seg is not None:\n",
    "            log_vars_idx += 1\n",
    "            seg_loss = self.seg_loss_criterion(pred_seg, target_seg)\n",
    "            total_loss += seg_loss / (2 * torch.exp(self.log_vars[log_vars_idx])) + self.log_vars[log_vars_idx]\n",
    "\n",
    "        # Final weights: [math.exp(log_var) ** 0.5 for log_var in log_vars]\n",
    "\n",
    "        return pred_ball_global, pred_ball_local, pred_events, pred_seg, local_ball_pos_xy, total_loss, self.log_vars.data.tolist()\n",
    "\n",
    "    def run_demo(self, resize_batch_input):\n",
    "        pred_ball_global, pred_ball_local, pred_events, pred_seg = self.model.run_demo(resize_batch_input)\n",
    "        return pred_ball_global, pred_ball_local, pred_events, pred_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4013cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f188b75d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d393b85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3d6b76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c31781a5",
   "metadata": {},
   "source": [
    "#### Train supports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41773a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if configs.is_master_node:\n",
    "    num_parameters = get_num_parameters(model)\n",
    "    logger.info('number of trained parameters of the model: {}'.format(num_parameters))\n",
    "\n",
    "optimizer = create_optimizer(configs, model)\n",
    "lr_scheduler = create_lr_scheduler(optimizer, configs)\n",
    "best_val_loss = np.inf\n",
    "earlystop_count = 0\n",
    "is_best = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95f0467",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs.is_master_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb76a438",
   "metadata": {},
   "outputs": [],
   "source": [
    "if configs.pretrained_path is not None:\n",
    "    model = load_pretrained_model(model, configs.pretrained_path, gpu_idx, configs.overwrite_global_2_local)\n",
    "    if logger is not None:\n",
    "        logger.info('loaded pretrained model at {}'.format(configs.pretrained_path))\n",
    "\n",
    "# optionally resume from a checkpoint\n",
    "if configs.resume_path is not None:\n",
    "    checkpoint = resume_model(configs.resume_path, configs.arch, configs.gpu_idx)\n",
    "    if hasattr(model, 'module'):\n",
    "        model.module.load_state_dict(checkpoint['state_dict'])\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "    best_val_loss = checkpoint['best_val_loss']\n",
    "    earlystop_count = checkpoint['earlystop_count']\n",
    "    configs.start_epoch = checkpoint['epoch'] + 1\n",
    "\n",
    "if logger is not None:\n",
    "    logger.info(\">>> Loading dataset & getting dataloader...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea3d2a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2437d62c",
   "metadata": {},
   "source": [
    "##### Datalaoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f26e568",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, train_sampler = create_train_val_dataloader(configs)\n",
    "test_loader = create_test_dataloader(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bbf5d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d00922",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(train_loader, model, optimizer, epoch, configs, logger):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    data_time = AverageMeter('Data', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "\n",
    "    progress = ProgressMeter(len(train_loader), [batch_time, data_time, losses],\n",
    "                             prefix=\"Train - Epoch: [{}/{}]\".format(epoch, configs.num_epochs))\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    for batch_idx, (resized_imgs, org_ball_pos_xy, global_ball_pos_xy, target_events, target_seg) in enumerate(\n",
    "            tqdm(train_loader)):\n",
    "        data_time.update(time.time() - start_time)\n",
    "        batch_size = resized_imgs.size(0)\n",
    "        target_seg = target_seg.to(configs.device, non_blocking=True)\n",
    "        resized_imgs = resized_imgs.to(configs.device, non_blocking=True).float()\n",
    "        pred_ball_global, pred_ball_local, pred_events, pred_seg, local_ball_pos_xy, total_loss, _ = model(\n",
    "            resized_imgs, org_ball_pos_xy, global_ball_pos_xy, target_events, target_seg)\n",
    "        # For torch.nn.DataParallel case\n",
    "        if (not configs.distributed) and (configs.gpu_idx is None):\n",
    "            total_loss = torch.mean(total_loss)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # compute gradient and perform backpropagation\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if configs.distributed:\n",
    "            reduced_loss = reduce_tensor(total_loss.data, configs.world_size)\n",
    "        else:\n",
    "            reduced_loss = total_loss.data\n",
    "        losses.update(to_python_float(reduced_loss), batch_size)\n",
    "        # measure elapsed time\n",
    "        torch.cuda.synchronize()\n",
    "        batch_time.update(time.time() - start_time)\n",
    "\n",
    "        # Log message\n",
    "        if logger is not None:\n",
    "            if ((batch_idx + 1) % configs.print_freq) == 0:\n",
    "                logger.info(progress.get_message(batch_idx))\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e76a5d0",
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "def evaluate_one_epoch(val_loader, model, epoch, configs, logger):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    data_time = AverageMeter('Data', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "\n",
    "    progress = ProgressMeter(len(val_loader), [batch_time, data_time, losses],\n",
    "                             prefix=\"Evaluate - Epoch: [{}/{}]\".format(epoch, configs.num_epochs))\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        start_time = time.time()\n",
    "        for batch_idx, (resized_imgs, org_ball_pos_xy, global_ball_pos_xy, target_events, target_seg) in enumerate(\n",
    "                tqdm(val_loader)):\n",
    "            data_time.update(time.time() - start_time)\n",
    "            batch_size = resized_imgs.size(0)\n",
    "            target_seg = target_seg.to(configs.device, non_blocking=True)\n",
    "            resized_imgs = resized_imgs.to(configs.device, non_blocking=True).float()\n",
    "            pred_ball_global, pred_ball_local, pred_events, pred_seg, local_ball_pos_xy, total_loss, _ = model(\n",
    "                resized_imgs, org_ball_pos_xy, global_ball_pos_xy, target_events, target_seg)\n",
    "\n",
    "            # For torch.nn.DataParallel case\n",
    "            if (not configs.distributed) and (configs.gpu_idx is None):\n",
    "                total_loss = torch.mean(total_loss)\n",
    "\n",
    "            if configs.distributed:\n",
    "                reduced_loss = reduce_tensor(total_loss.data, configs.world_size)\n",
    "            else:\n",
    "                reduced_loss = total_loss.data\n",
    "            losses.update(to_python_float(reduced_loss), batch_size)\n",
    "            # measure elapsed time\n",
    "            torch.cuda.synchronize()\n",
    "            batch_time.update(time.time() - start_time)\n",
    "\n",
    "            # Log message\n",
    "            if logger is not None:\n",
    "                if ((batch_idx + 1) % configs.print_freq) == 0:\n",
    "                    logger.info(progress.get_message(batch_idx))\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94862848",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f6f635",
   "metadata": {},
   "outputs": [],
   "source": [
    "if logger is not None:\n",
    "    logger.info('number of batches in train set: {}'.format(len(train_loader)))\n",
    "    if val_loader is not None:\n",
    "        logger.info('number of batches in val set: {}'.format(len(val_loader)))\n",
    "    logger.info('number of batches in test set: {}'.format(len(test_loader)))\n",
    "\n",
    "if configs.evaluate:\n",
    "    assert val_loader is not None, \"The validation should not be None\"\n",
    "    val_loss = evaluate_one_epoch(val_loader, model, configs.start_epoch - 1, configs, logger)\n",
    "    print('Evaluate, val_loss: {}'.format(val_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b407ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_loss = evaluate_one_epoch(test_loader, model, configs.start_epoch - 1, configs, logger)\n",
    "# val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e054e0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a327e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(configs.start_epoch, configs.num_epochs + 1):\n",
    "    # Get the current learning rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        lr = param_group['lr']\n",
    "    if logger is not None:\n",
    "        logger.info('{}'.format('*-' * 40))\n",
    "        logger.info('{} {}/{} {}'.format('=' * 35, epoch, configs.num_epochs, '=' * 35))\n",
    "        logger.info('{}'.format('*-' * 40))\n",
    "        logger.info('>>> Epoch: [{}/{}] learning rate: {:.2e}'.format(epoch, configs.num_epochs, lr))\n",
    "\n",
    "    if configs.distributed:\n",
    "        train_sampler.set_epoch(epoch)\n",
    "    # train for one epoch\n",
    "    train_loss = train_one_epoch(train_loader, model, optimizer, epoch, configs, logger)\n",
    "    loss_dict = {'train': train_loss}\n",
    "    if not configs.no_val:\n",
    "        val_loss = evaluate_one_epoch(val_loader, model, epoch, configs, logger)\n",
    "        is_best = val_loss <= best_val_loss\n",
    "        best_val_loss = min(val_loss, best_val_loss)\n",
    "        loss_dict['val'] = val_loss\n",
    "\n",
    "    if not configs.no_test:\n",
    "        test_loss = evaluate_one_epoch(test_loader, model, epoch, configs, logger)\n",
    "        loss_dict['test'] = test_loss\n",
    "    # Write tensorboard\n",
    "    if tb_writer is not None:\n",
    "        tb_writer.add_scalars('Loss', loss_dict, epoch)\n",
    "    # Save checkpoint\n",
    "    if configs.is_master_node and (is_best or ((epoch % configs.checkpoint_freq) == 0)):\n",
    "        saved_state = get_saved_state(model, optimizer, lr_scheduler, epoch, configs, best_val_loss,\n",
    "                                      earlystop_count)\n",
    "        save_checkpoint(configs.checkpoints_dir, configs.saved_fn, saved_state, is_best, epoch)\n",
    "    # Check early stop training\n",
    "    if configs.earlystop_patience is not None:\n",
    "        earlystop_count = 0 if is_best else (earlystop_count + 1)\n",
    "        print_string = ' |||\\t earlystop_count: {}'.format(earlystop_count)\n",
    "        if configs.earlystop_patience <= earlystop_count:\n",
    "            print_string += '\\n\\t--- Early stopping!!!'\n",
    "            break\n",
    "        else:\n",
    "            print_string += '\\n\\t--- Continue training..., earlystop_count: {}'.format(earlystop_count)\n",
    "        if logger is not None:\n",
    "            logger.info(print_string)\n",
    "    # Adjust learning rate\n",
    "    if configs.lr_type == 'plateau':\n",
    "        assert (not configs.no_val), \"Only use plateau when having validation set\"\n",
    "        lr_scheduler.step(val_loss)\n",
    "    else:\n",
    "        lr_scheduler.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3544217f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tb_writer is not None:\n",
    "    tb_writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae4198c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d55e60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "925de371",
   "metadata": {},
   "source": [
    "#### Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35f151d",
   "metadata": {},
   "source": [
    "##### Loss exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "63d1a855",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "00c3fd48",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "459e18cc",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def parse_args():    \n",
    "    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                        help='input batch size for training (default: 64)')\n",
    "    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                        help='input batch size for testing (default: 1000)')\n",
    "    parser.add_argument('--epochs', type=int, default=14, metavar='N',\n",
    "                        help='number of epochs to train (default: 14)')\n",
    "    parser.add_argument('--lr', type=float, default=1.0, metavar='LR',\n",
    "                        help='learning rate (default: 1.0)')\n",
    "    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n",
    "                        help='Learning rate step gamma (default: 0.7)')\n",
    "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                        help='disables CUDA training')\n",
    "    parser.add_argument('--no-mps', action='store_true', default=False,\n",
    "                        help='disables macOS GPU training')\n",
    "    parser.add_argument('--dry-run', action='store_true', default=False,\n",
    "                        help='quickly check a single pass')\n",
    "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                        help='random seed (default: 1)')\n",
    "    parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')\n",
    "    parser.add_argument('--save-model', action='store_true', default=False,\n",
    "                        help='For Saving the current Model')\n",
    "    args = parser.parse_args([])\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bb7481ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parse_args()\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "if use_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "elif use_mps:\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "train_kwargs = {'batch_size': args.batch_size}\n",
    "test_kwargs = {'batch_size': args.test_batch_size}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "327171df",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_cuda:\n",
    "    cuda_kwargs = {'num_workers': 1,\n",
    "                   'pin_memory': True,\n",
    "                   'shuffle': True}\n",
    "    train_kwargs.update(cuda_kwargs)\n",
    "    test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transform)\n",
    "dataset2 = datasets.MNIST('../data', train=False,\n",
    "                   transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c00660cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "89c5d334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (dropout1): Dropout(p=0.25, inplace=False)\n",
       "  (dropout2): Dropout(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad5b83f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b56a2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2a2530d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Training settings\n",
    "    \n",
    "    \n",
    "    \n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(args, model, device, train_loader, optimizer, epoch)\n",
    "        test(model, device, test_loader)\n",
    "        scheduler.step()\n",
    "\n",
    "    if args.save_model:\n",
    "        torch.save(model.state_dict(), \"mnist_cnn.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fb336bf0",
   "metadata": {
    "code_folding": [
     5,
     22
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.347594\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 0.983658\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 0.812437\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 0.549483\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.719684\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.471002\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.241950\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 0.357736\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.348226\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.236578\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.295051\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.234309\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.315701\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.230637\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.211834\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.188162\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.150411\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.308384\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.259356\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.248719\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.145961\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.222894\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.156624\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.159119\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.050251\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.131010\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.135093\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.174254\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.083571\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.259261\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.162590\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.247466\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.051887\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.251032\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.014991\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.158303\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.098659\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.175611\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.059502\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.165007\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.202951\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.175240\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.279905\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.080737\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.105725\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.148042\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.330207\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.078495\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.168134\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.090977\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.056740\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.048117\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.088875\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.047487\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.205670\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.081333\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.065141\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.116751\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.118451\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.161961\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.167889\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.050009\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.264257\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.153967\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.115861\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.147449\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.074751\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.147242\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.066532\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.029485\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.110802\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.101048\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.018070\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.051587\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.015607\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.161335\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.041299\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.126472\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.032603\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.167685\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.104360\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.113616\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.081995\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.098267\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.074194\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.281751\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.085266\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.123739\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.176813\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.352362\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.095352\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.121097\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.122138\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.052806\n",
      "\n",
      "Test set: Average loss: 0.0541, Accuracy: 9839/10000 (98%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.072091\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.157190\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.059434\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.033962\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.106852\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.184240\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.074310\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.129019\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.025858\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.072231\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.248620\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.036329\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.122077\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.098336\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.106356\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.028223\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.021852\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.028084\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.137840\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.042923\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.074529\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.004961\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.029210\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.108130\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.217392\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.046008\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.028387\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.015431\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.038984\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.067587\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.019593\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.047323\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.030083\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.038687\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.149451\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.106627\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.131209\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.087823\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.044596\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.040770\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.140482\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.204778\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.022061\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.026951\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.014393\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.274255\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.119688\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.238358\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.337632\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.163042\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.090871\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.029940\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.061937\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.059061\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.077286\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.193879\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.010298\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.120855\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.022385\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.114751\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.073289\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.117472\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.059675\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.144629\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.067044\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.080795\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.003755\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.270382\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.032103\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.017160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.005156\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.040091\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.042667\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.016043\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.052858\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.009573\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.045211\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.012477\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.111168\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.035781\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.134454\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.038345\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.058406\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.016266\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.021798\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.010563\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.383408\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.026147\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.087794\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.067867\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.005459\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.160532\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.125830\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.065556\n",
      "\n",
      "Test set: Average loss: 0.0361, Accuracy: 9878/10000 (99%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.013581\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.092735\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.036624\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.180249\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.067159\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.010208\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.044606\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.028842\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.045360\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.149401\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.014336\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.067158\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.043570\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.076957\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.016832\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.016536\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.009927\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.038328\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.131517\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.098019\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.033707\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.005020\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.032961\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.010002\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.035194\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.058404\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.030520\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.004597\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.006279\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.003130\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.041055\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.056350\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.016891\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.027863\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.030406\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.011738\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.081871\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.003621\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.008774\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.111978\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.005387\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.006866\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.043305\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.020664\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.025975\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.014950\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.076875\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.052665\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.051745\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.013105\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.113501\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.013846\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.005051\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.097298\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.085127\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.082614\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.090934\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.014608\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.012576\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.124350\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.011298\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.015547\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.003989\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.024165\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.174896\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.018644\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.028844\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.025417\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.049546\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.077180\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.025689\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.024839\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.009844\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.100813\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.015374\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.010145\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.060646\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.038636\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.013370\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.069100\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.025471\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.245594\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.046359\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.063601\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.115266\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.022365\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.094418\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.032764\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.076013\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.017833\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.054406\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.010379\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.005155\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.042601\n",
      "\n",
      "Test set: Average loss: 0.0360, Accuracy: 9886/10000 (99%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.027065\n",
      "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.012031\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.067081\n",
      "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.002953\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.047192\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.131052\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.013961\n",
      "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.006206\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.128880\n",
      "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.003470\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.056130\n",
      "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.080524\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.021365\n",
      "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.064907\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.031300\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.002662\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.002174\n",
      "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.049388\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.003263\n",
      "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.075681\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.001104\n",
      "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.041360\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.030226\n",
      "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.003079\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.098101\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.084074\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.012641\n",
      "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.045388\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.005088\n",
      "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.005889\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.016811\n",
      "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.122563\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.242754\n",
      "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.123073\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.022296\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.005521\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.040219\n",
      "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.011384\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.016956\n",
      "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.036171\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.096477\n",
      "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.052160\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.025657\n",
      "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.003757\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.003887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.001183\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.124050\n",
      "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.028677\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.023309\n",
      "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.000507\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.097581\n",
      "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.014723\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.020754\n",
      "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.038414\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.356770\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.088222\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.040106\n",
      "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.085385\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.101294\n",
      "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.043066\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.021173\n",
      "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.080902\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.259819\n",
      "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.021375\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.029478\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.012622\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.034518\n",
      "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.003754\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.172787\n",
      "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.050328\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.011068\n",
      "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.032388\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.002371\n",
      "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.056304\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.003260\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.006575\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.001492\n",
      "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.182271\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.003497\n",
      "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.012253\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.081501\n",
      "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.022986\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.162298\n",
      "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.016396\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.001437\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.058367\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.200560\n",
      "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.005047\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.011092\n",
      "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.122888\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.015903\n",
      "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.046832\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.101143\n",
      "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.016315\n",
      "\n",
      "Test set: Average loss: 0.0304, Accuracy: 9896/10000 (99%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.039375\n",
      "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.007725\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.011685\n",
      "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.042222\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.019422\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.037772\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.119908\n",
      "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.040447\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.007666\n",
      "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.002225\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.013758\n",
      "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.037462\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.014935\n",
      "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.003745\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.108370\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.022956\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.001315\n",
      "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.050540\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.001847\n",
      "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.147219\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.015966\n",
      "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.014090\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.007737\n",
      "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.007925\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.060858\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.048420\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.008569\n",
      "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.025732\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.004575\n",
      "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.013115\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.019279\n",
      "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.052545\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.025949\n",
      "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.050843\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.049584\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.005897\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.000923\n",
      "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.002530\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.013876\n",
      "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.008088\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.019814\n",
      "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.018807\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.037810\n",
      "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.087904\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.027495\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.016696\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.134831\n",
      "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.011244\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.136851\n",
      "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.073692\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.026104\n",
      "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.031079\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.002507\n",
      "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.042961\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.068002\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.102815\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.001480\n",
      "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.105564\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.065959\n",
      "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.005748\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.002808\n",
      "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.035093\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.003250\n",
      "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.000832\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.003904\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.047182\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.014995\n",
      "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.090626\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.089131\n",
      "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.069690\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.074110\n",
      "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.107941\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.016069\n",
      "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.004831\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.017116\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.029900\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.014831\n",
      "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.019707\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.036555\n",
      "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.029440\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.011288\n",
      "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.001592\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.020746\n",
      "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.019142\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.039365\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.022868\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.111531\n",
      "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.010701\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.016652\n",
      "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.008460\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.003210\n",
      "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.006078\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.019003\n",
      "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.009376\n",
      "\n",
      "Test set: Average loss: 0.0276, Accuracy: 9916/10000 (99%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.023617\n",
      "Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.009187\n",
      "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.084760\n",
      "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.003336\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.003254\n",
      "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.017143\n",
      "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.002823\n",
      "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.012507\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.001209\n",
      "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.029682\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.013758\n",
      "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.009416\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.022887\n",
      "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.005964\n",
      "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.004354\n",
      "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.052342\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.191182\n",
      "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.084136\n",
      "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.001030\n",
      "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.000848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.013639\n",
      "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.004920\n",
      "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.050332\n",
      "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.050175\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.058643\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.013721\n",
      "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.051024\n",
      "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.004658\n",
      "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.017013\n",
      "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.035396\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.019837\n",
      "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.002271\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.030845\n",
      "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.054844\n",
      "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.003125\n",
      "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.031238\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.015855\n",
      "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.011983\n",
      "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.003550\n",
      "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.002533\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.000560\n",
      "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.015702\n",
      "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.001822\n",
      "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.021464\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.132722\n",
      "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.022857\n",
      "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.003568\n",
      "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.000664\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.038246\n",
      "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.009882\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.017804\n",
      "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.002166\n",
      "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.007702\n",
      "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.011298\n",
      "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.007162\n",
      "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.063547\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.086367\n",
      "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.048108\n",
      "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.016794\n",
      "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.140212\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.027361\n",
      "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.063421\n",
      "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.067654\n",
      "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.021537\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.001440\n",
      "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.034341\n",
      "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.003778\n",
      "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.076154\n",
      "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.002591\n",
      "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.078218\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.010626\n",
      "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.012792\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.007547\n",
      "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.015064\n",
      "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.024560\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.005883\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.002880\n",
      "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.012496\n",
      "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.002330\n",
      "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.014911\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.006771\n",
      "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.127001\n",
      "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.007961\n",
      "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.020810\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.004141\n",
      "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.002832\n",
      "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.027154\n",
      "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.005833\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.069405\n",
      "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.092920\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.027495\n",
      "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.027408\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.037132\n",
      "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.007932\n",
      "\n",
      "Test set: Average loss: 0.0278, Accuracy: 9905/10000 (99%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.004429\n",
      "Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.033271\n",
      "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.034331\n",
      "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.023034\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.078125\n",
      "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.004066\n",
      "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.022249\n",
      "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.078854\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.007737\n",
      "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.004958\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.003818\n",
      "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.044922\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.035412\n",
      "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.020031\n",
      "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.007508\n",
      "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.020391\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.024075\n",
      "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.013784\n",
      "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.008835\n",
      "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.065327\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.004067\n",
      "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.024633\n",
      "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.003818\n",
      "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.003158\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.002037\n",
      "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.002278\n",
      "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.021837\n",
      "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.008012\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.035114\n",
      "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.038781\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.003391\n",
      "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.011120\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.013973\n",
      "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.113977\n",
      "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.003507\n",
      "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.004894\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.020056\n",
      "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.077458\n",
      "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.014904\n",
      "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.132570\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.030971\n",
      "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.001873\n",
      "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.001791\n",
      "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.010562\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.105263\n",
      "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.019132\n",
      "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.079520\n",
      "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.141121\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.003654\n",
      "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.000651\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.042245\n",
      "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.013096\n",
      "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.023074\n",
      "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.043449\n",
      "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.008477\n",
      "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.005923\n",
      "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.002804\n",
      "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.077924\n",
      "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.001529\n",
      "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.017647\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.013454\n",
      "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.011022\n",
      "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.023698\n",
      "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.105024\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.098654\n",
      "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.002006\n",
      "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.108038\n",
      "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.029833\n",
      "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.065050\n",
      "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.005078\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.027379\n",
      "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.014388\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.146805\n",
      "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.044518\n",
      "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.081171\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.024110\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.005186\n",
      "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.002801\n",
      "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.007116\n",
      "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.065445\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.004589\n",
      "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.109427\n",
      "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.004333\n",
      "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.056689\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.013178\n",
      "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.116786\n",
      "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.005324\n",
      "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.042489\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.007562\n",
      "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.002189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.031934\n",
      "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.097595\n",
      "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.062643\n",
      "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.002427\n",
      "\n",
      "Test set: Average loss: 0.0278, Accuracy: 9908/10000 (99%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.007112\n",
      "Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.005264\n",
      "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.005678\n",
      "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.002306\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.056845\n",
      "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.019223\n",
      "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.010082\n",
      "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.011308\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.017971\n",
      "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.004671\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.066789\n",
      "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.003995\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.011426\n",
      "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.005440\n",
      "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.015266\n",
      "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.001273\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.125849\n",
      "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.004807\n",
      "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.004823\n",
      "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.045061\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.019581\n",
      "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.003726\n",
      "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.017575\n",
      "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.006732\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.000674\n",
      "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.070850\n",
      "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.010166\n",
      "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.000936\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.008579\n",
      "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.042260\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.056651\n",
      "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.000870\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.001621\n",
      "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.013439\n",
      "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.016495\n",
      "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.142132\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.016999\n",
      "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.008783\n",
      "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.004521\n",
      "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.025539\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.003211\n",
      "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.048610\n",
      "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.002816\n",
      "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.021796\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.016795\n",
      "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.002708\n",
      "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.003730\n",
      "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.011791\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.118466\n",
      "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.007124\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.100671\n",
      "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.036381\n",
      "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.002427\n",
      "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.022661\n",
      "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.003810\n",
      "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.047376\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.004879\n",
      "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.001347\n",
      "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.006460\n",
      "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.009449\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.035507\n",
      "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.004555\n",
      "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.001519\n",
      "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.049333\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.014727\n",
      "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.069154\n",
      "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.004144\n",
      "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.001677\n",
      "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.014210\n",
      "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.163845\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.002877\n",
      "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.066308\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.003879\n",
      "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.009536\n",
      "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.019368\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.002453\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.007233\n",
      "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.062271\n",
      "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.016487\n",
      "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.001467\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.196126\n",
      "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.007554\n",
      "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.008167\n",
      "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.001653\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.000708\n",
      "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.001277\n",
      "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.012044\n",
      "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.035582\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.002097\n",
      "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.008259\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.000686\n",
      "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.118146\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.005141\n",
      "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.000442\n",
      "\n",
      "Test set: Average loss: 0.0282, Accuracy: 9912/10000 (99%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.062968\n",
      "Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.002460\n",
      "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.005770\n",
      "Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.006014\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.002796\n",
      "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.053297\n",
      "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.000756\n",
      "Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.008032\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.000307\n",
      "Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.026564\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.033960\n",
      "Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.008817\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.006475\n",
      "Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.149222\n",
      "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.002313\n",
      "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.054076\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.002752\n",
      "Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.002051\n",
      "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.055416\n",
      "Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.009739\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.000949\n",
      "Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.011208\n",
      "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.001434\n",
      "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.001733\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.015000\n",
      "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.006884\n",
      "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.008476\n",
      "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.057983\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.001633\n",
      "Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.012244\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.018375\n",
      "Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.014420\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.001013\n",
      "Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.048025\n",
      "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.001204\n",
      "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.007891\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.024900\n",
      "Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.040780\n",
      "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.001750\n",
      "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.003835\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.017819\n",
      "Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.025855\n",
      "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.015314\n",
      "Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.002202\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.005166\n",
      "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.077858\n",
      "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.024183\n",
      "Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.004528\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.006850\n",
      "Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.004737\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.169187\n",
      "Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.009204\n",
      "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.024202\n",
      "Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.019147\n",
      "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.002758\n",
      "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.003599\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.015739\n",
      "Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.007048\n",
      "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.014274\n",
      "Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.037144\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.002235\n",
      "Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.008991\n",
      "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.005445\n",
      "Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.115466\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.004774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.005630\n",
      "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.007628\n",
      "Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.006079\n",
      "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.003138\n",
      "Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.040169\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.083692\n",
      "Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.013739\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.016204\n",
      "Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.005373\n",
      "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.010468\n",
      "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.035193\n",
      "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.009995\n",
      "Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.039081\n",
      "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.012873\n",
      "Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.001141\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.040576\n",
      "Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.065846\n",
      "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.013157\n",
      "Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.002006\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.048303\n",
      "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.003331\n",
      "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.007602\n",
      "Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.013161\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.009696\n",
      "Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.009448\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.002560\n",
      "Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.016605\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.034137\n",
      "Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.002883\n",
      "\n",
      "Test set: Average loss: 0.0289, Accuracy: 9910/10000 (99%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.001651\n",
      "Train Epoch: 10 [640/60000 (1%)]\tLoss: 0.000753\n",
      "Train Epoch: 10 [1280/60000 (2%)]\tLoss: 0.033103\n",
      "Train Epoch: 10 [1920/60000 (3%)]\tLoss: 0.006380\n",
      "Train Epoch: 10 [2560/60000 (4%)]\tLoss: 0.031487\n",
      "Train Epoch: 10 [3200/60000 (5%)]\tLoss: 0.077436\n",
      "Train Epoch: 10 [3840/60000 (6%)]\tLoss: 0.010439\n",
      "Train Epoch: 10 [4480/60000 (7%)]\tLoss: 0.135553\n",
      "Train Epoch: 10 [5120/60000 (9%)]\tLoss: 0.039183\n",
      "Train Epoch: 10 [5760/60000 (10%)]\tLoss: 0.003737\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.028564\n",
      "Train Epoch: 10 [7040/60000 (12%)]\tLoss: 0.035904\n",
      "Train Epoch: 10 [7680/60000 (13%)]\tLoss: 0.005641\n",
      "Train Epoch: 10 [8320/60000 (14%)]\tLoss: 0.007022\n",
      "Train Epoch: 10 [8960/60000 (15%)]\tLoss: 0.005539\n",
      "Train Epoch: 10 [9600/60000 (16%)]\tLoss: 0.007524\n",
      "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 0.098776\n",
      "Train Epoch: 10 [10880/60000 (18%)]\tLoss: 0.053835\n",
      "Train Epoch: 10 [11520/60000 (19%)]\tLoss: 0.013464\n",
      "Train Epoch: 10 [12160/60000 (20%)]\tLoss: 0.006631\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.019679\n",
      "Train Epoch: 10 [13440/60000 (22%)]\tLoss: 0.001876\n",
      "Train Epoch: 10 [14080/60000 (23%)]\tLoss: 0.017358\n",
      "Train Epoch: 10 [14720/60000 (25%)]\tLoss: 0.023059\n",
      "Train Epoch: 10 [15360/60000 (26%)]\tLoss: 0.031229\n",
      "Train Epoch: 10 [16000/60000 (27%)]\tLoss: 0.032800\n",
      "Train Epoch: 10 [16640/60000 (28%)]\tLoss: 0.001742\n",
      "Train Epoch: 10 [17280/60000 (29%)]\tLoss: 0.015207\n",
      "Train Epoch: 10 [17920/60000 (30%)]\tLoss: 0.015838\n",
      "Train Epoch: 10 [18560/60000 (31%)]\tLoss: 0.018587\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.037021\n",
      "Train Epoch: 10 [19840/60000 (33%)]\tLoss: 0.017208\n",
      "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 0.004756\n",
      "Train Epoch: 10 [21120/60000 (35%)]\tLoss: 0.033029\n",
      "Train Epoch: 10 [21760/60000 (36%)]\tLoss: 0.012963\n",
      "Train Epoch: 10 [22400/60000 (37%)]\tLoss: 0.034604\n",
      "Train Epoch: 10 [23040/60000 (38%)]\tLoss: 0.015111\n",
      "Train Epoch: 10 [23680/60000 (39%)]\tLoss: 0.003161\n",
      "Train Epoch: 10 [24320/60000 (41%)]\tLoss: 0.212354\n",
      "Train Epoch: 10 [24960/60000 (42%)]\tLoss: 0.032017\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.003238\n",
      "Train Epoch: 10 [26240/60000 (44%)]\tLoss: 0.005730\n",
      "Train Epoch: 10 [26880/60000 (45%)]\tLoss: 0.016700\n",
      "Train Epoch: 10 [27520/60000 (46%)]\tLoss: 0.012707\n",
      "Train Epoch: 10 [28160/60000 (47%)]\tLoss: 0.021388\n",
      "Train Epoch: 10 [28800/60000 (48%)]\tLoss: 0.003333\n",
      "Train Epoch: 10 [29440/60000 (49%)]\tLoss: 0.103771\n",
      "Train Epoch: 10 [30080/60000 (50%)]\tLoss: 0.012956\n",
      "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 0.009447\n",
      "Train Epoch: 10 [31360/60000 (52%)]\tLoss: 0.004611\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.015876\n",
      "Train Epoch: 10 [32640/60000 (54%)]\tLoss: 0.006312\n",
      "Train Epoch: 10 [33280/60000 (55%)]\tLoss: 0.012340\n",
      "Train Epoch: 10 [33920/60000 (57%)]\tLoss: 0.008612\n",
      "Train Epoch: 10 [34560/60000 (58%)]\tLoss: 0.014661\n",
      "Train Epoch: 10 [35200/60000 (59%)]\tLoss: 0.007966\n",
      "Train Epoch: 10 [35840/60000 (60%)]\tLoss: 0.027986\n",
      "Train Epoch: 10 [36480/60000 (61%)]\tLoss: 0.003203\n",
      "Train Epoch: 10 [37120/60000 (62%)]\tLoss: 0.019675\n",
      "Train Epoch: 10 [37760/60000 (63%)]\tLoss: 0.003042\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.022297\n",
      "Train Epoch: 10 [39040/60000 (65%)]\tLoss: 0.017236\n",
      "Train Epoch: 10 [39680/60000 (66%)]\tLoss: 0.007174\n",
      "Train Epoch: 10 [40320/60000 (67%)]\tLoss: 0.031017\n",
      "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 0.096307\n",
      "Train Epoch: 10 [41600/60000 (69%)]\tLoss: 0.005800\n",
      "Train Epoch: 10 [42240/60000 (70%)]\tLoss: 0.008705\n",
      "Train Epoch: 10 [42880/60000 (71%)]\tLoss: 0.008460\n",
      "Train Epoch: 10 [43520/60000 (72%)]\tLoss: 0.057029\n",
      "Train Epoch: 10 [44160/60000 (74%)]\tLoss: 0.010457\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.006637\n",
      "Train Epoch: 10 [45440/60000 (76%)]\tLoss: 0.010450\n",
      "Train Epoch: 10 [46080/60000 (77%)]\tLoss: 0.004308\n",
      "Train Epoch: 10 [46720/60000 (78%)]\tLoss: 0.157209\n",
      "Train Epoch: 10 [47360/60000 (79%)]\tLoss: 0.004634\n",
      "Train Epoch: 10 [48000/60000 (80%)]\tLoss: 0.042641\n",
      "Train Epoch: 10 [48640/60000 (81%)]\tLoss: 0.016293\n",
      "Train Epoch: 10 [49280/60000 (82%)]\tLoss: 0.012615\n",
      "Train Epoch: 10 [49920/60000 (83%)]\tLoss: 0.066066\n",
      "Train Epoch: 10 [50560/60000 (84%)]\tLoss: 0.040445\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.011998\n",
      "Train Epoch: 10 [51840/60000 (86%)]\tLoss: 0.023318\n",
      "Train Epoch: 10 [52480/60000 (87%)]\tLoss: 0.029277\n",
      "Train Epoch: 10 [53120/60000 (88%)]\tLoss: 0.000187\n",
      "Train Epoch: 10 [53760/60000 (90%)]\tLoss: 0.006996\n",
      "Train Epoch: 10 [54400/60000 (91%)]\tLoss: 0.112029\n",
      "Train Epoch: 10 [55040/60000 (92%)]\tLoss: 0.012389\n",
      "Train Epoch: 10 [55680/60000 (93%)]\tLoss: 0.038251\n",
      "Train Epoch: 10 [56320/60000 (94%)]\tLoss: 0.003643\n",
      "Train Epoch: 10 [56960/60000 (95%)]\tLoss: 0.048783\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.002456\n",
      "Train Epoch: 10 [58240/60000 (97%)]\tLoss: 0.002680\n",
      "Train Epoch: 10 [58880/60000 (98%)]\tLoss: 0.002414\n",
      "Train Epoch: 10 [59520/60000 (99%)]\tLoss: 0.010041\n",
      "\n",
      "Test set: Average loss: 0.0279, Accuracy: 9913/10000 (99%)\n",
      "\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.031035\n",
      "Train Epoch: 11 [640/60000 (1%)]\tLoss: 0.056194\n",
      "Train Epoch: 11 [1280/60000 (2%)]\tLoss: 0.020854\n",
      "Train Epoch: 11 [1920/60000 (3%)]\tLoss: 0.005123\n",
      "Train Epoch: 11 [2560/60000 (4%)]\tLoss: 0.001652\n",
      "Train Epoch: 11 [3200/60000 (5%)]\tLoss: 0.006688\n",
      "Train Epoch: 11 [3840/60000 (6%)]\tLoss: 0.006969\n",
      "Train Epoch: 11 [4480/60000 (7%)]\tLoss: 0.030365\n",
      "Train Epoch: 11 [5120/60000 (9%)]\tLoss: 0.013071\n",
      "Train Epoch: 11 [5760/60000 (10%)]\tLoss: 0.004556\n",
      "Train Epoch: 11 [6400/60000 (11%)]\tLoss: 0.022697\n",
      "Train Epoch: 11 [7040/60000 (12%)]\tLoss: 0.012297\n",
      "Train Epoch: 11 [7680/60000 (13%)]\tLoss: 0.003932\n",
      "Train Epoch: 11 [8320/60000 (14%)]\tLoss: 0.021561\n",
      "Train Epoch: 11 [8960/60000 (15%)]\tLoss: 0.024802\n",
      "Train Epoch: 11 [9600/60000 (16%)]\tLoss: 0.001248\n",
      "Train Epoch: 11 [10240/60000 (17%)]\tLoss: 0.006277\n",
      "Train Epoch: 11 [10880/60000 (18%)]\tLoss: 0.024626\n",
      "Train Epoch: 11 [11520/60000 (19%)]\tLoss: 0.047501\n",
      "Train Epoch: 11 [12160/60000 (20%)]\tLoss: 0.002481\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 0.042176\n",
      "Train Epoch: 11 [13440/60000 (22%)]\tLoss: 0.008351\n",
      "Train Epoch: 11 [14080/60000 (23%)]\tLoss: 0.004153\n",
      "Train Epoch: 11 [14720/60000 (25%)]\tLoss: 0.011925\n",
      "Train Epoch: 11 [15360/60000 (26%)]\tLoss: 0.018501\n",
      "Train Epoch: 11 [16000/60000 (27%)]\tLoss: 0.038930\n",
      "Train Epoch: 11 [16640/60000 (28%)]\tLoss: 0.023223\n",
      "Train Epoch: 11 [17280/60000 (29%)]\tLoss: 0.016335\n",
      "Train Epoch: 11 [17920/60000 (30%)]\tLoss: 0.003181\n",
      "Train Epoch: 11 [18560/60000 (31%)]\tLoss: 0.003213\n",
      "Train Epoch: 11 [19200/60000 (32%)]\tLoss: 0.000197\n",
      "Train Epoch: 11 [19840/60000 (33%)]\tLoss: 0.004542\n",
      "Train Epoch: 11 [20480/60000 (34%)]\tLoss: 0.002369\n",
      "Train Epoch: 11 [21120/60000 (35%)]\tLoss: 0.002712\n",
      "Train Epoch: 11 [21760/60000 (36%)]\tLoss: 0.078533\n",
      "Train Epoch: 11 [22400/60000 (37%)]\tLoss: 0.036901\n",
      "Train Epoch: 11 [23040/60000 (38%)]\tLoss: 0.007329\n",
      "Train Epoch: 11 [23680/60000 (39%)]\tLoss: 0.027126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 11 [24320/60000 (41%)]\tLoss: 0.000265\n",
      "Train Epoch: 11 [24960/60000 (42%)]\tLoss: 0.036862\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.012726\n",
      "Train Epoch: 11 [26240/60000 (44%)]\tLoss: 0.019123\n",
      "Train Epoch: 11 [26880/60000 (45%)]\tLoss: 0.044979\n",
      "Train Epoch: 11 [27520/60000 (46%)]\tLoss: 0.001205\n",
      "Train Epoch: 11 [28160/60000 (47%)]\tLoss: 0.008954\n",
      "Train Epoch: 11 [28800/60000 (48%)]\tLoss: 0.037231\n",
      "Train Epoch: 11 [29440/60000 (49%)]\tLoss: 0.012875\n",
      "Train Epoch: 11 [30080/60000 (50%)]\tLoss: 0.024769\n",
      "Train Epoch: 11 [30720/60000 (51%)]\tLoss: 0.055777\n",
      "Train Epoch: 11 [31360/60000 (52%)]\tLoss: 0.021223\n",
      "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 0.143414\n",
      "Train Epoch: 11 [32640/60000 (54%)]\tLoss: 0.004562\n",
      "Train Epoch: 11 [33280/60000 (55%)]\tLoss: 0.027038\n",
      "Train Epoch: 11 [33920/60000 (57%)]\tLoss: 0.013368\n",
      "Train Epoch: 11 [34560/60000 (58%)]\tLoss: 0.098716\n",
      "Train Epoch: 11 [35200/60000 (59%)]\tLoss: 0.030464\n",
      "Train Epoch: 11 [35840/60000 (60%)]\tLoss: 0.005228\n",
      "Train Epoch: 11 [36480/60000 (61%)]\tLoss: 0.006219\n",
      "Train Epoch: 11 [37120/60000 (62%)]\tLoss: 0.152970\n",
      "Train Epoch: 11 [37760/60000 (63%)]\tLoss: 0.016374\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 0.018008\n",
      "Train Epoch: 11 [39040/60000 (65%)]\tLoss: 0.000686\n",
      "Train Epoch: 11 [39680/60000 (66%)]\tLoss: 0.001405\n",
      "Train Epoch: 11 [40320/60000 (67%)]\tLoss: 0.000136\n",
      "Train Epoch: 11 [40960/60000 (68%)]\tLoss: 0.028502\n",
      "Train Epoch: 11 [41600/60000 (69%)]\tLoss: 0.032303\n",
      "Train Epoch: 11 [42240/60000 (70%)]\tLoss: 0.017675\n",
      "Train Epoch: 11 [42880/60000 (71%)]\tLoss: 0.002560\n",
      "Train Epoch: 11 [43520/60000 (72%)]\tLoss: 0.004211\n",
      "Train Epoch: 11 [44160/60000 (74%)]\tLoss: 0.056049\n",
      "Train Epoch: 11 [44800/60000 (75%)]\tLoss: 0.024134\n",
      "Train Epoch: 11 [45440/60000 (76%)]\tLoss: 0.018553\n",
      "Train Epoch: 11 [46080/60000 (77%)]\tLoss: 0.086146\n",
      "Train Epoch: 11 [46720/60000 (78%)]\tLoss: 0.017802\n",
      "Train Epoch: 11 [47360/60000 (79%)]\tLoss: 0.001792\n",
      "Train Epoch: 11 [48000/60000 (80%)]\tLoss: 0.001378\n",
      "Train Epoch: 11 [48640/60000 (81%)]\tLoss: 0.002632\n",
      "Train Epoch: 11 [49280/60000 (82%)]\tLoss: 0.004759\n",
      "Train Epoch: 11 [49920/60000 (83%)]\tLoss: 0.005073\n",
      "Train Epoch: 11 [50560/60000 (84%)]\tLoss: 0.012728\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.099828\n",
      "Train Epoch: 11 [51840/60000 (86%)]\tLoss: 0.000757\n",
      "Train Epoch: 11 [52480/60000 (87%)]\tLoss: 0.004127\n",
      "Train Epoch: 11 [53120/60000 (88%)]\tLoss: 0.008733\n",
      "Train Epoch: 11 [53760/60000 (90%)]\tLoss: 0.060103\n",
      "Train Epoch: 11 [54400/60000 (91%)]\tLoss: 0.071365\n",
      "Train Epoch: 11 [55040/60000 (92%)]\tLoss: 0.011313\n",
      "Train Epoch: 11 [55680/60000 (93%)]\tLoss: 0.035462\n",
      "Train Epoch: 11 [56320/60000 (94%)]\tLoss: 0.013468\n",
      "Train Epoch: 11 [56960/60000 (95%)]\tLoss: 0.002527\n",
      "Train Epoch: 11 [57600/60000 (96%)]\tLoss: 0.030543\n",
      "Train Epoch: 11 [58240/60000 (97%)]\tLoss: 0.022511\n",
      "Train Epoch: 11 [58880/60000 (98%)]\tLoss: 0.002175\n",
      "Train Epoch: 11 [59520/60000 (99%)]\tLoss: 0.005702\n",
      "\n",
      "Test set: Average loss: 0.0264, Accuracy: 9918/10000 (99%)\n",
      "\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.003464\n",
      "Train Epoch: 12 [640/60000 (1%)]\tLoss: 0.045902\n",
      "Train Epoch: 12 [1280/60000 (2%)]\tLoss: 0.021651\n",
      "Train Epoch: 12 [1920/60000 (3%)]\tLoss: 0.151262\n",
      "Train Epoch: 12 [2560/60000 (4%)]\tLoss: 0.013961\n",
      "Train Epoch: 12 [3200/60000 (5%)]\tLoss: 0.044498\n",
      "Train Epoch: 12 [3840/60000 (6%)]\tLoss: 0.010403\n",
      "Train Epoch: 12 [4480/60000 (7%)]\tLoss: 0.000796\n",
      "Train Epoch: 12 [5120/60000 (9%)]\tLoss: 0.006555\n",
      "Train Epoch: 12 [5760/60000 (10%)]\tLoss: 0.007389\n",
      "Train Epoch: 12 [6400/60000 (11%)]\tLoss: 0.008479\n",
      "Train Epoch: 12 [7040/60000 (12%)]\tLoss: 0.010341\n",
      "Train Epoch: 12 [7680/60000 (13%)]\tLoss: 0.051156\n",
      "Train Epoch: 12 [8320/60000 (14%)]\tLoss: 0.004333\n",
      "Train Epoch: 12 [8960/60000 (15%)]\tLoss: 0.033065\n",
      "Train Epoch: 12 [9600/60000 (16%)]\tLoss: 0.011554\n",
      "Train Epoch: 12 [10240/60000 (17%)]\tLoss: 0.022373\n",
      "Train Epoch: 12 [10880/60000 (18%)]\tLoss: 0.008389\n",
      "Train Epoch: 12 [11520/60000 (19%)]\tLoss: 0.025011\n",
      "Train Epoch: 12 [12160/60000 (20%)]\tLoss: 0.048587\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 0.031969\n",
      "Train Epoch: 12 [13440/60000 (22%)]\tLoss: 0.003566\n",
      "Train Epoch: 12 [14080/60000 (23%)]\tLoss: 0.004910\n",
      "Train Epoch: 12 [14720/60000 (25%)]\tLoss: 0.003297\n",
      "Train Epoch: 12 [15360/60000 (26%)]\tLoss: 0.000774\n",
      "Train Epoch: 12 [16000/60000 (27%)]\tLoss: 0.004238\n",
      "Train Epoch: 12 [16640/60000 (28%)]\tLoss: 0.002563\n",
      "Train Epoch: 12 [17280/60000 (29%)]\tLoss: 0.027487\n",
      "Train Epoch: 12 [17920/60000 (30%)]\tLoss: 0.037796\n",
      "Train Epoch: 12 [18560/60000 (31%)]\tLoss: 0.033619\n",
      "Train Epoch: 12 [19200/60000 (32%)]\tLoss: 0.008010\n",
      "Train Epoch: 12 [19840/60000 (33%)]\tLoss: 0.003534\n",
      "Train Epoch: 12 [20480/60000 (34%)]\tLoss: 0.021005\n",
      "Train Epoch: 12 [21120/60000 (35%)]\tLoss: 0.015050\n",
      "Train Epoch: 12 [21760/60000 (36%)]\tLoss: 0.026355\n",
      "Train Epoch: 12 [22400/60000 (37%)]\tLoss: 0.000476\n",
      "Train Epoch: 12 [23040/60000 (38%)]\tLoss: 0.003649\n",
      "Train Epoch: 12 [23680/60000 (39%)]\tLoss: 0.025588\n",
      "Train Epoch: 12 [24320/60000 (41%)]\tLoss: 0.001266\n",
      "Train Epoch: 12 [24960/60000 (42%)]\tLoss: 0.020079\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.000390\n",
      "Train Epoch: 12 [26240/60000 (44%)]\tLoss: 0.005970\n",
      "Train Epoch: 12 [26880/60000 (45%)]\tLoss: 0.025715\n",
      "Train Epoch: 12 [27520/60000 (46%)]\tLoss: 0.004245\n",
      "Train Epoch: 12 [28160/60000 (47%)]\tLoss: 0.098398\n",
      "Train Epoch: 12 [28800/60000 (48%)]\tLoss: 0.059561\n",
      "Train Epoch: 12 [29440/60000 (49%)]\tLoss: 0.000331\n",
      "Train Epoch: 12 [30080/60000 (50%)]\tLoss: 0.002536\n",
      "Train Epoch: 12 [30720/60000 (51%)]\tLoss: 0.005829\n",
      "Train Epoch: 12 [31360/60000 (52%)]\tLoss: 0.010915\n",
      "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 0.000563\n",
      "Train Epoch: 12 [32640/60000 (54%)]\tLoss: 0.034113\n",
      "Train Epoch: 12 [33280/60000 (55%)]\tLoss: 0.030752\n",
      "Train Epoch: 12 [33920/60000 (57%)]\tLoss: 0.015370\n",
      "Train Epoch: 12 [34560/60000 (58%)]\tLoss: 0.004668\n",
      "Train Epoch: 12 [35200/60000 (59%)]\tLoss: 0.001572\n",
      "Train Epoch: 12 [35840/60000 (60%)]\tLoss: 0.001843\n",
      "Train Epoch: 12 [36480/60000 (61%)]\tLoss: 0.027095\n",
      "Train Epoch: 12 [37120/60000 (62%)]\tLoss: 0.009741\n",
      "Train Epoch: 12 [37760/60000 (63%)]\tLoss: 0.005646\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 0.007217\n",
      "Train Epoch: 12 [39040/60000 (65%)]\tLoss: 0.004725\n",
      "Train Epoch: 12 [39680/60000 (66%)]\tLoss: 0.003182\n",
      "Train Epoch: 12 [40320/60000 (67%)]\tLoss: 0.010894\n",
      "Train Epoch: 12 [40960/60000 (68%)]\tLoss: 0.045330\n",
      "Train Epoch: 12 [41600/60000 (69%)]\tLoss: 0.048238\n",
      "Train Epoch: 12 [42240/60000 (70%)]\tLoss: 0.000990\n",
      "Train Epoch: 12 [42880/60000 (71%)]\tLoss: 0.005966\n",
      "Train Epoch: 12 [43520/60000 (72%)]\tLoss: 0.005567\n",
      "Train Epoch: 12 [44160/60000 (74%)]\tLoss: 0.013546\n",
      "Train Epoch: 12 [44800/60000 (75%)]\tLoss: 0.112730\n",
      "Train Epoch: 12 [45440/60000 (76%)]\tLoss: 0.005204\n",
      "Train Epoch: 12 [46080/60000 (77%)]\tLoss: 0.093161\n",
      "Train Epoch: 12 [46720/60000 (78%)]\tLoss: 0.025974\n",
      "Train Epoch: 12 [47360/60000 (79%)]\tLoss: 0.006596\n",
      "Train Epoch: 12 [48000/60000 (80%)]\tLoss: 0.007201\n",
      "Train Epoch: 12 [48640/60000 (81%)]\tLoss: 0.015587\n",
      "Train Epoch: 12 [49280/60000 (82%)]\tLoss: 0.008517\n",
      "Train Epoch: 12 [49920/60000 (83%)]\tLoss: 0.004884\n",
      "Train Epoch: 12 [50560/60000 (84%)]\tLoss: 0.023048\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.012134\n",
      "Train Epoch: 12 [51840/60000 (86%)]\tLoss: 0.001019\n",
      "Train Epoch: 12 [52480/60000 (87%)]\tLoss: 0.036541\n",
      "Train Epoch: 12 [53120/60000 (88%)]\tLoss: 0.010565\n",
      "Train Epoch: 12 [53760/60000 (90%)]\tLoss: 0.070129\n",
      "Train Epoch: 12 [54400/60000 (91%)]\tLoss: 0.037603\n",
      "Train Epoch: 12 [55040/60000 (92%)]\tLoss: 0.058008\n",
      "Train Epoch: 12 [55680/60000 (93%)]\tLoss: 0.012821\n",
      "Train Epoch: 12 [56320/60000 (94%)]\tLoss: 0.075266\n",
      "Train Epoch: 12 [56960/60000 (95%)]\tLoss: 0.010198\n",
      "Train Epoch: 12 [57600/60000 (96%)]\tLoss: 0.011194\n",
      "Train Epoch: 12 [58240/60000 (97%)]\tLoss: 0.012142\n",
      "Train Epoch: 12 [58880/60000 (98%)]\tLoss: 0.006108\n",
      "Train Epoch: 12 [59520/60000 (99%)]\tLoss: 0.044397\n",
      "\n",
      "Test set: Average loss: 0.0264, Accuracy: 9916/10000 (99%)\n",
      "\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.001206\n",
      "Train Epoch: 13 [640/60000 (1%)]\tLoss: 0.002963\n",
      "Train Epoch: 13 [1280/60000 (2%)]\tLoss: 0.051333\n",
      "Train Epoch: 13 [1920/60000 (3%)]\tLoss: 0.012300\n",
      "Train Epoch: 13 [2560/60000 (4%)]\tLoss: 0.007616\n",
      "Train Epoch: 13 [3200/60000 (5%)]\tLoss: 0.002700\n",
      "Train Epoch: 13 [3840/60000 (6%)]\tLoss: 0.006010\n",
      "Train Epoch: 13 [4480/60000 (7%)]\tLoss: 0.006361\n",
      "Train Epoch: 13 [5120/60000 (9%)]\tLoss: 0.006394\n",
      "Train Epoch: 13 [5760/60000 (10%)]\tLoss: 0.001604\n",
      "Train Epoch: 13 [6400/60000 (11%)]\tLoss: 0.071937\n",
      "Train Epoch: 13 [7040/60000 (12%)]\tLoss: 0.002542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 13 [7680/60000 (13%)]\tLoss: 0.002479\n",
      "Train Epoch: 13 [8320/60000 (14%)]\tLoss: 0.020256\n",
      "Train Epoch: 13 [8960/60000 (15%)]\tLoss: 0.009801\n",
      "Train Epoch: 13 [9600/60000 (16%)]\tLoss: 0.036238\n",
      "Train Epoch: 13 [10240/60000 (17%)]\tLoss: 0.056786\n",
      "Train Epoch: 13 [10880/60000 (18%)]\tLoss: 0.086105\n",
      "Train Epoch: 13 [11520/60000 (19%)]\tLoss: 0.029677\n",
      "Train Epoch: 13 [12160/60000 (20%)]\tLoss: 0.003305\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 0.005562\n",
      "Train Epoch: 13 [13440/60000 (22%)]\tLoss: 0.037524\n",
      "Train Epoch: 13 [14080/60000 (23%)]\tLoss: 0.016089\n",
      "Train Epoch: 13 [14720/60000 (25%)]\tLoss: 0.002275\n",
      "Train Epoch: 13 [15360/60000 (26%)]\tLoss: 0.014272\n",
      "Train Epoch: 13 [16000/60000 (27%)]\tLoss: 0.002734\n",
      "Train Epoch: 13 [16640/60000 (28%)]\tLoss: 0.026553\n",
      "Train Epoch: 13 [17280/60000 (29%)]\tLoss: 0.011818\n",
      "Train Epoch: 13 [17920/60000 (30%)]\tLoss: 0.058890\n",
      "Train Epoch: 13 [18560/60000 (31%)]\tLoss: 0.002339\n",
      "Train Epoch: 13 [19200/60000 (32%)]\tLoss: 0.177723\n",
      "Train Epoch: 13 [19840/60000 (33%)]\tLoss: 0.009397\n",
      "Train Epoch: 13 [20480/60000 (34%)]\tLoss: 0.007097\n",
      "Train Epoch: 13 [21120/60000 (35%)]\tLoss: 0.004715\n",
      "Train Epoch: 13 [21760/60000 (36%)]\tLoss: 0.008475\n",
      "Train Epoch: 13 [22400/60000 (37%)]\tLoss: 0.002863\n",
      "Train Epoch: 13 [23040/60000 (38%)]\tLoss: 0.009779\n",
      "Train Epoch: 13 [23680/60000 (39%)]\tLoss: 0.022856\n",
      "Train Epoch: 13 [24320/60000 (41%)]\tLoss: 0.004966\n",
      "Train Epoch: 13 [24960/60000 (42%)]\tLoss: 0.050235\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.036998\n",
      "Train Epoch: 13 [26240/60000 (44%)]\tLoss: 0.005845\n",
      "Train Epoch: 13 [26880/60000 (45%)]\tLoss: 0.017857\n",
      "Train Epoch: 13 [27520/60000 (46%)]\tLoss: 0.003443\n",
      "Train Epoch: 13 [28160/60000 (47%)]\tLoss: 0.009048\n",
      "Train Epoch: 13 [28800/60000 (48%)]\tLoss: 0.014653\n",
      "Train Epoch: 13 [29440/60000 (49%)]\tLoss: 0.010003\n",
      "Train Epoch: 13 [30080/60000 (50%)]\tLoss: 0.027063\n",
      "Train Epoch: 13 [30720/60000 (51%)]\tLoss: 0.002106\n",
      "Train Epoch: 13 [31360/60000 (52%)]\tLoss: 0.020417\n",
      "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 0.001000\n",
      "Train Epoch: 13 [32640/60000 (54%)]\tLoss: 0.003736\n",
      "Train Epoch: 13 [33280/60000 (55%)]\tLoss: 0.009293\n",
      "Train Epoch: 13 [33920/60000 (57%)]\tLoss: 0.039156\n",
      "Train Epoch: 13 [34560/60000 (58%)]\tLoss: 0.009792\n",
      "Train Epoch: 13 [35200/60000 (59%)]\tLoss: 0.006429\n",
      "Train Epoch: 13 [35840/60000 (60%)]\tLoss: 0.123989\n",
      "Train Epoch: 13 [36480/60000 (61%)]\tLoss: 0.054227\n",
      "Train Epoch: 13 [37120/60000 (62%)]\tLoss: 0.001780\n",
      "Train Epoch: 13 [37760/60000 (63%)]\tLoss: 0.031717\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 0.004809\n",
      "Train Epoch: 13 [39040/60000 (65%)]\tLoss: 0.023320\n",
      "Train Epoch: 13 [39680/60000 (66%)]\tLoss: 0.080258\n",
      "Train Epoch: 13 [40320/60000 (67%)]\tLoss: 0.018058\n",
      "Train Epoch: 13 [40960/60000 (68%)]\tLoss: 0.020848\n",
      "Train Epoch: 13 [41600/60000 (69%)]\tLoss: 0.061261\n",
      "Train Epoch: 13 [42240/60000 (70%)]\tLoss: 0.001866\n",
      "Train Epoch: 13 [42880/60000 (71%)]\tLoss: 0.017106\n",
      "Train Epoch: 13 [43520/60000 (72%)]\tLoss: 0.013353\n",
      "Train Epoch: 13 [44160/60000 (74%)]\tLoss: 0.094873\n",
      "Train Epoch: 13 [44800/60000 (75%)]\tLoss: 0.003371\n",
      "Train Epoch: 13 [45440/60000 (76%)]\tLoss: 0.008501\n",
      "Train Epoch: 13 [46080/60000 (77%)]\tLoss: 0.014435\n",
      "Train Epoch: 13 [46720/60000 (78%)]\tLoss: 0.005466\n",
      "Train Epoch: 13 [47360/60000 (79%)]\tLoss: 0.002805\n",
      "Train Epoch: 13 [48000/60000 (80%)]\tLoss: 0.007681\n",
      "Train Epoch: 13 [48640/60000 (81%)]\tLoss: 0.067510\n",
      "Train Epoch: 13 [49280/60000 (82%)]\tLoss: 0.006815\n",
      "Train Epoch: 13 [49920/60000 (83%)]\tLoss: 0.035181\n",
      "Train Epoch: 13 [50560/60000 (84%)]\tLoss: 0.019378\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.050460\n",
      "Train Epoch: 13 [51840/60000 (86%)]\tLoss: 0.052671\n",
      "Train Epoch: 13 [52480/60000 (87%)]\tLoss: 0.002290\n",
      "Train Epoch: 13 [53120/60000 (88%)]\tLoss: 0.062535\n",
      "Train Epoch: 13 [53760/60000 (90%)]\tLoss: 0.041152\n",
      "Train Epoch: 13 [54400/60000 (91%)]\tLoss: 0.001098\n",
      "Train Epoch: 13 [55040/60000 (92%)]\tLoss: 0.003917\n",
      "Train Epoch: 13 [55680/60000 (93%)]\tLoss: 0.000148\n",
      "Train Epoch: 13 [56320/60000 (94%)]\tLoss: 0.014468\n",
      "Train Epoch: 13 [56960/60000 (95%)]\tLoss: 0.014498\n",
      "Train Epoch: 13 [57600/60000 (96%)]\tLoss: 0.062698\n",
      "Train Epoch: 13 [58240/60000 (97%)]\tLoss: 0.004065\n",
      "Train Epoch: 13 [58880/60000 (98%)]\tLoss: 0.001227\n",
      "Train Epoch: 13 [59520/60000 (99%)]\tLoss: 0.000745\n",
      "\n",
      "Test set: Average loss: 0.0266, Accuracy: 9917/10000 (99%)\n",
      "\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.015470\n",
      "Train Epoch: 14 [640/60000 (1%)]\tLoss: 0.017090\n",
      "Train Epoch: 14 [1280/60000 (2%)]\tLoss: 0.008084\n",
      "Train Epoch: 14 [1920/60000 (3%)]\tLoss: 0.025318\n",
      "Train Epoch: 14 [2560/60000 (4%)]\tLoss: 0.014140\n",
      "Train Epoch: 14 [3200/60000 (5%)]\tLoss: 0.095554\n",
      "Train Epoch: 14 [3840/60000 (6%)]\tLoss: 0.055360\n",
      "Train Epoch: 14 [4480/60000 (7%)]\tLoss: 0.020847\n",
      "Train Epoch: 14 [5120/60000 (9%)]\tLoss: 0.002182\n",
      "Train Epoch: 14 [5760/60000 (10%)]\tLoss: 0.032170\n",
      "Train Epoch: 14 [6400/60000 (11%)]\tLoss: 0.001237\n",
      "Train Epoch: 14 [7040/60000 (12%)]\tLoss: 0.011159\n",
      "Train Epoch: 14 [7680/60000 (13%)]\tLoss: 0.112080\n",
      "Train Epoch: 14 [8320/60000 (14%)]\tLoss: 0.043928\n",
      "Train Epoch: 14 [8960/60000 (15%)]\tLoss: 0.006378\n",
      "Train Epoch: 14 [9600/60000 (16%)]\tLoss: 0.005965\n",
      "Train Epoch: 14 [10240/60000 (17%)]\tLoss: 0.017513\n",
      "Train Epoch: 14 [10880/60000 (18%)]\tLoss: 0.007471\n",
      "Train Epoch: 14 [11520/60000 (19%)]\tLoss: 0.009003\n",
      "Train Epoch: 14 [12160/60000 (20%)]\tLoss: 0.006178\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 0.034965\n",
      "Train Epoch: 14 [13440/60000 (22%)]\tLoss: 0.001001\n",
      "Train Epoch: 14 [14080/60000 (23%)]\tLoss: 0.011836\n",
      "Train Epoch: 14 [14720/60000 (25%)]\tLoss: 0.009963\n",
      "Train Epoch: 14 [15360/60000 (26%)]\tLoss: 0.037457\n",
      "Train Epoch: 14 [16000/60000 (27%)]\tLoss: 0.002756\n",
      "Train Epoch: 14 [16640/60000 (28%)]\tLoss: 0.003975\n",
      "Train Epoch: 14 [17280/60000 (29%)]\tLoss: 0.012041\n",
      "Train Epoch: 14 [17920/60000 (30%)]\tLoss: 0.010375\n",
      "Train Epoch: 14 [18560/60000 (31%)]\tLoss: 0.000860\n",
      "Train Epoch: 14 [19200/60000 (32%)]\tLoss: 0.003366\n",
      "Train Epoch: 14 [19840/60000 (33%)]\tLoss: 0.012219\n",
      "Train Epoch: 14 [20480/60000 (34%)]\tLoss: 0.002816\n",
      "Train Epoch: 14 [21120/60000 (35%)]\tLoss: 0.003229\n",
      "Train Epoch: 14 [21760/60000 (36%)]\tLoss: 0.000984\n",
      "Train Epoch: 14 [22400/60000 (37%)]\tLoss: 0.011725\n",
      "Train Epoch: 14 [23040/60000 (38%)]\tLoss: 0.000202\n",
      "Train Epoch: 14 [23680/60000 (39%)]\tLoss: 0.032860\n",
      "Train Epoch: 14 [24320/60000 (41%)]\tLoss: 0.070438\n",
      "Train Epoch: 14 [24960/60000 (42%)]\tLoss: 0.011595\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.038572\n",
      "Train Epoch: 14 [26240/60000 (44%)]\tLoss: 0.051780\n",
      "Train Epoch: 14 [26880/60000 (45%)]\tLoss: 0.070699\n",
      "Train Epoch: 14 [27520/60000 (46%)]\tLoss: 0.008083\n",
      "Train Epoch: 14 [28160/60000 (47%)]\tLoss: 0.001225\n",
      "Train Epoch: 14 [28800/60000 (48%)]\tLoss: 0.000462\n",
      "Train Epoch: 14 [29440/60000 (49%)]\tLoss: 0.063813\n",
      "Train Epoch: 14 [30080/60000 (50%)]\tLoss: 0.004381\n",
      "Train Epoch: 14 [30720/60000 (51%)]\tLoss: 0.006797\n",
      "Train Epoch: 14 [31360/60000 (52%)]\tLoss: 0.010796\n",
      "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 0.116495\n",
      "Train Epoch: 14 [32640/60000 (54%)]\tLoss: 0.013848\n",
      "Train Epoch: 14 [33280/60000 (55%)]\tLoss: 0.005473\n",
      "Train Epoch: 14 [33920/60000 (57%)]\tLoss: 0.024258\n",
      "Train Epoch: 14 [34560/60000 (58%)]\tLoss: 0.005380\n",
      "Train Epoch: 14 [35200/60000 (59%)]\tLoss: 0.001238\n",
      "Train Epoch: 14 [35840/60000 (60%)]\tLoss: 0.015159\n",
      "Train Epoch: 14 [36480/60000 (61%)]\tLoss: 0.005534\n",
      "Train Epoch: 14 [37120/60000 (62%)]\tLoss: 0.002308\n",
      "Train Epoch: 14 [37760/60000 (63%)]\tLoss: 0.009553\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 0.001388\n",
      "Train Epoch: 14 [39040/60000 (65%)]\tLoss: 0.054800\n",
      "Train Epoch: 14 [39680/60000 (66%)]\tLoss: 0.082784\n",
      "Train Epoch: 14 [40320/60000 (67%)]\tLoss: 0.015374\n",
      "Train Epoch: 14 [40960/60000 (68%)]\tLoss: 0.007163\n",
      "Train Epoch: 14 [41600/60000 (69%)]\tLoss: 0.030913\n",
      "Train Epoch: 14 [42240/60000 (70%)]\tLoss: 0.004643\n",
      "Train Epoch: 14 [42880/60000 (71%)]\tLoss: 0.035403\n",
      "Train Epoch: 14 [43520/60000 (72%)]\tLoss: 0.061531\n",
      "Train Epoch: 14 [44160/60000 (74%)]\tLoss: 0.001280\n",
      "Train Epoch: 14 [44800/60000 (75%)]\tLoss: 0.008375\n",
      "Train Epoch: 14 [45440/60000 (76%)]\tLoss: 0.004609\n",
      "Train Epoch: 14 [46080/60000 (77%)]\tLoss: 0.004650\n",
      "Train Epoch: 14 [46720/60000 (78%)]\tLoss: 0.049854\n",
      "Train Epoch: 14 [47360/60000 (79%)]\tLoss: 0.001507\n",
      "Train Epoch: 14 [48000/60000 (80%)]\tLoss: 0.002232\n",
      "Train Epoch: 14 [48640/60000 (81%)]\tLoss: 0.035669\n",
      "Train Epoch: 14 [49280/60000 (82%)]\tLoss: 0.000408\n",
      "Train Epoch: 14 [49920/60000 (83%)]\tLoss: 0.007884\n",
      "Train Epoch: 14 [50560/60000 (84%)]\tLoss: 0.002319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.028309\n",
      "Train Epoch: 14 [51840/60000 (86%)]\tLoss: 0.014011\n",
      "Train Epoch: 14 [52480/60000 (87%)]\tLoss: 0.001878\n",
      "Train Epoch: 14 [53120/60000 (88%)]\tLoss: 0.035631\n",
      "Train Epoch: 14 [53760/60000 (90%)]\tLoss: 0.044884\n",
      "Train Epoch: 14 [54400/60000 (91%)]\tLoss: 0.008772\n",
      "Train Epoch: 14 [55040/60000 (92%)]\tLoss: 0.013300\n",
      "Train Epoch: 14 [55680/60000 (93%)]\tLoss: 0.002793\n",
      "Train Epoch: 14 [56320/60000 (94%)]\tLoss: 0.003425\n",
      "Train Epoch: 14 [56960/60000 (95%)]\tLoss: 0.004649\n",
      "Train Epoch: 14 [57600/60000 (96%)]\tLoss: 0.006291\n",
      "Train Epoch: 14 [58240/60000 (97%)]\tLoss: 0.058299\n",
      "Train Epoch: 14 [58880/60000 (98%)]\tLoss: 0.010471\n",
      "Train Epoch: 14 [59520/60000 (99%)]\tLoss: 0.000436\n",
      "\n",
      "Test set: Average loss: 0.0262, Accuracy: 9919/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            if args.dry_run:\n",
    "                break\n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca2374f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd3b749",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e32ba1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7d4dc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5847a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec6197d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a9955f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "means\n",
      "[1.213796   5.86499022 7.03832544]\n",
      "\n",
      "covariance matrix\n",
      "[[ 0.77620761  2.66301628  0.64372258]\n",
      " [ 2.66301628  9.59372956  2.05562816]\n",
      " [ 0.64372258  2.05562816 11.26476218]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.213796  , 5.86499022, 7.03832544])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(37)\n",
    "\n",
    "N = 10\n",
    "\n",
    "x1 = np.random.normal(1, 1, N)\n",
    "x2 = np.random.normal(1 + 3.5 * x1, 1, N)\n",
    "x3 = np.random.normal(8, 3, N)\n",
    "\n",
    "data = np.vstack([x1, x2, x3]).T\n",
    "means = data.mean(axis=0)\n",
    "cov = np.cov(data.T)\n",
    "\n",
    "print('means')\n",
    "print(means)\n",
    "print('')\n",
    "print('covariance matrix')\n",
    "print(cov)\n",
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ca5a414d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc4f98a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2a9332a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "\n",
    "df = pd.DataFrame(data, columns=['x1', 'x2', 'x3'])\n",
    "df.plot.kde(ax=ax)\n",
    "\n",
    "_ = ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9881713d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "# marginal PDFs\n",
    "d1 = multivariate_normal(means[0], cov[0, 0])\n",
    "d2 = multivariate_normal(means[1], cov[1, 1])\n",
    "d3 = multivariate_normal(means[2], cov[2, 2])\n",
    "\n",
    "# joint PDFs\n",
    "jd12 = multivariate_normal(means[[0, 1]], cov[[0, 1]][:, [0, 1]])\n",
    "jd13 = multivariate_normal(means[[0, 2]], cov[[0, 2]][:, [0, 2]])\n",
    "jd23 = multivariate_normal(means[[1, 2]], cov[[1, 2]][:, [1, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c10affa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 100\n"
     ]
    }
   ],
   "source": [
    "x1_vals = np.linspace(x1.min(), x1.max(), num=100, endpoint=True)\n",
    "x2_vals = np.linspace(x2.min(), x2.max(), num=100, endpoint=True)\n",
    "x3_vals = np.linspace(x3.min(), x3.max(), num=100, endpoint=True)\n",
    "\n",
    "print(len(x1_vals), len(x2_vals), len(x3_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3087d51f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "394.3706103509655"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "# x1 and X2\n",
    "triplets = ((jd12.pdf(tup), d1.pdf(tup[0]), d2.pdf(tup[1])) for tup in itertools.product(*[x1_vals, x2_vals]))\n",
    "np.sum([p_xy * (np.log(p_xy) - np.log(p_x) - np.log(p_y)) for p_xy, p_x, p_y in triplets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2112f225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.49817190334607"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x1 and X3\n",
    "triplets = ((jd13.pdf(tup), d1.pdf(tup[0]), d3.pdf(tup[1])) for tup in itertools.product(*[x1_vals, x3_vals]))\n",
    "np.sum([p_xy * (np.log(p_xy) - np.log(p_x) - np.log(p_y)) for p_xy, p_x, p_y in triplets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "df94a7a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8399862719286941"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X2 and X3\n",
    "triplets = ((jd23.pdf(tup), d2.pdf(tup[0]), d3.pdf(tup[1])) for tup in itertools.product(*[x2_vals, x3_vals]))\n",
    "np.sum([p_xy * (np.log(p_xy) - np.log(p_x) - np.log(p_y)) for p_xy, p_x, p_y in triplets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d674cac4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8a6e65ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# values1 = np.array([0.5, 1, 0.5, 0  , 0, 0  , 0, 0,0])\n",
    "# values2 = np.array([0  , 0, 0  , 0.5, 1, 0.5, 0, 0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1f1aa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a8eca6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "536398fd",
   "metadata": {
    "code_folding": [
     38,
     79
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# import skimage.io\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "\n",
    "\n",
    "class MutualInformation(nn.Module):\n",
    "\n",
    "    def __init__(self, sigma=0.1, num_bins=256, normalize=True):\n",
    "        super(MutualInformation, self).__init__()\n",
    "\n",
    "        self.sigma = sigma\n",
    "        self.num_bins = num_bins\n",
    "        self.normalize = normalize\n",
    "        self.epsilon = 1e-10\n",
    "\n",
    "        self.bins = nn.Parameter(torch.linspace(0, 1, num_bins, device=device).float(), requires_grad=False)\n",
    "\n",
    "    def marginalPdf(self, values):\n",
    "\n",
    "        residuals = values - self.bins #.unsqueeze(0).unsqueeze(0)\n",
    "        kernel_values = torch.exp(-0.5*(residuals / self.sigma).pow(2))\n",
    "\n",
    "        pdf = torch.mean(kernel_values, dim=1)\n",
    "        normalization = torch.sum(pdf, dim=0) + self.epsilon\n",
    "        pdf = pdf / normalization\n",
    "\n",
    "        return pdf, kernel_values\n",
    "\n",
    "    def jointPdf(self, kernel_values1, kernel_values2):\n",
    "\n",
    "        joint_kernel_values = torch.matmul(kernel_values1.transpose(1, 2), kernel_values2) \n",
    "        normalization = torch.sum(joint_kernel_values, dim=(1,2)).view(-1, 1, 1) + self.epsilon\n",
    "        pdf = joint_kernel_values / normalization\n",
    "\n",
    "        return pdf\n",
    "\n",
    "    def getMutualInformation(self, input1, input2):\n",
    "        '''\n",
    "            input1: B, C, H, W\n",
    "            input2: B, C, H, W\n",
    "\n",
    "            return: scalar\n",
    "        '''\n",
    "\n",
    "        # Torch tensors for images between (0, 1)\n",
    "#         input1 = input1*255\n",
    "#         input2 = input2*255\n",
    "\n",
    "#         B, C, H, W = input1.shape\n",
    "#         assert((input1.shape == input2.shape))\n",
    "\n",
    "#         x1 = input1.view(B, H*W, C)\n",
    "#         x2 = input2.view(B, H*W, C)\n",
    "        \n",
    "\n",
    "        pdf_x1, kernel_values1 = self.marginalPdf(input1)\n",
    "#         pdf_x2, kernel_values2 = self.marginalPdf(input2)\n",
    "#         pdf_x1x2 = self.jointPdf(kernel_values1, kernel_values2)\n",
    "\n",
    "#         H_x1 = -torch.sum(pdf_x1*torch.log2(pdf_x1 + self.epsilon), dim=1)\n",
    "#         H_x2 = -torch.sum(pdf_x2*torch.log2(pdf_x2 + self.epsilon), dim=1)\n",
    "#         H_x1x2 = -torch.sum(pdf_x1x2*torch.log2(pdf_x1x2 + self.epsilon), dim=(1,2))\n",
    "\n",
    "#         mutual_information = H_x1 + H_x2 - H_x1x2\n",
    "\n",
    "#         if self.normalize:\n",
    "#             mutual_information = 2*mutual_information/(H_x1+H_x2)\n",
    "\n",
    "#         return mutual_information\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        '''\n",
    "            input1: B, C, H, W\n",
    "            input2: B, C, H, W\n",
    "\n",
    "            return: scalar\n",
    "        '''\n",
    "        return self.getMutualInformation(input1, input2)\n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "\n",
    "#     device = 'cpu'\n",
    "\n",
    "#     ### Create test cases ###\n",
    "# #     img1 = Image.open('grad.jpg').convert('L')\n",
    "# #     img2 = img1.rotate(10)\n",
    "\n",
    "# #     arr1 = np.array(img1)\n",
    "# #     arr2 = np.array(img2)\n",
    "\n",
    "#     mi_true_1 = normalized_mutual_info_score(values1, values1)\n",
    "#     mi_true_2 = normalized_mutual_info_score(values2, values2)\n",
    "\n",
    "#     img1 = transforms.ToTensor() (img1).unsqueeze(dim=0).to(device)\n",
    "#     img2 = transforms.ToTensor() (img2).unsqueeze(dim=0).to(device)\n",
    "\n",
    "#     # Pair of different images, pair of same images\n",
    "#     input1 = torch.cat([img1, img2])\n",
    "#     input2 = torch.cat([img2, img2])\n",
    "\n",
    "    \n",
    "\n",
    "#     mi_test_1 = mi_test[0].cpu().numpy()\n",
    "#     mi_test_2 = mi_test[1].cpu().numpy()\n",
    "\n",
    "#     print('Image Pair 1 | sklearn MI: {}, this MI: {}'.format(mi_true_1, mi_test_1))\n",
    "#     print('Image Pair 2 | sklearn MI: {}, this MI: {}'.format(mi_true_2, mi_test_2))\n",
    "\n",
    "#     assert(np.abs(mi_test_1 - mi_true_1) < 0.05)\n",
    "#     assert(np.abs(mi_test_2 - mi_true_2) < 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "32fb4518",
   "metadata": {},
   "outputs": [],
   "source": [
    "MI = MutualInformation(num_bins=128, sigma=0.1, normalize=True).to(device)\n",
    "# mi_test = MI(values1, values2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a9cde52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "values1 = torch.Tensor(np.expand_dims(np.random.rand(128), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "940fbc0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fed68921",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_x1, kernel_values1 = MI.marginalPdf(values1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8027482e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea684ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18059476",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7b31afd3",
   "metadata": {},
   "outputs": [],
   "source": [
    " residuals = torch.Tensor(np.random.rand(128)) - MI.bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fba6b238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.8278,  0.1639,  0.1785,  0.6819,  0.3485,  0.3345,  0.3552,  0.9083,\n",
       "         0.1181,  0.5284,  0.2619, -0.0493,  0.8409,  0.1621,  0.0155,  0.5363,\n",
       "         0.7198,  0.7340,  0.3963,  0.7220,  0.4109,  0.3908, -0.0760,  0.0980,\n",
       "        -0.1552,  0.7210,  0.5210,  0.1158,  0.7175,  0.1726,  0.2964,  0.1104,\n",
       "        -0.2404,  0.3140, -0.0062, -0.0323, -0.1098,  0.0818,  0.2908, -0.1560,\n",
       "         0.1668,  0.0644,  0.2533,  0.3859, -0.0768,  0.0796,  0.3341,  0.2856,\n",
       "         0.1213,  0.1830,  0.1340,  0.3472, -0.1011, -0.2193,  0.4479, -0.4123,\n",
       "         0.3070,  0.0503,  0.0047, -0.3552,  0.1070,  0.0171,  0.0234, -0.0300,\n",
       "         0.0062,  0.2516,  0.1059,  0.3219, -0.2370,  0.4442, -0.1247, -0.3405,\n",
       "        -0.4074, -0.3714, -0.1972, -0.3810, -0.0354, -0.4055, -0.1022,  0.3308,\n",
       "        -0.6187, -0.3682, -0.1425, -0.2006,  0.1941,  0.0229,  0.0700, -0.5826,\n",
       "         0.0935,  0.1701, -0.0561, -0.5208, -0.2593,  0.1190, -0.4561, -0.2596,\n",
       "         0.1460, -0.0159, -0.1068, -0.5403, -0.6290,  0.0018, -0.3815, -0.3407,\n",
       "        -0.4182, -0.2394, -0.0955, -0.5792, -0.1336,  0.0146, -0.4475, -0.1980,\n",
       "        -0.1774, -0.8762, -0.7796, -0.0734, -0.0904, -0.3129,  0.0120, -0.3955,\n",
       "        -0.7817, -0.7238, -0.5274, -0.0919, -0.0196,  0.0026, -0.7454, -0.8063])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e625af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e881cf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d921a618",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2b3bde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37dec1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0beff3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cb1bf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930451fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa85ce83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6be78d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8121749f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b393ac2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61505fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f760bdde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca7452e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e6fb35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d9aefd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9608e8ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609b8e47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93710396",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
