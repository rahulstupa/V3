{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f86eb28c",
   "metadata": {},
   "source": [
    "# TTNET Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0573fe36",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3a84b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d228fb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd5f6a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from turbojpeg import TurboJPEG\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from data_process.ttnet_data_utils import load_raw_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e7f38e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Dataset\n",
    "from utils.misc import *\n",
    "from utils.logger import Logger\n",
    "from config.config import parse_configs\n",
    "from utils.train_utils import create_optimizer, create_lr_scheduler,  reduce_tensor, to_python_float, get_saved_state, save_checkpoint\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0317525",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2928a482",
   "metadata": {},
   "source": [
    "### Configs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac6e3ff",
   "metadata": {},
   "source": [
    "##### Phase 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c97da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configs = parse_configs()\n",
    "# configs.distributed = False\n",
    "# # configs.multitask_learning = True\n",
    "\n",
    "# # Phase 1\n",
    "# configs.smooth_labelling = True\n",
    "# configs.global_weight    = 5\n",
    "# configs.lr_factor        = 0.5\n",
    "# configs.saved_fn         = 'ttnet_1st_phase_wtt'\n",
    "# configs.no_val           = True\n",
    "# configs.lr               = 0.001 \n",
    "# configs.lr_type          = 'step_lr' \n",
    "# configs.lr_step_size     = 10 \n",
    "# configs.lr_factor        = 0.1\n",
    "# configs.gpu_idx          = 0 \n",
    "# configs.global_weight    = 5. \n",
    "# configs.no_event         = True\n",
    "# configs.no_local         = True\n",
    "# configs.print_freq       = 500\n",
    "# configs.batch_size       = 24\n",
    "# configs.seg_weight       = 1.\n",
    "# configs.num_workers      = 8\n",
    "# configs.smooth_labelling = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb357e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0817abe0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92e4258e",
   "metadata": {},
   "source": [
    "##### Phase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f3e486f",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# # Phase 2\n",
    "configs = parse_configs()\n",
    "configs.distributed = False\n",
    "# configs.multitask_learning = True\n",
    "\n",
    "#Phase 2\n",
    "configs.batch_size = 24\n",
    "configs.num_workers = 8\n",
    "configs.saved_fn  = 'ttnet_2nd_phase_wtt' \n",
    "configs.no_val = True  \n",
    "configs.lr = 0.001 \n",
    "configs.lr_type = 'step_lr' \n",
    "configs.lr_step_size =  10 \n",
    "configs.lr_factor = 0.1 \n",
    "configs.gpu_idx = 0 \n",
    "configs.global_weight = 0. \n",
    "configs.event_weight = 2. \n",
    "configs.local_weight = 1. \n",
    "configs.pretrained_path = \"../../checkpoints/ttnet/ttnet_1st_phase_wtt_epoch_30.pth\"\n",
    "configs.overwrite_global_2_local  = True\n",
    "configs.freeze_global  = True\n",
    "configs.smooth_labelling = True\n",
    "configs.sigma =  1.0\n",
    "configs.print_freq =  1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc60649a",
   "metadata": {},
   "source": [
    "##### Phase 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70db7d9",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# # phase 3\n",
    "# configs = parse_configs()\n",
    "# configs.distributed = False\n",
    "# configs.batch_size = 24\n",
    "# configs.num_workers = 8\n",
    "# configs.saved_fn  = 'ttnet_3nd_phase' \n",
    "# configs.no_val = True  \n",
    "# configs.lr = 0.0001 \n",
    "# configs.lr_type = 'step_lr' \n",
    "# configs.lr_step_size =  10 \n",
    "# configs.lr_factor = 0.2 \n",
    "# configs.gpu_idx = 0 \n",
    "# configs.global_weight = 1. \n",
    "# configs.event_weight = 1. \n",
    "# configs.local_weight = 1. \n",
    "# configs.pretrained_path = \"../../checkpoints/ttnet/ttnet_2nd_phase_epoch_30.pth\"\n",
    "# configs.smooth_labelling = True\n",
    "# configs.sigma =  1.0\n",
    "# configs.print_freq =  1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc681181",
   "metadata": {},
   "source": [
    "##### Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f89947d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-18 18:34:44,283: logger.py - info(), at Line 38:INFO:\n",
      ">>> Created a new logger\n",
      "2022-12-18 18:34:44,285: logger.py - info(), at Line 38:INFO:\n",
      ">>> configs: {'seed': 2020, 'saved_fn': 'ttnet_2nd_phase_wtt', 'arch': 'ttnet', 'dropout_p': 0.5, 'multitask_learning': False, 'no_local': False, 'no_event': False, 'pretrained_path': '../../checkpoints/ttnet/ttnet_1st_phase_wtt_epoch_30.pth', 'overwrite_global_2_local': True, 'working_dir': '../../', 'no_val': True, 'no_test': False, 'val_size': 0.2, 'smooth_labelling': True, 'num_samples': None, 'num_workers': 8, 'batch_size': 24, 'print_freq': 1000, 'checkpoint_freq': 2, 'sigma': 1.0, 'thresh_ball_pos_mask': 0.05, 'start_epoch': 1, 'num_epochs': 30, 'lr': 0.001, 'minimum_lr': 1e-07, 'momentum': 0.9, 'weight_decay': 0.0, 'optimizer_type': 'adam', 'lr_type': 'step_lr', 'lr_factor': 0.1, 'lr_step_size': 10, 'lr_patience': 3, 'earlystop_patience': None, 'freeze_global': True, 'freeze_local': False, 'freeze_event': False, 'global_weight': 0.0, 'local_weight': 1.0, 'event_weight': 2.0, 'world_size': -1, 'rank': -1, 'dist_url': 'tcp://127.0.0.1:29500', 'dist_backend': 'nccl', 'gpu_idx': 0, 'no_cuda': False, 'multiprocessing_distributed': False, 'evaluate': False, 'resume_path': None, 'use_best_checkpoint': True, 'event_thresh': 0.5, 'save_test_output': False, 'video_path': None, 'output_format': 'text', 'show_image': False, 'save_demo_output': False, 'device': device(type='cuda'), 'ngpus_per_node': 1, 'pin_memory': True, 'dataset_dir': '../../dataset', 'train_game_list': ['game_1', 'game_2', 'game_3', 'game_4', 'game_5'], 'test_game_list': ['test_1', 'test_2', 'test_3', 'test_4', 'test_5', 'test_6', 'test_7'], 'events_dict': {'bounce': 0, 'net': 1, 'empty_event': 2}, 'events_weights_loss_dict': {'bounce': 1.0, 'net': 3.0}, 'events_weights_loss': [1.0, 3.0], 'num_events': 2, 'num_frames_sequence': 9, 'org_size': [1920, 1080], 'input_size': [320, 128], 'tasks': ['global', 'local', 'event'], 'tasks_loss_weight': [1.0, 1.0, 1.0], 'freeze_modules_list': [], 'checkpoints_dir': '../../checkpoints/ttnet', 'logs_dir': '../../logs/ttnet', 'saved_weight_name': '../../checkpoints/ttnet/ttnet_best.pth', 'results_dir': '../../results', 'distributed': False}\n"
     ]
    }
   ],
   "source": [
    "logger = Logger(configs.logs_dir, configs.saved_fn)\n",
    "logger.info('>>> Created a new logger')\n",
    "logger.info('>>> configs: {}'.format(configs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7084f0",
   "metadata": {},
   "source": [
    "### Dataloader Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b9f0f5",
   "metadata": {},
   "source": [
    "##### TTNet Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60df1e14",
   "metadata": {
    "code_folding": [
     13,
     16,
     19,
     24
    ]
   },
   "outputs": [],
   "source": [
    "class TTNet_Dataset(Dataset):\n",
    "    def __init__(self, events_infor, org_size, input_size, transform=None, num_samples=None):\n",
    "        self.events_infor = events_infor\n",
    "        self.w_org = org_size[0]\n",
    "        self.h_org = org_size[1]\n",
    "        self.w_input = input_size[0]\n",
    "        self.h_input = input_size[1]\n",
    "        self.w_resize_ratio = self.w_org / self.w_input\n",
    "        self.h_resize_ratio = self.h_org / self.h_input\n",
    "        self.transform = transform\n",
    "        if num_samples is not None:\n",
    "            self.events_infor = self.events_infor[:num_samples]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.events_infor)\n",
    "\n",
    "    def __resize_ball_pos__(self, ball_pos_xy, w_ratio, h_ratio):\n",
    "        return np.array([ball_pos_xy[0] / w_ratio, ball_pos_xy[1] / h_ratio])\n",
    "\n",
    "    def __check_ball_pos__(self, ball_pos_xy, w, h):\n",
    "        if not ((0 < ball_pos_xy[0] < w) and (0 < ball_pos_xy[1] < h)):\n",
    "            ball_pos_xy[0] = -1.\n",
    "            ball_pos_xy[1] = -1.\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path_list, org_ball_pos_xy, target_events, seg_path = self.events_infor[index]\n",
    "        # Load segmentation\n",
    "        seg_img = load_raw_img(seg_path)\n",
    "        self.jpeg_reader = TurboJPEG()  # improve it later (Only initialize it once)\n",
    "        # Load a sequence of images (-4, 4), resize images before stacking them together\n",
    "        # Use TurboJPEG to speed up the loading images' phase\n",
    "        resized_imgs = []\n",
    "        for img_path in img_path_list:\n",
    "            in_file = open(img_path, 'rb')\n",
    "            resized_imgs.append(cv2.resize(self.jpeg_reader.decode(in_file.read(), 0), (self.w_input, self.h_input)))\n",
    "            in_file.close()\n",
    "        resized_imgs = np.dstack(resized_imgs)  # (128, 320, 27)\n",
    "        # Adjust ball pos: full HD --> (320, 128)\n",
    "        global_ball_pos_xy = self.__resize_ball_pos__(org_ball_pos_xy, self.w_resize_ratio, self.h_resize_ratio)\n",
    "\n",
    "        # Apply augmentation\n",
    "        if self.transform:\n",
    "            resized_imgs, global_ball_pos_xy, seg_img = self.transform(resized_imgs, global_ball_pos_xy, seg_img)\n",
    "        # Adjust ball pos: (320, 128) --> full HD\n",
    "        org_ball_pos_xy = self.__resize_ball_pos__(global_ball_pos_xy, 1. / self.w_resize_ratio,\n",
    "                                                   1. / self.h_resize_ratio)\n",
    "        # If the ball position is outside of the resized image, set position to -1, -1 --> No ball (just for safety)\n",
    "        self.__check_ball_pos__(org_ball_pos_xy, self.w_org, self.h_org)\n",
    "        self.__check_ball_pos__(global_ball_pos_xy, self.w_input, self.h_input)\n",
    "\n",
    "        # Transpose (H, W, C) to (C, H, W) --> fit input of Pytorch model\n",
    "        resized_imgs = resized_imgs.transpose(2, 0, 1)\n",
    "        target_seg = seg_img.transpose(2, 0, 1).astype(np.float)\n",
    "        # Segmentation mask should be 0 or 1\n",
    "        target_seg[target_seg < 75] = 0.\n",
    "        target_seg[target_seg >= 75] = 1.\n",
    "\n",
    "        return resized_imgs, org_ball_pos_xy.astype(np.int), global_ball_pos_xy.astype(np.int), target_events, target_seg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecc1e18",
   "metadata": {},
   "source": [
    "##### Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7170c1e7",
   "metadata": {
    "code_folding": [
     1,
     11,
     22,
     33,
     51,
     90,
     117,
     118
    ]
   },
   "outputs": [],
   "source": [
    "## Transformations\n",
    "class Compose(object):\n",
    "    def __init__(self, transforms, p=1.0):\n",
    "        self.transforms = transforms\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, imgs, ball_position_xy, seg_img):\n",
    "        if random.random() <= self.p:\n",
    "            for t in self.transforms:\n",
    "                imgs, ball_position_xy, seg_img = t(imgs, ball_position_xy, seg_img)\n",
    "        return imgs, ball_position_xy, seg_img\n",
    "class Normalize():\n",
    "    def __init__(self, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), num_frames_sequence=9, p=1.0):\n",
    "        self.p = p\n",
    "        self.mean = np.repeat(np.array(mean).reshape(1, 1, 3), repeats=num_frames_sequence, axis=-1)\n",
    "        self.std = np.repeat(np.array(std).reshape(1, 1, 3), repeats=num_frames_sequence, axis=-1)\n",
    "\n",
    "    def __call__(self, imgs, ball_position_xy, seg_img):\n",
    "        if random.random() < self.p:\n",
    "            imgs = ((imgs / 255.) - self.mean) / self.std\n",
    "\n",
    "        return imgs, ball_position_xy, seg_img\n",
    "class Denormalize():\n",
    "    def __init__(self, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), p=1.0):\n",
    "        self.p = p\n",
    "        self.mean = np.array(mean).reshape(1, 1, 3)\n",
    "        self.std = np.array(std).reshape(1, 1, 3)\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img = (img * self.std + self.mean) * 255.\n",
    "        img = img.astype(np.uint8)\n",
    "\n",
    "        return img\n",
    "class Resize(object):\n",
    "    def __init__(self, new_size, p=0.5, interpolation=cv2.INTER_LINEAR):\n",
    "        self.new_size = new_size\n",
    "        self.p = p\n",
    "        self.interpolation = interpolation\n",
    "\n",
    "    def __call__(self, imgs, ball_position_xy, seg_img):\n",
    "        if random.random() <= self.p:\n",
    "            h, w, c = imgs.shape\n",
    "            # Resize a sequence of images\n",
    "            imgs = cv2.resize(imgs, self.new_size, interpolation=self.interpolation)\n",
    "            # Dont need to resize seg_img\n",
    "            # Adjust ball position\n",
    "            w_ratio = w / self.new_size[0]\n",
    "            h_ratio = h / self.new_size[1]\n",
    "            ball_position_xy = np.array([ball_position_xy[0] / w_ratio, ball_position_xy[1] / h_ratio])\n",
    "\n",
    "        return imgs, ball_position_xy, seg_img\n",
    "class Random_Crop(object):\n",
    "    def __init__(self, max_reduction_percent=0.15, p=0.5, interpolation=cv2.INTER_LINEAR):\n",
    "        self.max_reduction_percent = max_reduction_percent\n",
    "        self.p = p\n",
    "        self.interpolation = interpolation\n",
    "\n",
    "    def __call__(self, imgs, ball_position_xy, seg_img):\n",
    "        # imgs are before resizing\n",
    "        if random.random() <= self.p:\n",
    "            h, w, c = imgs.shape\n",
    "            # Calculate min_x, max_x, min_y, max_y\n",
    "            remain_percent = random.uniform(1. - self.max_reduction_percent, 1.)\n",
    "            new_w = remain_percent * w\n",
    "            min_x = int(random.uniform(0, w - new_w))\n",
    "            max_x = int(min_x + new_w)\n",
    "            w_ratio = w / new_w\n",
    "\n",
    "            new_h = remain_percent * h\n",
    "            min_y = int(random.uniform(0, h - new_h))\n",
    "            max_y = int(new_h + min_y)\n",
    "            h_ratio = h / new_h\n",
    "            # crop a sequence of images\n",
    "            imgs = imgs[min_y:max_y, min_x:max_x, :]\n",
    "            imgs = cv2.resize(imgs, (w, h), interpolation=self.interpolation)\n",
    "            # crop seg_img\n",
    "            seg_img_h, seg_img_w, _ = seg_img.shape\n",
    "            # 1. Resize to original\n",
    "            if (seg_img_h != h) or (seg_img_w != w):\n",
    "                seg_img = cv2.resize(seg_img, (w, h), interpolation=self.interpolation)\n",
    "            # 2. Crop\n",
    "            seg_img = seg_img[min_y:max_y, min_x:max_x, :]\n",
    "            # 3. Resize to (128, 320, 3)\n",
    "            seg_img = cv2.resize(seg_img, (seg_img_w, seg_img_h), interpolation=self.interpolation)\n",
    "\n",
    "            # Adjust ball position\n",
    "            ball_position_xy = np.array([(ball_position_xy[0] - min_x) * w_ratio,\n",
    "                                         (ball_position_xy[1] - min_y) * h_ratio])\n",
    "\n",
    "        return imgs, ball_position_xy, seg_img\n",
    "class Random_Rotate(object):\n",
    "    def __init__(self, rotation_angle_limit=15, p=0.5):\n",
    "        self.rotation_angle_limit = rotation_angle_limit\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, imgs, ball_position_xy, seg_img):\n",
    "        if random.random() <= self.p:\n",
    "            random_angle = random.uniform(-self.rotation_angle_limit, self.rotation_angle_limit)\n",
    "            # Rotate a sequence of imgs\n",
    "            h, w, c = imgs.shape\n",
    "            center = (int(w / 2), int(h / 2))\n",
    "            rotate_matrix = cv2.getRotationMatrix2D(center, random_angle, 1.)\n",
    "            imgs = cv2.warpAffine(imgs, rotate_matrix, (w, h), flags=cv2.INTER_LINEAR)\n",
    "\n",
    "            # Adjust ball position, apply the same rotate_matrix for the sequential images\n",
    "            ball_position_xy = rotate_matrix.dot(np.array([ball_position_xy[0], ball_position_xy[1], 1.]).T)\n",
    "\n",
    "            # Rotate seg_img\n",
    "            seg_h, seg_w, seg_c = seg_img.shape\n",
    "            if (seg_h != h) or (seg_w != w):\n",
    "                seg_center = (int(seg_w / 2), int(seg_h / 2))\n",
    "                seg_rotate_matrix = cv2.getRotationMatrix2D(seg_center, random_angle, 1.)\n",
    "            else:\n",
    "                seg_rotate_matrix = rotate_matrix\n",
    "            seg_img = cv2.warpAffine(seg_img, seg_rotate_matrix, (seg_w, seg_h), flags=cv2.INTER_LINEAR)\n",
    "\n",
    "        return imgs, ball_position_xy, seg_img\n",
    "class Random_HFlip(object):\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, imgs, ball_position_xy, seg_img):\n",
    "        if random.random() <= self.p:\n",
    "            h, w, c = imgs.shape\n",
    "            # Horizontal flip a sequence of imgs\n",
    "            imgs = cv2.flip(imgs, 1)\n",
    "            # Horizontal flip seg_img\n",
    "            seg_img = cv2.flip(seg_img, 1)\n",
    "\n",
    "            # Adjust ball position: Same y, new x = w - x\n",
    "            ball_position_xy[0] = w - ball_position_xy[0]\n",
    "\n",
    "        return imgs, ball_position_xy, seg_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd45187",
   "metadata": {},
   "source": [
    "##### TTNet Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2855c86",
   "metadata": {
    "code_folding": [
     0,
     4,
     8,
     33,
     40,
     105
    ]
   },
   "outputs": [],
   "source": [
    "def load_raw_img(img_path):\n",
    "    \"\"\"Load raw image based on the path to the image\"\"\"\n",
    "    img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)  # BGR --> RGB\n",
    "    return img\n",
    "def gaussian_1d(pos, muy, sigma):\n",
    "    \"\"\"Create 1D Gaussian distribution based on ball position (muy), and std (sigma)\"\"\"\n",
    "    target = torch.exp(- (((pos - muy) / sigma) ** 2) / 2)\n",
    "    return target\n",
    "def create_target_ball(ball_position_xy, sigma, w, h, thresh_mask, device):\n",
    "    \"\"\"Create target for the ball detection stages\n",
    "\n",
    "    :param ball_position_xy: Position of the ball (x,y)\n",
    "    :param sigma: standard deviation (a hyperparameter)\n",
    "    :param w: width of the resize image\n",
    "    :param h: height of the resize image\n",
    "    :param thresh_mask: if values of 1D Gaussian < thresh_mask --> set to 0 to reduce computation\n",
    "    :param device: cuda() or cpu()\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    w, h = int(w), int(h)\n",
    "    target_ball_position = torch.zeros((w + h,), device=device)\n",
    "    # Only do the next step if the ball is existed\n",
    "    if (w > ball_position_xy[0] > 0) and (h > ball_position_xy[1] > 0):\n",
    "        # For x\n",
    "        x_pos = torch.arange(0, w, device=device)\n",
    "        target_ball_position[:w] = gaussian_1d(x_pos, ball_position_xy[0], sigma=sigma)\n",
    "        # For y\n",
    "        y_pos = torch.arange(0, h, device=device)\n",
    "        target_ball_position[w:] = gaussian_1d(y_pos, ball_position_xy[1], sigma=sigma)\n",
    "\n",
    "        target_ball_position[target_ball_position < thresh_mask] = 0.\n",
    "\n",
    "    return target_ball_position\n",
    "def smooth_event_labelling(event_class, smooth_idx, event_frameidx):\n",
    "    target_events = np.zeros((2,))\n",
    "    if event_class < 2:\n",
    "        n = smooth_idx - event_frameidx\n",
    "        target_events[event_class] = np.cos(n * np.pi / 8)\n",
    "        target_events[target_events < 0.01] = 0.\n",
    "    return target_events\n",
    "def get_events_infor(game_list, configs, dataset_type):\n",
    "    \"\"\"Get information of sequences of images based on events\n",
    "\n",
    "    :param game_list: List of games (video names)\n",
    "    :return:\n",
    "    [\n",
    "        each event: [[img_path_list], ball_position, target_events, segmentation_path]\n",
    "    ]\n",
    "    \"\"\"\n",
    "    # the paper mentioned 25, but used 9 frames only\n",
    "    num_frames_from_event = int((configs.num_frames_sequence - 1) / 2)\n",
    "\n",
    "    annos_dir = os.path.join(configs.dataset_dir, dataset_type, 'annotations')\n",
    "    images_dir = os.path.join(configs.dataset_dir, dataset_type, 'images')\n",
    "    events_infor = []\n",
    "    events_labels = []\n",
    "    for game_name in game_list:\n",
    "        ball_annos_path = os.path.join(annos_dir, game_name, 'ball_markup.json')\n",
    "        events_annos_path = os.path.join(annos_dir, game_name, 'events_markup.json')\n",
    "        # Load ball annotations\n",
    "        json_ball = open(ball_annos_path)\n",
    "        ball_annos = json.load(json_ball)\n",
    "\n",
    "        # Load events annotations\n",
    "        json_events = open(events_annos_path)\n",
    "        events_annos = json.load(json_events)\n",
    "        for event_frameidx, event_name in events_annos.items():\n",
    "            event_frameidx = int(event_frameidx)\n",
    "            smooth_frame_indices = [event_frameidx]  # By default\n",
    "            if (event_name != 'empty_event') and (configs.smooth_labelling):\n",
    "                smooth_frame_indices = [idx for idx in range(event_frameidx - num_frames_from_event,\n",
    "                                                             event_frameidx + num_frames_from_event + 1)]\n",
    "\n",
    "            for smooth_idx in smooth_frame_indices:\n",
    "                sub_smooth_frame_indices = [idx for idx in range(smooth_idx - num_frames_from_event,\n",
    "                                                                 smooth_idx + num_frames_from_event + 1)]\n",
    "                img_path_list = []\n",
    "                for sub_smooth_idx in sub_smooth_frame_indices:\n",
    "                    img_path = os.path.join(images_dir, game_name, 'img_{:06d}.jpg'.format(sub_smooth_idx))\n",
    "                    img_path_list.append(img_path)\n",
    "                last_f_idx = smooth_idx + num_frames_from_event\n",
    "                # Get ball position for the last frame in the sequence\n",
    "                if '{}'.format(last_f_idx) not in ball_annos.keys():\n",
    "                    print('smooth_idx: {} - no ball position for the frame idx {}'.format(smooth_idx, last_f_idx))\n",
    "                    continue\n",
    "                ball_position_xy = ball_annos['{}'.format(last_f_idx)]\n",
    "                ball_position_xy = np.array([ball_position_xy['x'], ball_position_xy['y']], dtype=np.int)\n",
    "                # Ignore the event without ball information\n",
    "                if (ball_position_xy[0] < 0) or (ball_position_xy[1] < 0):\n",
    "                    continue\n",
    "\n",
    "                # Get segmentation path for the last frame in the sequence\n",
    "                seg_path = os.path.join(annos_dir, game_name, 'segmentation_masks', '{}.png'.format(last_f_idx))\n",
    "                if not os.path.isfile(seg_path):\n",
    "                    print(\"smooth_idx: {} - The segmentation path {} is invalid\".format(smooth_idx, seg_path))\n",
    "                    continue\n",
    "                event_class = configs.events_dict[event_name]\n",
    "\n",
    "                target_events = smooth_event_labelling(event_class, smooth_idx, event_frameidx)\n",
    "                events_infor.append([img_path_list, ball_position_xy, target_events, seg_path])\n",
    "                # Re-label if the event is neither bounce nor net hit\n",
    "                if (target_events[0] == 0) and (target_events[1] == 0):\n",
    "                    event_class = 2\n",
    "                events_labels.append(event_class)\n",
    "    return events_infor, events_labels\n",
    "def train_val_data_separation(configs):\n",
    "    \"\"\"Seperate data to training and validation sets\"\"\"\n",
    "    dataset_type = 'training'\n",
    "    events_infor, events_labels = get_events_infor(configs.train_game_list, configs, dataset_type)\n",
    "    if configs.no_val:\n",
    "        train_events_infor = events_infor\n",
    "        train_events_labels = events_labels\n",
    "        val_events_infor = None\n",
    "        val_events_labels = None\n",
    "    else:\n",
    "        train_events_infor, val_events_infor, train_events_labels, val_events_labels = train_test_split(events_infor,\n",
    "                                                                                                        events_labels,\n",
    "                                                                                                        shuffle=True,\n",
    "                                                                                                        test_size=configs.val_size,\n",
    "                                                                                                        random_state=configs.seed,\n",
    "                                                                                                        stratify=events_labels)\n",
    "    return train_events_infor, val_events_infor, train_events_labels, val_events_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0b15623",
   "metadata": {
    "code_folding": [
     1,
     31
    ]
   },
   "outputs": [],
   "source": [
    "# # Dataloader\n",
    "def create_train_val_dataloader(configs):\n",
    "    \"\"\"Create dataloader for training and validate\"\"\"\n",
    "\n",
    "    train_transform = Compose([\n",
    "        Random_Crop(max_reduction_percent=0.15, p=0.5),\n",
    "        Random_HFlip(p=0.5),\n",
    "        Random_Rotate(rotation_angle_limit=10, p=0.5),\n",
    "    ], p=1.)\n",
    "\n",
    "    train_events_infor, val_events_infor, *_ = train_val_data_separation(configs)\n",
    "    train_dataset = TTNet_Dataset(train_events_infor, configs.org_size, configs.input_size, transform=train_transform,\n",
    "                                  num_samples=configs.num_samples)\n",
    "    train_sampler = None\n",
    "    if configs.distributed:\n",
    "        train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=configs.batch_size, shuffle=(train_sampler is None),\n",
    "                                  pin_memory=configs.pin_memory, num_workers=configs.num_workers, sampler=train_sampler)\n",
    "\n",
    "    val_dataloader = None\n",
    "    if not configs.no_val:\n",
    "        val_transform = None\n",
    "        val_sampler = None\n",
    "        val_dataset = TTNet_Dataset(val_events_infor, configs.org_size, configs.input_size, transform=val_transform,\n",
    "                                    num_samples=configs.num_samples)\n",
    "        if configs.distributed:\n",
    "            val_sampler = torch.utils.data.distributed.DistributedSampler(val_dataset, shuffle=False)\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=configs.batch_size, shuffle=False,\n",
    "                                    pin_memory=configs.pin_memory, num_workers=configs.num_workers, sampler=val_sampler)\n",
    "\n",
    "    return train_dataloader, val_dataloader, train_sampler\n",
    "def create_test_dataloader(configs):\n",
    "    \"\"\"Create dataloader for testing phase\"\"\"\n",
    "\n",
    "    test_transform = None\n",
    "    dataset_type = 'test'\n",
    "    test_events_infor, test_events_labels = get_events_infor(configs.test_game_list, configs, dataset_type)\n",
    "    test_dataset = TTNet_Dataset(test_events_infor, configs.org_size, configs.input_size, transform=test_transform,\n",
    "                                 num_samples=configs.num_samples)\n",
    "    test_sampler = None\n",
    "    if configs.distributed:\n",
    "        test_sampler = torch.utils.data.distributed.DistributedSampler(test_dataset)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=configs.batch_size, shuffle=False,\n",
    "                                 pin_memory=configs.pin_memory, num_workers=configs.num_workers, sampler=test_sampler)\n",
    "\n",
    "    return test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c87dca2c",
   "metadata": {
    "code_folding": [
     0,
     33,
     39,
     43,
     51,
     62
    ]
   },
   "outputs": [],
   "source": [
    "def check_dataset():\n",
    "    configs = parse_configs()\n",
    "    game_list    = ['game_1']\n",
    "    dataset_type = 'training'\n",
    "    train_events_infor, val_events_infor, *_ = train_val_data_separation(configs)\n",
    "    print('len(train_events_infor): {}'.format(len(train_events_infor)))\n",
    "    # Test transformation\n",
    "    transform = Compose([\n",
    "        Random_Crop(max_reduction_percent=0.15, p=1.),\n",
    "        Random_HFlip(p=1.),\n",
    "        Random_Rotate(rotation_angle_limit=15, p=1.)\n",
    "    ], p=1.)\n",
    "    ttnet_dataset = TTNet_Dataset(train_events_infor, configs.org_size, configs.input_size, transform=transform)\n",
    "    print('len(ttnet_dataset): {}'.format(len(ttnet_dataset)))\n",
    "    example_index = 100\n",
    "#     resized_imgs, org_ball_pos_xy, global_ball_pos_xy, target_event = ttnet_dataset.__getitem__(example_index)\n",
    "    resized_imgs, org_ball_pos_xy, global_ball_pos_xy, target_event, target_seg = ttnet_dataset.__getitem__(example_index)\n",
    "    if 1:\n",
    "        # Test F.interpolate, we can simply use cv2.resize() to get origin_imgs from resized_imgs\n",
    "        # Achieve better quality of images and faster\n",
    "        origin_imgs = F.interpolate(torch.from_numpy(resized_imgs).unsqueeze(0).float(), (1080, 1920))\n",
    "        origin_imgs = origin_imgs.squeeze().numpy().transpose(1, 2, 0).astype(np.uint8)\n",
    "        print('F.interpolate - origin_imgs shape: {}'.format(origin_imgs.shape))\n",
    "        resized_imgs = resized_imgs.transpose(1, 2, 0)\n",
    "        print('resized_imgs shape: {}'.format(resized_imgs.shape))\n",
    "    else:\n",
    "        # Test cv2.resize\n",
    "        resized_imgs = resized_imgs.transpose(1, 2, 0)\n",
    "        print('resized_imgs shape: {}'.format(resized_imgs.shape))\n",
    "        origin_imgs = cv2.resize(resized_imgs, (1920, 1080))\n",
    "        print('cv2.resize - origin_imgs shape: {}'.format(origin_imgs.shape))\n",
    "        \n",
    "    out_images_dir = os.path.join(configs.results_dir, 'debug', 'ttnet_dataset')\n",
    "    if not os.path.isdir(out_images_dir):\n",
    "        os.makedirs(out_images_dir)\n",
    "        \n",
    "    fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(20, 20))\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    for i in range(configs.num_frames_sequence):\n",
    "        img = origin_imgs[:, :, (i * 3): (i + 1) * 3]\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title('image {}'.format(i))\n",
    "    fig.suptitle(\n",
    "        'Event: is bounce {}, is net: {}, ball_position_xy: (x= {}, y= {})'.format(target_event[0], target_event[1],\n",
    "                                                                                   org_ball_pos_xy[0],\n",
    "                                                                                   org_ball_pos_xy[1]),\n",
    "        fontsize=16)\n",
    "    plt.savefig(os.path.join(out_images_dir, 'org_all_imgs_{}.jpg'.format(example_index)))\n",
    "\n",
    "\n",
    "    for i in range(configs.num_frames_sequence):\n",
    "        img = resized_imgs[:, :, (i * 3): (i + 1) * 3]\n",
    "        if (i == (configs.num_frames_sequence - 1)):\n",
    "            img = cv2.resize(img, (img.shape[1], img.shape[0]))\n",
    "            ball_img = cv2.circle(img, tuple(global_ball_pos_xy), radius=5, color=(255, 0, 0), thickness=2)\n",
    "            ball_img = cv2.cvtColor(ball_img, cv2.COLOR_RGB2BGR)\n",
    "            cv2.imwrite(os.path.join(out_images_dir, 'augment_img_{}.jpg'.format(example_index)),\n",
    "                        ball_img)\n",
    "\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title('image {}'.format(i))\n",
    "    fig.suptitle(\n",
    "        'Event: is bounce {}, is net: {}, ball_position_xy: (x= {}, y= {})'.format(target_event[0], target_event[1],\n",
    "                                                                                   global_ball_pos_xy[0],\n",
    "                                                                                   global_ball_pos_xy[1]),\n",
    "        fontsize=16)\n",
    "    plt.savefig(os.path.join(out_images_dir, 'augment_all_imgs_{}.jpg'.format(example_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a730e76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "912f4a3a",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def check_dataloader():\n",
    "    configs = parse_configs()\n",
    "    configs.distributed = False  # For testing\n",
    "    train_dataloader, val_dataloader, train_sampler = create_train_val_dataloader(configs)\n",
    "    print('len train_dataloader: {}, val_dataloader: {}'.format(len(train_dataloader), len(val_dataloader)))\n",
    "    return train_dataloader, val_dataloader, train_sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2eef7064",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len train_dataloader: 369, val_dataloader: 93\n",
      "torch.Size([8, 27, 128, 320])\n",
      "torch.Size([8, 2])\n",
      "torch.Size([8, 2])\n",
      "torch.Size([8, 2])\n",
      "torch.Size([8, 3, 128, 320])\n"
     ]
    }
   ],
   "source": [
    "train_dataloader, val_dataloader, train_sampler = check_dataloader()\n",
    "for data in train_dataloader:\n",
    "    for i in range(len(data)):\n",
    "        print(data[i].shape)\n",
    "    break\n",
    "# data[0] : stack of images ( 9 frames per batch )\n",
    "# data[1] : original coordinated of ball\n",
    "# data[2] : scaled coordinates of ball\n",
    "# data[3] : event labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e796e19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32m../../dataset/training/annotations/game_5/segmentation_masks/80393.png\u001b[0m*\r\n"
     ]
    }
   ],
   "source": [
    "ls ../../dataset/training/annotations/game_5/segmentation_masks/80393.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10238e5",
   "metadata": {},
   "source": [
    "##### Video Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4800a635",
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# Video loader\n",
    "class TTNet_Video_Loader:\n",
    "    \"\"\"The loader for demo with a video input\"\"\"\n",
    "\n",
    "    def __init__(self, video_path, input_size=(320, 128), num_frames_sequence=9):\n",
    "        assert os.path.isfile(video_path), \"No video at {}\".format(video_path)\n",
    "        self.cap = cv2.VideoCapture(video_path)\n",
    "        self.video_fps = int(round(self.cap.get(cv2.CAP_PROP_FPS)))\n",
    "        self.video_w = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        self.video_h = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        self.video_num_frames = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "        self.width = input_size[0]\n",
    "        self.height = input_size[1]\n",
    "        self.count = 0\n",
    "        self.num_frames_sequence = num_frames_sequence\n",
    "        print('Length of the video: {:d} frames'.format(self.video_num_frames))\n",
    "\n",
    "        self.images_sequence = deque(maxlen=num_frames_sequence)\n",
    "        self.get_first_images_sequence()\n",
    "\n",
    "    def get_first_images_sequence(self):\n",
    "        # Load (self.num_frames_sequence - 1) images\n",
    "        while (self.count < self.num_frames_sequence):\n",
    "            self.count += 1\n",
    "            ret, frame = self.cap.read()  # BGR\n",
    "            assert ret, 'Failed to load frame {:d}'.format(self.count)\n",
    "            self.images_sequence.append(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB), (self.width, self.height)))\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.count = -1\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        self.count += 1\n",
    "        if self.count == len(self):\n",
    "            raise StopIteration\n",
    "        # Read image\n",
    "\n",
    "        ret, frame = self.cap.read()  # BGR\n",
    "        assert ret, 'Failed to load frame {:d}'.format(self.count)\n",
    "        self.images_sequence.append(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB), (self.width, self.height)))\n",
    "        resized_imgs = np.dstack(self.images_sequence)  # (128, 320, 27)\n",
    "        # Transpose (H, W, C) to (C, H, W) --> fit input of TTNet model\n",
    "        resized_imgs = resized_imgs.transpose(2, 0, 1)  # (27, 128, 320)\n",
    "\n",
    "        return self.count, resized_imgs\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.video_num_frames - self.num_frames_sequence + 1  # number of sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ffd7f0",
   "metadata": {},
   "source": [
    "##### Data Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6095b31",
   "metadata": {
    "code_folding": [
     2,
     12,
     37,
     45,
     49,
     110
    ]
   },
   "outputs": [],
   "source": [
    "# Data utils\n",
    "\n",
    "def load_raw_img(img_path):\n",
    "    \"\"\"Load raw image based on the path to the image\"\"\"\n",
    "    img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)  # BGR --> RGB\n",
    "    return img\n",
    "\n",
    "def gaussian_1d(pos, muy, sigma):\n",
    "    \"\"\"Create 1D Gaussian distribution based on ball position (muy), and std (sigma)\"\"\"\n",
    "    target = torch.exp(- (((pos - muy) / sigma) ** 2) / 2)\n",
    "    return target\n",
    "\n",
    "def create_target_ball(ball_position_xy, sigma, w, h, thresh_mask, device):\n",
    "    \"\"\"Create target for the ball detection stages\n",
    "    :param ball_position_xy: Position of the ball (x,y)\n",
    "    :param sigma: standard deviation (a hyperparameter)\n",
    "    :param w: width of the resize image\n",
    "    :param h: height of the resize image\n",
    "    :param thresh_mask: if values of 1D Gaussian < thresh_mask --> set to 0 to reduce computation\n",
    "    :param device: cuda() or cpu()\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    w, h = int(w), int(h)\n",
    "    target_ball_position = torch.zeros((w + h,), device=device)\n",
    "    # Only do the next step if the ball is existed\n",
    "    if (w > ball_position_xy[0] > 0) and (h > ball_position_xy[1] > 0):\n",
    "        # For x\n",
    "        x_pos = torch.arange(0, w, device=device)\n",
    "        target_ball_position[:w] = gaussian_1d(x_pos, ball_position_xy[0], sigma=sigma)\n",
    "        # For y\n",
    "        y_pos = torch.arange(0, h, device=device)\n",
    "        target_ball_position[w:] = gaussian_1d(y_pos, ball_position_xy[1], sigma=sigma)\n",
    "\n",
    "        target_ball_position[target_ball_position < thresh_mask] = 0.\n",
    "\n",
    "    return target_ball_position\n",
    "\n",
    "def smooth_event_labelling(event_class, smooth_idx, event_frameidx):\n",
    "    target_events = np.zeros((2,))\n",
    "    if event_class < 2:\n",
    "        n = smooth_idx - event_frameidx\n",
    "        target_events[event_class] = np.cos(n * np.pi / 8)\n",
    "        target_events[target_events < 0.01] = 0.\n",
    "    return target_events\n",
    "\n",
    "def get_events_infor(game_list, configs, dataset_type):\n",
    "    \"\"\"Get information of sequences of images based on events\n",
    "    :param game_list: List of games (video names)\n",
    "    :return:\n",
    "    [\n",
    "        each event: [[img_path_list], ball_position, target_events, segmentation_path]\n",
    "    ]\n",
    "    \"\"\"\n",
    "    # the paper mentioned 25, but used 9 frames only\n",
    "    num_frames_from_event = int((configs.num_frames_sequence - 1) / 2)\n",
    "\n",
    "    annos_dir = os.path.join(configs.dataset_dir, dataset_type, 'annotations')\n",
    "    images_dir = os.path.join(configs.dataset_dir, dataset_type, 'images')\n",
    "    events_infor = []\n",
    "    events_labels = []\n",
    "    for game_name in game_list:\n",
    "        ball_annos_path = os.path.join(annos_dir, game_name, 'ball_markup.json')\n",
    "        events_annos_path = os.path.join(annos_dir, game_name, 'events_markup.json')\n",
    "        # Load ball annotations\n",
    "        json_ball = open(ball_annos_path)\n",
    "        ball_annos = json.load(json_ball)\n",
    "\n",
    "        # Load events annotations\n",
    "        json_events = open(events_annos_path)\n",
    "        events_annos = json.load(json_events)\n",
    "        for event_frameidx, event_name in events_annos.items():\n",
    "            event_frameidx = int(event_frameidx)\n",
    "            smooth_frame_indices = [event_frameidx]  # By default\n",
    "            if (event_name != 'empty_event') and (configs.smooth_labelling):\n",
    "                smooth_frame_indices = [idx for idx in range(event_frameidx - num_frames_from_event,\n",
    "                                                             event_frameidx + num_frames_from_event + 1)]\n",
    "\n",
    "            for smooth_idx in smooth_frame_indices:\n",
    "                sub_smooth_frame_indices = [idx for idx in range(smooth_idx - num_frames_from_event,\n",
    "                                                                 smooth_idx + num_frames_from_event + 1)]\n",
    "                img_path_list = []\n",
    "                for sub_smooth_idx in sub_smooth_frame_indices:\n",
    "                    img_path = os.path.join(images_dir, game_name, 'img_{:06d}.jpg'.format(sub_smooth_idx))\n",
    "                    img_path_list.append(img_path)\n",
    "                last_f_idx = smooth_idx + num_frames_from_event\n",
    "                # Get ball position for the last frame in the sequence\n",
    "                if '{}'.format(last_f_idx) not in ball_annos.keys():\n",
    "                    print('smooth_idx: {} - no ball position for the frame idx {}'.format(smooth_idx, last_f_idx))\n",
    "                    continue\n",
    "                ball_position_xy = ball_annos['{}'.format(last_f_idx)]\n",
    "                ball_position_xy = np.array([ball_position_xy['x'], ball_position_xy['y']], dtype=np.int)\n",
    "                # Ignore the event without ball information\n",
    "                if (ball_position_xy[0] < 0) or (ball_position_xy[1] < 0):\n",
    "                    continue\n",
    "\n",
    "                # Get segmentation path for the last frame in the sequence\n",
    "                seg_path = os.path.join(annos_dir, game_name, 'segmentation_masks', '{}.png'.format(last_f_idx))\n",
    "                if not os.path.isfile(seg_path):\n",
    "                    print(\"smooth_idx: {} - The segmentation path {} is invalid\".format(smooth_idx, seg_path))\n",
    "                    continue\n",
    "                event_class = configs.events_dict[event_name]\n",
    "\n",
    "                target_events = smooth_event_labelling(event_class, smooth_idx, event_frameidx)\n",
    "                events_infor.append([img_path_list, ball_position_xy, target_events, seg_path])\n",
    "                # Re-label if the event is neither bounce nor net hit\n",
    "                if (target_events[0] == 0) and (target_events[1] == 0):\n",
    "                    event_class = 2\n",
    "                events_labels.append(event_class)\n",
    "    return events_infor, events_labels\n",
    "\n",
    "def train_val_data_separation(configs):\n",
    "    \"\"\"Seperate data to training and validation sets\"\"\"\n",
    "    dataset_type = 'training'\n",
    "    events_infor, events_labels = get_events_infor(configs.train_game_list, configs, dataset_type)\n",
    "    if configs.no_val:\n",
    "        train_events_infor = events_infor\n",
    "        train_events_labels = events_labels\n",
    "        val_events_infor = None\n",
    "        val_events_labels = None\n",
    "    else:\n",
    "        train_events_infor, val_events_infor, train_events_labels, val_events_labels = train_test_split(events_infor,\n",
    "                                                                                                        events_labels,\n",
    "                                                                                                        shuffle=True,\n",
    "                                                                                                        test_size=configs.val_size,\n",
    "                                                                                                        random_state=configs.seed,\n",
    "                                                                                                        stratify=events_labels)\n",
    "    return train_events_infor, val_events_infor, train_events_labels, val_events_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6530f82a",
   "metadata": {},
   "source": [
    "### Model Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dd1a4b",
   "metadata": {},
   "source": [
    "#### TTNet Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bd01cc",
   "metadata": {},
   "source": [
    "<img src=\"artifacts/network.png\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a2a726",
   "metadata": {},
   "source": [
    "##### ConvBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9781acf1",
   "metadata": {
    "code_folding": [
     11
    ]
   },
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.maxpool(self.relu(self.batchnorm(self.conv(x))))\n",
    "        return x\n",
    "class ConvBlock_without_Pooling(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvBlock_without_Pooling, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.batchnorm(self.conv(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345c9afb",
   "metadata": {},
   "source": [
    "##### DeconvBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "407549b8",
   "metadata": {
    "code_folding": [
     0,
     19,
     56,
     82,
     113
    ]
   },
   "outputs": [],
   "source": [
    "class DeconvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DeconvBlock, self).__init__()\n",
    "        middle_channels = int(in_channels / 4)\n",
    "        self.conv1 = nn.Conv2d(in_channels, middle_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(middle_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.batchnorm_tconv = nn.BatchNorm2d(middle_channels)\n",
    "        self.tconv = nn.ConvTranspose2d(middle_channels, middle_channels, kernel_size=3, stride=2, padding=1,\n",
    "                                        output_padding=1)\n",
    "        self.conv2 = nn.Conv2d(middle_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.batchnorm1(self.conv1(x)))\n",
    "        x = self.relu(self.batchnorm_tconv(self.tconv(x)))\n",
    "        x = self.relu(self.batchnorm2(self.conv2(x)))\n",
    "\n",
    "        return x\n",
    "class BallDetection(nn.Module):\n",
    "    def __init__(self, num_frames_sequence, dropout_p):\n",
    "        super(BallDetection, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_frames_sequence * 3, 64, kernel_size=1, stride=1, padding=0)\n",
    "        self.batchnorm = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.convblock1 = ConvBlock(in_channels=64, out_channels=64)\n",
    "        self.convblock2 = ConvBlock(in_channels=64, out_channels=64)\n",
    "        self.dropout2d = nn.Dropout2d(p=dropout_p)\n",
    "        self.convblock3 = ConvBlock(in_channels=64, out_channels=128)\n",
    "        self.convblock4 = ConvBlock(in_channels=128, out_channels=128)\n",
    "        self.convblock5 = ConvBlock(in_channels=128, out_channels=256)\n",
    "        self.convblock6 = ConvBlock(in_channels=256, out_channels=256)\n",
    "        self.fc1 = nn.Linear(in_features=2560, out_features=1792)\n",
    "        self.fc2 = nn.Linear(in_features=1792, out_features=896)\n",
    "        self.fc3 = nn.Linear(in_features=896, out_features=448)\n",
    "        self.dropout1d = nn.Dropout(p=dropout_p)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.batchnorm(self.conv1(x)))\n",
    "        out_block2 = self.convblock2(self.convblock1(x))\n",
    "        x = self.dropout2d(out_block2)\n",
    "        out_block3 = self.convblock3(x)\n",
    "        out_block4 = self.convblock4(out_block3)\n",
    "        x = self.dropout2d(out_block4)\n",
    "        out_block5 = self.convblock5(out_block4)\n",
    "        features = self.convblock6(out_block5)\n",
    "\n",
    "        x = self.dropout2d(features)\n",
    "        x = x.contiguous().view(x.size(0), -1)\n",
    "\n",
    "        x = self.dropout1d(self.relu(self.fc1(x)))\n",
    "        x = self.dropout1d(self.relu(self.fc2(x)))\n",
    "        out = self.sigmoid(self.fc3(x))\n",
    "\n",
    "        return out, features, out_block2, out_block3, out_block4, out_block5\n",
    "class EventsSpotting(nn.Module):\n",
    "    def __init__(self, dropout_p):\n",
    "        super(EventsSpotting, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(512, 64, kernel_size=1, stride=1, padding=0)\n",
    "        self.batchnorm = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout2d = nn.Dropout2d(p=dropout_p)\n",
    "        self.convblock = ConvBlock_without_Pooling(in_channels=64, out_channels=64)\n",
    "        self.fc1 = nn.Linear(in_features=640, out_features=512)\n",
    "        self.fc2 = nn.Linear(in_features=512, out_features=2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, global_features, local_features):\n",
    "        input_eventspotting = torch.cat((global_features, local_features), dim=1)\n",
    "        x = self.relu(self.batchnorm(self.conv1(input_eventspotting)))\n",
    "        x = self.dropout2d(x)\n",
    "        x = self.convblock(x)\n",
    "        x = self.dropout2d(x)\n",
    "        x = self.convblock(x)\n",
    "        x = self.dropout2d(x)\n",
    "\n",
    "        x = x.contiguous().view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        out = self.sigmoid(self.fc2(x))\n",
    "\n",
    "        return out\n",
    "class Segmentation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Segmentation, self).__init__()\n",
    "        self.deconvblock5 = DeconvBlock(in_channels=256, out_channels=128)\n",
    "        self.deconvblock4 = DeconvBlock(in_channels=128, out_channels=128)\n",
    "        self.deconvblock3 = DeconvBlock(in_channels=128, out_channels=64)\n",
    "        self.deconvblock2 = DeconvBlock(in_channels=64, out_channels=64)\n",
    "        self.tconv = nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=3, stride=2, padding=0,\n",
    "                                        output_padding=0)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv1 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=0)\n",
    "        self.conv2 = nn.Conv2d(32, 3, kernel_size=2, stride=1, padding=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, out_block2, out_block3, out_block4, out_block5):\n",
    "        x = self.deconvblock5(out_block5)\n",
    "        x = x + out_block4\n",
    "        x = self.deconvblock4(x)\n",
    "        x = x + out_block3\n",
    "        x = self.deconvblock3(x)\n",
    "\n",
    "        x = x + out_block2\n",
    "        x = self.deconvblock2(x)\n",
    "\n",
    "        x = self.relu(self.tconv(x))\n",
    "\n",
    "        x = self.relu(self.conv1(x))\n",
    "\n",
    "        out = self.sigmoid(self.conv2(x))\n",
    "\n",
    "        return out\n",
    "class TTNet(nn.Module):\n",
    "    def __init__(self, dropout_p, tasks, input_size, thresh_ball_pos_mask, num_frames_sequence,\n",
    "                 mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n",
    "        super(TTNet, self).__init__()\n",
    "        self.tasks = tasks\n",
    "        self.ball_local_stage, self.events_spotting, self.segmentation = None, None, None\n",
    "        self.ball_global_stage = BallDetection(num_frames_sequence=num_frames_sequence, dropout_p=dropout_p)\n",
    "        if 'local' in tasks:\n",
    "            self.ball_local_stage = BallDetection(num_frames_sequence=num_frames_sequence, dropout_p=dropout_p)\n",
    "        if 'event' in tasks:\n",
    "            self.events_spotting = EventsSpotting(dropout_p=dropout_p)\n",
    "        if 'seg' in tasks:\n",
    "            self.segmentation = Segmentation()\n",
    "        self.w_resize = input_size[0]\n",
    "        self.h_resize = input_size[1]\n",
    "        self.thresh_ball_pos_mask = thresh_ball_pos_mask\n",
    "        self.mean = torch.repeat_interleave(torch.tensor(mean).view(1, 3, 1, 1), repeats=9, dim=1)\n",
    "        self.std = torch.repeat_interleave(torch.tensor(std).view(1, 3, 1, 1), repeats=9, dim=1)\n",
    "\n",
    "    def forward(self, resize_batch_input, org_ball_pos_xy):\n",
    "        \"\"\"Forward propagation\n",
    "        :param resize_batch_input: (batch_size, 27, 128, 320)\n",
    "        :param org_ball_pos_xy: (batch_size, 2) --> Use it to get ground-truth for the local stage\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        pred_ball_local, pred_events, pred_seg, local_ball_pos_xy = None, None, None, None\n",
    "\n",
    "        # Normalize the input before compute forward propagation\n",
    "        pred_ball_global, global_features, out_block2, out_block3, out_block4, out_block5 = self.ball_global_stage(\n",
    "            self.__normalize__(resize_batch_input))\n",
    "        if self.ball_local_stage is not None:\n",
    "            # Based on the prediction of the global stage, crop the original images\n",
    "            input_ball_local, cropped_params = self.__crop_original_batch__(resize_batch_input, pred_ball_global)\n",
    "            # Get the ground truth of the ball for the local stage\n",
    "            local_ball_pos_xy = self.__get_groundtruth_local_ball_pos__(org_ball_pos_xy, cropped_params)\n",
    "            # Normalize the input before compute forward propagation\n",
    "            pred_ball_local, local_features, *_ = self.ball_local_stage(self.__normalize__(input_ball_local))\n",
    "            # Only consider the events spotting if the model has the local stage for ball detection\n",
    "            if self.events_spotting is not None:\n",
    "                pred_events = self.events_spotting(global_features, local_features)\n",
    "        if self.segmentation is not None:\n",
    "            pred_seg = self.segmentation(out_block2, out_block3, out_block4, out_block5)\n",
    "\n",
    "        return pred_ball_global, pred_ball_local, pred_events, pred_seg, local_ball_pos_xy\n",
    "\n",
    "    def run_demo(self, resize_batch_input):\n",
    "        \"\"\"Only for full 4 stages/modules in TTNet\"\"\"\n",
    "\n",
    "        # Normalize the input before compute forward propagation\n",
    "        pred_ball_global, global_features, out_block2, out_block3, out_block4, out_block5 = self.ball_global_stage(\n",
    "            self.__normalize__(resize_batch_input))\n",
    "        input_ball_local, cropped_params = self.__crop_original_batch__(resize_batch_input, pred_ball_global)\n",
    "        # Normalize the input before compute forward propagation\n",
    "        pred_ball_local, local_features, *_ = self.ball_local_stage(self.__normalize__(input_ball_local))\n",
    "        pred_events = self.events_spotting(global_features, local_features)\n",
    "        pred_seg = self.segmentation(out_block2, out_block3, out_block4, out_block5)\n",
    "\n",
    "        return pred_ball_global, pred_ball_local, pred_events, pred_seg\n",
    "\n",
    "    def __normalize__(self, x):\n",
    "        if not self.mean.is_cuda:\n",
    "            self.mean = self.mean.cuda()\n",
    "            self.std = self.std.cuda()\n",
    "\n",
    "        return (x / 255. - self.mean) / self.std\n",
    "\n",
    "    def __get_groundtruth_local_ball_pos__(self, org_ball_pos_xy, cropped_params):\n",
    "        local_ball_pos_xy = torch.zeros_like(org_ball_pos_xy)  # no grad for torch.zeros_like output\n",
    "\n",
    "        for idx, params in enumerate(cropped_params):\n",
    "            is_ball_detected, x_min, x_max, y_min, y_max, x_pad, y_pad = params\n",
    "\n",
    "            if is_ball_detected:\n",
    "                # Get the local ball position based on the crop image informaion\n",
    "                local_ball_pos_xy[idx, 0] = max(org_ball_pos_xy[idx, 0] - x_min + x_pad, -1)\n",
    "                local_ball_pos_xy[idx, 1] = max(org_ball_pos_xy[idx, 1] - y_min + y_pad, -1)\n",
    "                # If the ball is outside of the cropped image --> set position to -1, -1 --> No ball\n",
    "                if (local_ball_pos_xy[idx, 0] >= self.w_resize) or (local_ball_pos_xy[idx, 1] >= self.h_resize) or (\n",
    "                        local_ball_pos_xy[idx, 0] < 0) or (local_ball_pos_xy[idx, 1] < 0):\n",
    "                    local_ball_pos_xy[idx, 0] = -1\n",
    "                    local_ball_pos_xy[idx, 1] = -1\n",
    "            else:\n",
    "                local_ball_pos_xy[idx, 0] = -1\n",
    "                local_ball_pos_xy[idx, 1] = -1\n",
    "        return local_ball_pos_xy\n",
    "\n",
    "    def __crop_original_batch__(self, resize_batch_input, pred_ball_global):\n",
    "        \"\"\"Get input of the local stage by cropping the original images based on the predicted ball position\n",
    "            of the global stage\n",
    "        :param resize_batch_input: (batch_size, 27, 128, 320)\n",
    "        :param pred_ball_global: (batch_size, 448)\n",
    "        :param org_ball_pos_xy: (batch_size, 2)\n",
    "        :return: input_ball_local (batch_size, 27, 128, 320)\n",
    "        \"\"\"\n",
    "        # Process input for local stage based on output of the global one\n",
    "\n",
    "        batch_size = resize_batch_input.size(0)\n",
    "        h_original, w_original = 1080, 1920\n",
    "        h_ratio = h_original / self.h_resize\n",
    "        w_ratio = w_original / self.w_resize\n",
    "        pred_ball_global_mask = pred_ball_global.clone().detach()\n",
    "        pred_ball_global_mask[pred_ball_global_mask < self.thresh_ball_pos_mask] = 0.\n",
    "\n",
    "        # Crop the original images\n",
    "        input_ball_local = torch.zeros_like(resize_batch_input)  # same shape with resize_batch_input, no grad\n",
    "        original_batch_input = F.interpolate(resize_batch_input, (h_original, w_original))  # On GPU\n",
    "        cropped_params = []\n",
    "        for idx in range(batch_size):\n",
    "            pred_ball_pos_x = pred_ball_global_mask[idx, :self.w_resize]\n",
    "            pred_ball_pos_y = pred_ball_global_mask[idx, self.w_resize:]\n",
    "            # If the ball is not detected, we crop the center of the images, set ball_poss to [-1, -1]\n",
    "            if (torch.sum(pred_ball_pos_x) == 0.) or (torch.sum(pred_ball_pos_y) == 0.):\n",
    "                # Assume the ball is in the center image\n",
    "                x_center = int(self.w_resize / 2)\n",
    "                y_center = int(self.h_resize / 2)\n",
    "                is_ball_detected = False\n",
    "            else:\n",
    "                x_center = torch.argmax(pred_ball_pos_x)  # Upper part\n",
    "                y_center = torch.argmax(pred_ball_pos_y)  # Lower part\n",
    "                is_ball_detected = True\n",
    "\n",
    "            # Adjust ball position to the original size\n",
    "            x_center = int(x_center * w_ratio)\n",
    "            y_center = int(y_center * h_ratio)\n",
    "\n",
    "            x_min, x_max, y_min, y_max = self.__get_crop_params__(x_center, y_center, self.w_resize, self.h_resize,\n",
    "                                                                  w_original, h_original)\n",
    "            # Put image to the center\n",
    "            h_crop = y_max - y_min\n",
    "            w_crop = x_max - x_min\n",
    "            x_pad = 0\n",
    "            y_pad = 0\n",
    "            if (h_crop != self.h_resize) or (w_crop != self.w_resize):\n",
    "                x_pad = int((self.w_resize - w_crop) / 2)\n",
    "                y_pad = int((self.h_resize - h_crop) / 2)\n",
    "                input_ball_local[idx, :, y_pad:(y_pad + h_crop), x_pad:(x_pad + w_crop)] = original_batch_input[idx, :,\n",
    "                                                                                           y_min:y_max, x_min: x_max]\n",
    "            else:\n",
    "                input_ball_local[idx, :, :, :] = original_batch_input[idx, :, y_min:y_max, x_min: x_max]\n",
    "            cropped_params.append([is_ball_detected, x_min, x_max, y_min, y_max, x_pad, y_pad])\n",
    "\n",
    "        return input_ball_local, cropped_params\n",
    "\n",
    "    def __get_crop_params__(self, x_center, y_center, w_resize, h_resize, w_original, h_original):\n",
    "        x_min = max(0, x_center - int(w_resize / 2))\n",
    "        y_min = max(0, y_center - int(h_resize / 2))\n",
    "\n",
    "        x_max = min(w_original, x_min + w_resize)\n",
    "        y_max = min(h_original, y_min + h_resize)\n",
    "\n",
    "        return x_min, x_max, y_min, y_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5072fdff",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def check_ttnet():\n",
    "    tasks = ['global', 'local', 'event']\n",
    "    ttnet = TTNet(dropout_p=0.5, tasks=tasks, input_size=(320, 128), thresh_ball_pos_mask=0.01,\n",
    "                  num_frames_sequence=9).cuda()\n",
    "    resize_batch_input = torch.rand((1, 27, 128, 320)).cuda()\n",
    "    org_ball_pos_xy = torch.rand((1, 2)).cuda()\n",
    "    start = time.time()\n",
    "    \n",
    "#     pred_ball_global, pred_ball_local, pred_events, local_ball_pos_xy = ttnet(resize_batch_input, org_ball_pos_xy)\n",
    "    pred_ball_global, pred_ball_local, pred_events, pred_seg, local_ball_pos_xy = ttnet(resize_batch_input, org_ball_pos_xy)\n",
    "    \n",
    "#     print(\"DEBUG Unbalaced loss: \", pred_ball_global.shape, pred_ball_local.shape, pred_events.shape, local_ball_pos_xy.shape)    \n",
    "        \n",
    "    if pred_ball_global is not None:\n",
    "        print('pred_ball_global: {}'.format(pred_ball_global.size()))\n",
    "    if pred_ball_local is not None:\n",
    "        print('pred_ball_local: {}'.format(pred_ball_local.size()))\n",
    "    if pred_events is not None:\n",
    "        print('pred_events: {}'.format(pred_events.size()))\n",
    "    print('local_ball_pos_xy: {}'.format(local_ball_pos_xy.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e22e3c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_ball_global: torch.Size([1, 448])\n",
      "pred_ball_local: torch.Size([1, 448])\n",
      "pred_events: torch.Size([1, 2])\n",
      "local_ball_pos_xy: torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "check_ttnet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90505b50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "610138d4",
   "metadata": {},
   "source": [
    "### Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "159518e5",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Ball_Detection_Loss(nn.Module):\n",
    "    def __init__(self, w, h, epsilon=1e-9):\n",
    "        super(Ball_Detection_Loss, self).__init__()\n",
    "        self.w = w\n",
    "        self.h = h\n",
    "        self.epsilon = epsilon\n",
    "        self.criterion = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "\n",
    "    def forward(self, pred_ball_position, target_ball_position):\n",
    "        x_pred = pred_ball_position[:, :self.w]\n",
    "        y_pred = pred_ball_position[:, self.w:]\n",
    "\n",
    "        x_target = target_ball_position[:, :self.w]\n",
    "        y_target = target_ball_position[:, self.w:]\n",
    "\n",
    "        loss_ball_x = - torch.mean(x_target * torch.log(x_pred + self.epsilon) + (1 - x_target) * torch.log(1 - x_pred + self.epsilon))\n",
    "        loss_ball_y = - torch.mean(y_target * torch.log(y_pred + self.epsilon) + (1 - y_target) * torch.log(1 - y_pred + self.epsilon))\n",
    "#         loss_ball_x = self.criterion(x_target.log(), x_pred)\n",
    "#         loss_ball_y = self.criterion(y_target.log(), y_pred)\n",
    "        \n",
    "        return loss_ball_x + loss_ball_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8afebcbe",
   "metadata": {
    "code_folding": [
     0,
     12,
     20,
     28
    ]
   },
   "outputs": [],
   "source": [
    "class Events_Spotting_Loss(nn.Module):\n",
    "    def __init__(self, weights=(1, 3), num_events=2, epsilon=1e-9):\n",
    "        super(Events_Spotting_Loss, self).__init__()\n",
    "        self.weights = torch.tensor(weights).view(1, 2)\n",
    "        self.weights = self.weights / self.weights.sum()\n",
    "        self.num_events = num_events\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, pred_events, target_events):\n",
    "        self.weights = self.weights.cuda()\n",
    "        return - torch.mean(self.weights * (target_events * torch.log(pred_events + self.epsilon) + (1. - target_events) * torch.log(1 - pred_events + self.epsilon)))\n",
    "\n",
    "class DICE_Smotth_Loss(nn.Module):\n",
    "    def __init__(self, epsilon=1e-9):\n",
    "        super(DICE_Smotth_Loss, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, pred_seg, target_seg):\n",
    "        return 1. - ((torch.sum(2 * pred_seg * target_seg) + self.epsilon) / (torch.sum(pred_seg) + torch.sum(target_seg) + self.epsilon))\n",
    "\n",
    "class BCE_Loss(nn.Module):\n",
    "    def __init__(self, epsilon=1e-9):\n",
    "        super(BCE_Loss, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, pred_seg, target_seg):\n",
    "        return - torch.mean(target_seg * torch.log(pred_seg + self.epsilon) + (1 - target_seg) * torch.log(1 - pred_seg + self.epsilon))\n",
    "\n",
    "class Segmentation_Loss(nn.Module):\n",
    "    def __init__(self, bce_weight=0.5):\n",
    "        super(Segmentation_Loss, self).__init__()\n",
    "        self.bce_criterion = BCE_Loss(epsilon=1e-9)\n",
    "        self.dice_criterion = DICE_Smotth_Loss(epsilon=1e-9)\n",
    "        self.bce_weight = bce_weight\n",
    "\n",
    "    def forward(self, pred_seg, target_seg):\n",
    "        target_seg = target_seg.float()\n",
    "        loss_bce = self.bce_criterion(pred_seg, target_seg)\n",
    "        loss_dice = self.dice_criterion(pred_seg, target_seg)\n",
    "        loss_seg = (1 - self.bce_weight) * loss_dice + self.bce_weight * loss_bce\n",
    "        return loss_seg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a966691",
   "metadata": {},
   "source": [
    "##### DICE Smotth Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f0f0452",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class DICE_Smotth_Loss(nn.Module):\n",
    "    def __init__(self, epsilon=1e-9):\n",
    "        super(DICE_Smotth_Loss, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, pred_seg, target_seg):\n",
    "        return 1. - ((torch.sum(2 * pred_seg * target_seg) + self.epsilon) / (torch.sum(pred_seg) + torch.sum(target_seg) + self.epsilon))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0536a4a",
   "metadata": {},
   "source": [
    "##### BCE Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e175791c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCE_Loss(nn.Module):\n",
    "    def __init__(self, epsilon=1e-9):\n",
    "        super(BCE_Loss, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, pred_seg, target_seg):\n",
    "        return - torch.mean(target_seg * torch.log(pred_seg + self.epsilon) + (1 - target_seg) * torch.log(1 - pred_seg + self.epsilon))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b14245b",
   "metadata": {},
   "source": [
    "##### Segmentation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8771437e",
   "metadata": {
    "code_folding": [
     1,
     7
    ]
   },
   "outputs": [],
   "source": [
    "class Segmentation_Loss(nn.Module):\n",
    "    def __init__(self, bce_weight=0.5):\n",
    "        super(Segmentation_Loss, self).__init__()\n",
    "        self.bce_criterion = BCE_Loss(epsilon=1e-9)\n",
    "        self.dice_criterion = DICE_Smotth_Loss(epsilon=1e-9)\n",
    "        self.bce_weight = bce_weight\n",
    "\n",
    "    def forward(self, pred_seg, target_seg):\n",
    "        target_seg = target_seg.float()\n",
    "        loss_bce = self.bce_criterion(pred_seg, target_seg)\n",
    "        loss_dice = self.dice_criterion(pred_seg, target_seg)\n",
    "        loss_seg = (1 - self.bce_weight) * loss_dice + self.bce_weight * loss_bce\n",
    "        return loss_seg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d674a56e",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb85d713",
   "metadata": {},
   "source": [
    "##### Unbalanced Loss Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "08206a2c",
   "metadata": {
    "code_folding": [
     0,
     1,
     16
    ]
   },
   "outputs": [],
   "source": [
    "class Unbalance_Loss_Model(nn.Module):\n",
    "    def __init__(self, model, tasks_loss_weight, weights_events, input_size, sigma, thresh_ball_pos_mask, device):\n",
    "        super(Unbalance_Loss_Model, self).__init__()\n",
    "        self.model = model\n",
    "        self.tasks_loss_weight = torch.tensor(tasks_loss_weight)\n",
    "        self.tasks_loss_weight = self.tasks_loss_weight / self.tasks_loss_weight.sum()\n",
    "        self.num_events = len(tasks_loss_weight)\n",
    "        self.w = input_size[0]\n",
    "        self.h = input_size[1]\n",
    "        self.sigma = sigma\n",
    "        self.thresh_ball_pos_mask = thresh_ball_pos_mask\n",
    "        self.device = device\n",
    "        self.ball_loss_criterion = Ball_Detection_Loss(self.w, self.h)\n",
    "        self.event_loss_criterion = Events_Spotting_Loss(weights=weights_events, num_events=self.num_events)\n",
    "        self.seg_loss_criterion = Segmentation_Loss()\n",
    "\n",
    "    def forward(self, resize_batch_input, org_ball_pos_xy, global_ball_pos_xy, target_events, target_seg):\n",
    "        pred_ball_global, pred_ball_local, pred_events, pred_seg, local_ball_pos_xy = self.model(resize_batch_input,\n",
    "                                                                                                 org_ball_pos_xy)\n",
    "        # Create target for events spotting and ball position (local and global)\n",
    "        batch_size = pred_ball_global.size(0)\n",
    "        target_ball_global = torch.zeros_like(pred_ball_global)\n",
    "        task_idx = 0\n",
    "        for sample_idx in range(batch_size):\n",
    "            target_ball_global[sample_idx] = create_target_ball(global_ball_pos_xy[sample_idx], sigma=self.sigma,\n",
    "                                                                w=self.w, h=self.h,\n",
    "                                                                thresh_mask=self.thresh_ball_pos_mask,\n",
    "                                                                device=self.device)\n",
    "        global_ball_loss = self.ball_loss_criterion(pred_ball_global, target_ball_global)\n",
    "        total_loss = global_ball_loss * self.tasks_loss_weight[task_idx]\n",
    "\n",
    "        if pred_ball_local is not None:\n",
    "            task_idx += 1\n",
    "            target_ball_local = torch.zeros_like(pred_ball_local)\n",
    "            for sample_idx in range(batch_size):\n",
    "                target_ball_local[sample_idx] = create_target_ball(local_ball_pos_xy[sample_idx], sigma=self.sigma,\n",
    "                                                                   w=self.w, h=self.h,\n",
    "                                                                   thresh_mask=self.thresh_ball_pos_mask,\n",
    "                                                                   device=self.device)\n",
    "            local_ball_loss = self.ball_loss_criterion(pred_ball_local, target_ball_local)\n",
    "            total_loss += local_ball_loss * self.tasks_loss_weight[task_idx]\n",
    "\n",
    "        if pred_events is not None:\n",
    "            task_idx += 1\n",
    "            target_events = target_events.to(device=self.device)\n",
    "            event_loss = self.event_loss_criterion(pred_events, target_events)\n",
    "            total_loss += event_loss * self.tasks_loss_weight[task_idx]\n",
    "\n",
    "        if pred_seg is not None:\n",
    "            task_idx += 1\n",
    "            seg_loss = self.seg_loss_criterion(pred_seg, target_seg)\n",
    "            total_loss += seg_loss * self.tasks_loss_weight[task_idx]\n",
    "\n",
    "        return pred_ball_global, pred_ball_local, pred_events, pred_seg, local_ball_pos_xy, total_loss, None\n",
    "\n",
    "    def run_demo(self, resize_batch_input):\n",
    "        pred_ball_global, pred_ball_local, pred_events, pred_seg = self.model.run_demo(resize_batch_input)\n",
    "        return pred_ball_global, pred_ball_local, pred_events, pred_seg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dc0a4c",
   "metadata": {},
   "source": [
    "##### Multi-task Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ba7b989a",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Multi_Task_Learning_Model(nn.Module):\n",
    "    \"\"\"\n",
    "    Original paper: \"Multi-task learning using uncertainty to weigh losses for scene geometry and semantics\" - CVPR 2018\n",
    "    url: https://arxiv.org/pdf/1705.07115.pdf\n",
    "    refer code: https://github.com/Hui-Li/multi-task-learning-example-PyTorch\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, tasks, num_events, weights_events, input_size, sigma, thresh_ball_pos_mask, device):\n",
    "        super(Multi_Task_Learning_Model, self).__init__()\n",
    "        self.model = model\n",
    "        self.tasks = tasks\n",
    "        self.num_tasks = len(tasks)\n",
    "        self.log_vars = nn.Parameter(torch.zeros((self.num_tasks)))\n",
    "        self.w = input_size[0]\n",
    "        self.h = input_size[1]\n",
    "        self.sigma = sigma\n",
    "        self.thresh_ball_pos_mask = thresh_ball_pos_mask\n",
    "        self.device = device\n",
    "        self.ball_loss_criterion = Ball_Detection_Loss(self.w, self.h)\n",
    "        self.event_loss_criterion = Events_Spotting_Loss(weights=weights_events, num_events=num_events)\n",
    "        self.seg_loss_criterion = Segmentation_Loss()\n",
    "\n",
    "    def forward(self, resize_batch_input, org_ball_pos_xy, global_ball_pos_xy, target_events, target_seg):\n",
    "        log_vars_idx = 0\n",
    "        pred_ball_global, pred_ball_local, pred_events, pred_seg, local_ball_pos_xy = self.model(resize_batch_input,\n",
    "                                                                                                 org_ball_pos_xy)\n",
    "        # Create target for events spotting and ball position (local and global)\n",
    "        batch_size = pred_ball_global.size(0)\n",
    "        target_ball_global = torch.zeros_like(pred_ball_global)\n",
    "        for sample_idx in range(batch_size):\n",
    "            target_ball_global[sample_idx] = create_target_ball(global_ball_pos_xy[sample_idx], sigma=self.sigma,\n",
    "                                                                w=self.w, h=self.h,\n",
    "                                                                thresh_mask=self.thresh_ball_pos_mask,\n",
    "                                                                device=self.device)\n",
    "        global_ball_loss = self.ball_loss_criterion(pred_ball_global, target_ball_global)\n",
    "        total_loss = global_ball_loss / (torch.exp(2 * self.log_vars[log_vars_idx])) + self.log_vars[log_vars_idx]\n",
    "\n",
    "        if pred_ball_local is not None:\n",
    "            log_vars_idx += 1\n",
    "            target_ball_local = torch.zeros_like(pred_ball_local)\n",
    "            for sample_idx in range(batch_size):\n",
    "                target_ball_local[sample_idx] = create_target_ball(local_ball_pos_xy[sample_idx], sigma=self.sigma,\n",
    "                                                                   w=self.w, h=self.h,\n",
    "                                                                   thresh_mask=self.thresh_ball_pos_mask,\n",
    "                                                                   device=self.device)\n",
    "            local_ball_loss = self.ball_loss_criterion(pred_ball_local, target_ball_local)\n",
    "            total_loss += local_ball_loss / (torch.exp(2 * self.log_vars[log_vars_idx])) + self.log_vars[log_vars_idx]\n",
    "\n",
    "        if pred_events is not None:\n",
    "            log_vars_idx += 1\n",
    "            target_events = target_events.to(device=self.device)\n",
    "            event_loss = self.event_loss_criterion(pred_events, target_events)\n",
    "            total_loss += event_loss / (2 * torch.exp(self.log_vars[log_vars_idx])) + self.log_vars[log_vars_idx]\n",
    "\n",
    "        if pred_seg is not None:\n",
    "            log_vars_idx += 1\n",
    "            seg_loss = self.seg_loss_criterion(pred_seg, target_seg)\n",
    "            total_loss += seg_loss / (2 * torch.exp(self.log_vars[log_vars_idx])) + self.log_vars[log_vars_idx]\n",
    "\n",
    "        # Final weights: [math.exp(log_var) ** 0.5 for log_var in log_vars]\n",
    "\n",
    "        return pred_ball_global, pred_ball_local, pred_events, pred_seg, local_ball_pos_xy, total_loss, self.log_vars.data.tolist()\n",
    "\n",
    "    def run_demo(self, resize_batch_input):\n",
    "        pred_ball_global, pred_ball_local, pred_events, pred_seg = self.model.run_demo(resize_batch_input)\n",
    "        return pred_ball_global, pred_ball_local, pred_events, pred_seg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb1d4d8",
   "metadata": {},
   "source": [
    "##### Model Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "049f246d",
   "metadata": {
    "code_folding": [
     0,
     21,
     29,
     39,
     51,
     85,
     98
    ]
   },
   "outputs": [],
   "source": [
    "def create_model(configs):\n",
    "    \"\"\"Create model based on architecture name\"\"\"\n",
    "    if configs.arch == 'ttnet':\n",
    "        ttnet_model = TTNet(dropout_p=configs.dropout_p, tasks=configs.tasks, input_size=configs.input_size,\n",
    "                            thresh_ball_pos_mask=configs.thresh_ball_pos_mask,\n",
    "                            num_frames_sequence=configs.num_frames_sequence)\n",
    "    else:\n",
    "        assert False, 'Undefined model backbone'\n",
    "\n",
    "    if configs.multitask_learning == True:\n",
    "        model = Multi_Task_Learning_Model(ttnet_model, tasks=configs.tasks, num_events=configs.num_events,\n",
    "                                          weights_events=configs.events_weights_loss,\n",
    "                                          input_size=configs.input_size, sigma=configs.sigma,\n",
    "                                          thresh_ball_pos_mask=configs.thresh_ball_pos_mask, device=configs.device)\n",
    "    else:\n",
    "        model = Unbalance_Loss_Model(ttnet_model, tasks_loss_weight=configs.tasks_loss_weight,\n",
    "                                     weights_events=configs.events_weights_loss, input_size=configs.input_size,\n",
    "                                     sigma=configs.sigma, thresh_ball_pos_mask=configs.thresh_ball_pos_mask,\n",
    "                                     device=configs.device)\n",
    "\n",
    "    return model\n",
    "def get_num_parameters(model):\n",
    "    \"\"\"Count number of trained parameters of the model\"\"\"\n",
    "    if hasattr(model, 'module'):\n",
    "        num_parameters = sum(p.numel() for p in model.module.parameters() if p.requires_grad)\n",
    "    else:\n",
    "        num_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    return num_parameters\n",
    "def freeze_model(model, freeze_modules_list):\n",
    "    \"\"\"Freeze modules of the model based on the configuration\"\"\"\n",
    "    for layer_name, p in model.named_parameters():\n",
    "        p.requires_grad = True\n",
    "        for freeze_module in freeze_modules_list:\n",
    "            if freeze_module in layer_name:\n",
    "                p.requires_grad = False\n",
    "                break\n",
    "\n",
    "    return model\n",
    "def load_weights_local_stage(pretrained_dict):\n",
    "    \"\"\"Overwrite the weights of the global stage to the local stage\"\"\"\n",
    "    local_weights_dict = {}\n",
    "    for layer_name, v in pretrained_dict.items():\n",
    "        if 'ball_global_stage' in layer_name:\n",
    "            layer_name_parts = layer_name.split('.')\n",
    "            layer_name_parts[1] = 'ball_local_stage'\n",
    "            local_name = '.'.join(layer_name_parts)\n",
    "            local_weights_dict[local_name] = v\n",
    "\n",
    "    return {**pretrained_dict, **local_weights_dict}\n",
    "\n",
    "def load_pretrained_model(model, pretrained_path, gpu_idx, overwrite_global_2_local):\n",
    "    \"\"\"Load weights from the pretrained model\"\"\"\n",
    "    assert os.path.isfile(pretrained_path), \"=> no checkpoint found at '{}'\".format(pretrained_path)\n",
    "    if gpu_idx is None:\n",
    "        checkpoint = torch.load(pretrained_path, map_location='cpu')\n",
    "    else:\n",
    "        # Map model to be loaded to specified single gpu.\n",
    "        loc = 'cuda:{}'.format(gpu_idx)\n",
    "        checkpoint = torch.load(pretrained_path, map_location=loc)\n",
    "    pretrained_dict = checkpoint['state_dict']\n",
    "    if hasattr(model, 'module'):\n",
    "        model_state_dict = model.module.state_dict()\n",
    "        # 1. filter out unnecessary keys\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_state_dict}\n",
    "        # Load global to local stage\n",
    "        if overwrite_global_2_local:\n",
    "            pretrained_dict = load_weights_local_stage(pretrained_dict)\n",
    "        # 2. overwrite entries in the existing state dict\n",
    "        model_state_dict.update(pretrained_dict)\n",
    "        # 3. load the new state dict\n",
    "        model.module.load_state_dict(model_state_dict)\n",
    "    else:\n",
    "        model_state_dict = model.state_dict()\n",
    "        # 1. filter out unnecessary keys\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_state_dict}\n",
    "        # Load global to local stage\n",
    "        if overwrite_global_2_local:\n",
    "            pretrained_dict = load_weights_local_stage(pretrained_dict)\n",
    "        # 2. overwrite entries in the existing state dict\n",
    "        model_state_dict.update(pretrained_dict)\n",
    "        # 3. load the new state dict\n",
    "        model.load_state_dict(model_state_dict)\n",
    "    return model\n",
    "\n",
    "def resume_model(resume_path, arch, gpu_idx):\n",
    "    \"\"\"Resume training model from the previous trained checkpoint\"\"\"\n",
    "    assert os.path.isfile(resume_path), \"=> no checkpoint found at '{}'\".format(resume_path)\n",
    "    if gpu_idx is None:\n",
    "        checkpoint = torch.load(resume_path, map_location='cpu')\n",
    "    else:\n",
    "        # Map model to be loaded to specified single gpu.\n",
    "        loc = 'cuda:{}'.format(gpu_idx)\n",
    "        checkpoint = torch.load(resume_path, map_location=loc)\n",
    "    assert arch == checkpoint['configs'].arch, \"Load the different arch...\"\n",
    "    print(\"=> loaded checkpoint '{}' (epoch {})\".format(resume_path, checkpoint['epoch']))\n",
    "\n",
    "    return checkpoint\n",
    "def make_data_parallel(model, configs):\n",
    "    if configs.distributed:\n",
    "        # For multiprocessing distributed, DistributedDataParallel constructor\n",
    "        # should always set the single device scope, otherwise,\n",
    "        # DistributedDataParallel will use all available devices.\n",
    "        if configs.gpu_idx is not None:\n",
    "            torch.cuda.set_device(configs.gpu_idx)\n",
    "            model.cuda(configs.gpu_idx)\n",
    "            # When using a single GPU per process and per\n",
    "            # DistributedDataParallel, we need to divide the batch size\n",
    "            # ourselves based on the total number of GPUs we have\n",
    "            configs.batch_size = int(configs.batch_size / configs.ngpus_per_node)\n",
    "            configs.num_workers = int((configs.num_workers + configs.ngpus_per_node - 1) / configs.ngpus_per_node)\n",
    "            model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[configs.gpu_idx],\n",
    "                                                              find_unused_parameters=True)\n",
    "        else:\n",
    "            model.cuda()\n",
    "            # DistributedDataParallel will divide and allocate batch_size to all\n",
    "            # available GPUs if device_ids are not set\n",
    "            model = torch.nn.parallel.DistributedDataParallel(model)\n",
    "    elif configs.gpu_idx is not None:\n",
    "        torch.cuda.set_device(configs.gpu_idx)\n",
    "        model = model.cuda(configs.gpu_idx)\n",
    "    else:\n",
    "        # DataParallel will divide and allocate batch_size to all available GPUs\n",
    "        model = torch.nn.DataParallel(model).cuda()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f7bc4f",
   "metadata": {},
   "source": [
    "### Training Debugger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "185839b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters:  15.963138 M\n"
     ]
    }
   ],
   "source": [
    "model = create_model(configs)\n",
    "# model = make_data_parallel(model, configs)\n",
    "model = freeze_model(model, configs.freeze_modules_list)\n",
    "print(\"Total Parameters: \", get_num_parameters(model) / 1000000 , \"M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0a346e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../checkpoints/ttnet/ttnet_1st_phase_wtt_epoch_30.pth None\n"
     ]
    }
   ],
   "source": [
    "optimizer       = create_optimizer(configs, model)\n",
    "lr_scheduler    = create_lr_scheduler(optimizer, configs)\n",
    "best_val_loss   = np.inf\n",
    "earlystop_count = 0\n",
    "is_best         = False\n",
    "print(configs.pretrained_path, configs.resume_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d928a2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # optionally load weight from a checkpoint\n",
    "# if configs.pretrained_path is not None:\n",
    "#     model = load_pretrained_model(model, configs.pretrained_path, gpu_idx, configs.overwrite_global_2_local)\n",
    "#     if logger is not None:\n",
    "#         logger.info('loaded pretrained model at {}'.format(configs.pretrained_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a4f6bd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # optionally resume from a checkpoint\n",
    "# if configs.resume_path is not None:\n",
    "#     checkpoint = resume_model(configs.resume_path, configs.arch, configs.gpu_idx)\n",
    "#     if hasattr(model, 'module'):\n",
    "#         model.module.load_state_dict(checkpoint['state_dict'])\n",
    "#     else:\n",
    "#         model.load_state_dict(checkpoint['state_dict'])\n",
    "#     optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "#     lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "#     best_val_loss = checkpoint['best_val_loss']\n",
    "#     earlystop_count = checkpoint['earlystop_count']\n",
    "#     configs.start_epoch = checkpoint['epoch'] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2c69a021",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wtt_1']\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# for i in range(3):\n",
    "#     configs['train_game_list'].pop()\n",
    "# configs['train_game_list'].pop(0)\n",
    "configs['train_game_list'] = ['wtt_1']\n",
    "print(configs['train_game_list'])\n",
    "# for i in enumerate(train_loader):\n",
    "#     print(len(i[1]))\n",
    "print(configs['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0f9e945b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len train_dataloader: 369, val_dataloader: 93\n"
     ]
    }
   ],
   "source": [
    "# Create dataloader\n",
    "train_loader, val_loader, train_sampler = check_dataloader()\n",
    "# train_loader, val_loader, train_sampler = create_train_val_dataloader(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "30fdcefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"../../dataset/training/annotations/wtt_1/segmentation_masks/3775.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a3f56445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(384, 640, 3)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aa5556fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f8e8dcca780>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dd8d9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963f1390",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3d0a19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8dd9d1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-18 18:35:02,472: logger.py - info(), at Line 38:INFO:\n",
      "number of batches in train set: 369\n",
      "2022-12-18 18:35:02,472: logger.py - info(), at Line 38:INFO:\n",
      "number of batches in val set: 93\n",
      "2022-12-18 18:35:02,473: logger.py - info(), at Line 38:INFO:\n",
      "number of batches in test set: 157\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_loader = create_test_dataloader(configs)\n",
    "if logger is not None:\n",
    "    logger.info('number of batches in train set: {}'.format(len(train_loader)))\n",
    "    if val_loader is not None:\n",
    "        logger.info('number of batches in val set: {}'.format(len(val_loader)))\n",
    "    logger.info('number of batches in test set: {}'.format(len(test_loader)))\n",
    "\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cf76a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "78d444d2",
   "metadata": {
    "code_folding": [
     0,
     47
    ]
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(train_loader, model, optimizer, epoch, configs, logger):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    data_time = AverageMeter('Data', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "\n",
    "    progress = ProgressMeter(len(train_loader), [batch_time, data_time, losses],\n",
    "                             prefix=\"Train - Epoch: [{}/{}]\".format(epoch, configs.num_epochs))\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    for batch_idx, (resized_imgs, org_ball_pos_xy, global_ball_pos_xy, target_events, target_seg) in enumerate(\n",
    "            tqdm(train_loader)):\n",
    "        data_time.update(time.time() - start_time)\n",
    "        batch_size = resized_imgs.size(0)\n",
    "        target_seg = target_seg.to(configs.device, non_blocking=True)\n",
    "        resized_imgs = resized_imgs.to(configs.device, non_blocking=True).float()\n",
    "        pred_ball_global, pred_ball_local, pred_events, pred_seg, local_ball_pos_xy, total_loss, _ = model(\n",
    "            resized_imgs, org_ball_pos_xy, global_ball_pos_xy, target_events, target_seg)\n",
    "        # For torch.nn.DataParallel case\n",
    "        if (not configs.distributed) and (configs.gpu_idx is None):\n",
    "            total_loss = torch.mean(total_loss)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # compute gradient and perform backpropagation\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if configs.distributed:\n",
    "            reduced_loss = reduce_tensor(total_loss.data, configs.world_size)\n",
    "        else:\n",
    "            reduced_loss = total_loss.data\n",
    "        losses.update(to_python_float(reduced_loss), batch_size)\n",
    "        # measure elapsed time\n",
    "        torch.cuda.synchronize()\n",
    "        batch_time.update(time.time() - start_time)\n",
    "\n",
    "        # Log message\n",
    "        if logger is not None:\n",
    "            if ((batch_idx + 1) % configs.print_freq) == 0:\n",
    "                logger.info(progress.get_message(batch_idx))\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "    return losses.avg\n",
    "\n",
    "def evaluate_one_epoch(val_loader, model, epoch, configs, logger):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    data_time = AverageMeter('Data', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "\n",
    "    progress = ProgressMeter(len(val_loader), [batch_time, data_time, losses],\n",
    "                             prefix=\"Evaluate - Epoch: [{}/{}]\".format(epoch, configs.num_epochs))\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        start_time = time.time()\n",
    "        for batch_idx, (resized_imgs, org_ball_pos_xy, global_ball_pos_xy, target_events, target_seg) in enumerate(\n",
    "                tqdm(val_loader)):\n",
    "            data_time.update(time.time() - start_time)\n",
    "            batch_size = resized_imgs.size(0)\n",
    "            target_seg = target_seg.to(configs.device, non_blocking=True)\n",
    "            resized_imgs = resized_imgs.to(configs.device, non_blocking=True).float()\n",
    "            pred_ball_global, pred_ball_local, pred_events, pred_seg, local_ball_pos_xy, total_loss, _ = model(\n",
    "                resized_imgs, org_ball_pos_xy, global_ball_pos_xy, target_events, target_seg)\n",
    "\n",
    "            # For torch.nn.DataParallel case\n",
    "            if (not configs.distributed) and (configs.gpu_idx is None):\n",
    "                total_loss = torch.mean(total_loss)\n",
    "\n",
    "            if configs.distributed:\n",
    "                reduced_loss = reduce_tensor(total_loss.data, configs.world_size)\n",
    "            else:\n",
    "                reduced_loss = total_loss.data\n",
    "            losses.update(to_python_float(reduced_loss), batch_size)\n",
    "            # measure elapsed time\n",
    "            torch.cuda.synchronize()\n",
    "            batch_time.update(time.time() - start_time)\n",
    "\n",
    "            # Log message\n",
    "            if logger is not None:\n",
    "                if ((batch_idx + 1) % configs.print_freq) == 0:\n",
    "                    logger.info(progress.get_message(batch_idx))\n",
    "\n",
    "            start_time = time.time()\n",
    "    print(\"Loss: \", losses.avg)\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cf64fa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(configs.device)\n",
    "configs.batch_size = 64\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "75af42e1",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-18 18:35:09,578: logger.py - info(), at Line 38:INFO:\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "2022-12-18 18:35:09,579: logger.py - info(), at Line 38:INFO:\n",
      "=================================== 1/30 ===================================\n",
      "2022-12-18 18:35:09,580: logger.py - info(), at Line 38:INFO:\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "2022-12-18 18:35:09,581: logger.py - info(), at Line 38:INFO:\n",
      ">>> Epoch: [1/30] learning rate: 1.00e-03\n",
      "100%|| 369/369 [02:03<00:00,  2.98it/s]\n",
      "100%|| 157/157 [01:29<00:00,  1.76it/s]\n",
      "2022-12-18 18:38:42,712: logger.py - info(), at Line 38:INFO:\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "2022-12-18 18:38:42,713: logger.py - info(), at Line 38:INFO:\n",
      "=================================== 2/30 ===================================\n",
      "2022-12-18 18:38:42,714: logger.py - info(), at Line 38:INFO:\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "2022-12-18 18:38:42,715: logger.py - info(), at Line 38:INFO:\n",
      ">>> Epoch: [2/30] learning rate: 1.00e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.19032511512015052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 369/369 [02:15<00:00,  2.72it/s]\n",
      "100%|| 157/157 [01:31<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.20757365461734614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-18 18:42:29,728: logger.py - info(), at Line 38:INFO:\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "2022-12-18 18:42:29,729: logger.py - info(), at Line 38:INFO:\n",
      "=================================== 3/30 ===================================\n",
      "2022-12-18 18:42:29,729: logger.py - info(), at Line 38:INFO:\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "2022-12-18 18:42:29,730: logger.py - info(), at Line 38:INFO:\n",
      ">>> Epoch: [3/30] learning rate: 1.00e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save a checkpoint at ../../checkpoints/ttnet/ttnet_2nd_phase_wtt_epoch_2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 369/369 [02:16<00:00,  2.70it/s]\n",
      "100%|| 157/157 [01:32<00:00,  1.69it/s]\n",
      "2022-12-18 18:46:19,057: logger.py - info(), at Line 38:INFO:\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "2022-12-18 18:46:19,057: logger.py - info(), at Line 38:INFO:\n",
      "=================================== 4/30 ===================================\n",
      "2022-12-18 18:46:19,059: logger.py - info(), at Line 38:INFO:\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "2022-12-18 18:46:19,059: logger.py - info(), at Line 38:INFO:\n",
      ">>> Epoch: [4/30] learning rate: 1.00e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.19113365840613367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 369/369 [02:17<00:00,  2.68it/s]\n",
      "100%|| 157/157 [01:34<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.1562670284468377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-18 18:50:11,189: logger.py - info(), at Line 38:INFO:\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "2022-12-18 18:50:11,190: logger.py - info(), at Line 38:INFO:\n",
      "=================================== 5/30 ===================================\n",
      "2022-12-18 18:50:11,190: logger.py - info(), at Line 38:INFO:\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "2022-12-18 18:50:11,191: logger.py - info(), at Line 38:INFO:\n",
      ">>> Epoch: [5/30] learning rate: 1.00e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save a checkpoint at ../../checkpoints/ttnet/ttnet_2nd_phase_wtt_epoch_4.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 369/369 [02:17<00:00,  2.68it/s]\n",
      "100%|| 157/157 [01:34<00:00,  1.66it/s]\n",
      "2022-12-18 18:54:03,641: logger.py - info(), at Line 38:INFO:\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "2022-12-18 18:54:03,642: logger.py - info(), at Line 38:INFO:\n",
      "=================================== 6/30 ===================================\n",
      "2022-12-18 18:54:03,642: logger.py - info(), at Line 38:INFO:\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "2022-12-18 18:54:03,643: logger.py - info(), at Line 38:INFO:\n",
      ">>> Epoch: [6/30] learning rate: 1.00e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.14842644089323825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 369/369 [02:17<00:00,  2.69it/s]\n",
      "100%|| 157/157 [01:36<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.13668974226595687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-18 18:57:57,633: logger.py - info(), at Line 38:INFO:\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "2022-12-18 18:57:57,634: logger.py - info(), at Line 38:INFO:\n",
      "=================================== 7/30 ===================================\n",
      "2022-12-18 18:57:57,635: logger.py - info(), at Line 38:INFO:\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "2022-12-18 18:57:57,635: logger.py - info(), at Line 38:INFO:\n",
      ">>> Epoch: [7/30] learning rate: 1.00e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save a checkpoint at ../../checkpoints/ttnet/ttnet_2nd_phase_wtt_epoch_6.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 369/369 [02:16<00:00,  2.71it/s]\n",
      "100%|| 157/157 [01:36<00:00,  1.63it/s]\n",
      "2022-12-18 19:01:50,292: logger.py - info(), at Line 38:INFO:\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "2022-12-18 19:01:50,293: logger.py - info(), at Line 38:INFO:\n",
      "=================================== 8/30 ===================================\n",
      "2022-12-18 19:01:50,294: logger.py - info(), at Line 38:INFO:\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "2022-12-18 19:01:50,296: logger.py - info(), at Line 38:INFO:\n",
      ">>> Epoch: [8/30] learning rate: 1.00e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.13640662158462807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 369/369 [02:15<00:00,  2.72it/s]\n",
      "100%|| 157/157 [01:37<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.14697804226955327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-18 19:05:43,744: logger.py - info(), at Line 38:INFO:\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "2022-12-18 19:05:43,745: logger.py - info(), at Line 38:INFO:\n",
      "=================================== 9/30 ===================================\n",
      "2022-12-18 19:05:43,746: logger.py - info(), at Line 38:INFO:\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "2022-12-18 19:05:43,746: logger.py - info(), at Line 38:INFO:\n",
      ">>> Epoch: [9/30] learning rate: 1.00e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save a checkpoint at ../../checkpoints/ttnet/ttnet_2nd_phase_wtt_epoch_8.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 369/369 [02:15<00:00,  2.72it/s]\n",
      "100%|| 157/157 [01:38<00:00,  1.60it/s]\n",
      "2022-12-18 19:09:37,481: logger.py - info(), at Line 38:INFO:\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "2022-12-18 19:09:37,482: logger.py - info(), at Line 38:INFO:\n",
      "=================================== 10/30 ===================================\n",
      "2022-12-18 19:09:37,482: logger.py - info(), at Line 38:INFO:\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "2022-12-18 19:09:37,483: logger.py - info(), at Line 38:INFO:\n",
      ">>> Epoch: [10/30] learning rate: 1.00e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.1515995601214941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 369/369 [02:15<00:00,  2.72it/s]\n",
      "100%|| 157/157 [01:38<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.1712522576257912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-18 19:13:31,619: logger.py - info(), at Line 38:INFO:\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "2022-12-18 19:13:31,620: logger.py - info(), at Line 38:INFO:\n",
      "=================================== 11/30 ===================================\n",
      "2022-12-18 19:13:31,621: logger.py - info(), at Line 38:INFO:\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "2022-12-18 19:13:31,621: logger.py - info(), at Line 38:INFO:\n",
      ">>> Epoch: [11/30] learning rate: 1.00e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save a checkpoint at ../../checkpoints/ttnet/ttnet_2nd_phase_wtt_epoch_10.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 369/369 [02:15<00:00,  2.73it/s]\n",
      "100%|| 157/157 [01:38<00:00,  1.60it/s]\n",
      "2022-12-18 19:17:24,884: logger.py - info(), at Line 38:INFO:\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "2022-12-18 19:17:24,885: logger.py - info(), at Line 38:INFO:\n",
      "=================================== 12/30 ===================================\n",
      "2022-12-18 19:17:24,886: logger.py - info(), at Line 38:INFO:\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "2022-12-18 19:17:24,887: logger.py - info(), at Line 38:INFO:\n",
      ">>> Epoch: [12/30] learning rate: 1.00e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.19789395781038413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 369/369 [02:16<00:00,  2.71it/s]\n",
      "100%|| 157/157 [01:37<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.16032164114774003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-18 19:21:18,603: logger.py - info(), at Line 38:INFO:\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "2022-12-18 19:21:18,604: logger.py - info(), at Line 38:INFO:\n",
      "=================================== 13/30 ===================================\n",
      "2022-12-18 19:21:18,604: logger.py - info(), at Line 38:INFO:\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "2022-12-18 19:21:18,604: logger.py - info(), at Line 38:INFO:\n",
      ">>> Epoch: [13/30] learning rate: 1.00e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save a checkpoint at ../../checkpoints/ttnet/ttnet_2nd_phase_wtt_epoch_12.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 369/369 [02:16<00:00,  2.71it/s]\n",
      "100%|| 157/157 [01:36<00:00,  1.63it/s]\n",
      "2022-12-18 19:25:11,152: logger.py - info(), at Line 38:INFO:\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "2022-12-18 19:25:11,153: logger.py - info(), at Line 38:INFO:\n",
      "=================================== 14/30 ===================================\n",
      "2022-12-18 19:25:11,153: logger.py - info(), at Line 38:INFO:\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "2022-12-18 19:25:11,155: logger.py - info(), at Line 38:INFO:\n",
      ">>> Epoch: [14/30] learning rate: 1.00e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.15179577463871632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 369/369 [02:16<00:00,  2.70it/s]\n",
      "100%|| 157/157 [01:36<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.18797919981392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-18 19:29:04,117: logger.py - info(), at Line 38:INFO:\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "2022-12-18 19:29:04,118: logger.py - info(), at Line 38:INFO:\n",
      "=================================== 15/30 ===================================\n",
      "2022-12-18 19:29:04,118: logger.py - info(), at Line 38:INFO:\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "2022-12-18 19:29:04,118: logger.py - info(), at Line 38:INFO:\n",
      ">>> Epoch: [15/30] learning rate: 1.00e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save a checkpoint at ../../checkpoints/ttnet/ttnet_2nd_phase_wtt_epoch_14.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 369/369 [02:17<00:00,  2.68it/s]\n",
      "100%|| 157/157 [01:35<00:00,  1.65it/s]\n",
      "2022-12-18 19:32:57,196: logger.py - info(), at Line 38:INFO:\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "2022-12-18 19:32:57,196: logger.py - info(), at Line 38:INFO:\n",
      "=================================== 16/30 ===================================\n",
      "2022-12-18 19:32:57,197: logger.py - info(), at Line 38:INFO:\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "2022-12-18 19:32:57,197: logger.py - info(), at Line 38:INFO:\n",
      ">>> Epoch: [16/30] learning rate: 1.00e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.1924124499013847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 369/369 [02:18<00:00,  2.66it/s]\n",
      "100%|| 157/157 [01:34<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.14439348075052622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-18 19:36:50,642: logger.py - info(), at Line 38:INFO:\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "2022-12-18 19:36:50,643: logger.py - info(), at Line 38:INFO:\n",
      "=================================== 17/30 ===================================\n",
      "2022-12-18 19:36:50,644: logger.py - info(), at Line 38:INFO:\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "2022-12-18 19:36:50,644: logger.py - info(), at Line 38:INFO:\n",
      ">>> Epoch: [17/30] learning rate: 1.00e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save a checkpoint at ../../checkpoints/ttnet/ttnet_2nd_phase_wtt_epoch_16.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 369/369 [02:20<00:00,  2.63it/s]\n",
      "100%|| 157/157 [01:34<00:00,  1.67it/s]\n",
      "2022-12-18 19:40:44,836: logger.py - info(), at Line 38:INFO:\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "2022-12-18 19:40:44,836: logger.py - info(), at Line 38:INFO:\n",
      "=================================== 18/30 ===================================\n",
      "2022-12-18 19:40:44,837: logger.py - info(), at Line 38:INFO:\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "2022-12-18 19:40:44,837: logger.py - info(), at Line 38:INFO:\n",
      ">>> Epoch: [18/30] learning rate: 1.00e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.14972659825611825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 369/369 [02:18<00:00,  2.66it/s]\n",
      "100%|| 157/157 [01:33<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.17043470212228132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-18 19:44:37,218: logger.py - info(), at Line 38:INFO:\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "2022-12-18 19:44:37,219: logger.py - info(), at Line 38:INFO:\n",
      "=================================== 19/30 ===================================\n",
      "2022-12-18 19:44:37,220: logger.py - info(), at Line 38:INFO:\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "2022-12-18 19:44:37,220: logger.py - info(), at Line 38:INFO:\n",
      ">>> Epoch: [19/30] learning rate: 1.00e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save a checkpoint at ../../checkpoints/ttnet/ttnet_2nd_phase_wtt_epoch_18.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 369/369 [02:19<00:00,  2.65it/s]\n",
      "100%|| 157/157 [01:33<00:00,  1.68it/s]\n",
      "2022-12-18 19:48:29,764: logger.py - info(), at Line 38:INFO:\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "2022-12-18 19:48:29,765: logger.py - info(), at Line 38:INFO:\n",
      "=================================== 20/30 ===================================\n",
      "2022-12-18 19:48:29,766: logger.py - info(), at Line 38:INFO:\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "2022-12-18 19:48:29,767: logger.py - info(), at Line 38:INFO:\n",
      ">>> Epoch: [20/30] learning rate: 1.00e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.14511783799721195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 369/369 [02:18<00:00,  2.66it/s]\n",
      "100%|| 157/157 [01:33<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.16377656188777107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-18 19:52:21,828: logger.py - info(), at Line 38:INFO:\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "2022-12-18 19:52:21,829: logger.py - info(), at Line 38:INFO:\n",
      "=================================== 21/30 ===================================\n",
      "2022-12-18 19:52:21,829: logger.py - info(), at Line 38:INFO:\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "2022-12-18 19:52:21,830: logger.py - info(), at Line 38:INFO:\n",
      ">>> Epoch: [21/30] learning rate: 1.00e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save a checkpoint at ../../checkpoints/ttnet/ttnet_2nd_phase_wtt_epoch_20.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 369/369 [02:19<00:00,  2.65it/s]\n",
      "100%|| 157/157 [01:33<00:00,  1.68it/s]\n",
      "2022-12-18 19:56:14,322: logger.py - info(), at Line 38:INFO:\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "2022-12-18 19:56:14,323: logger.py - info(), at Line 38:INFO:\n",
      "=================================== 22/30 ===================================\n",
      "2022-12-18 19:56:14,324: logger.py - info(), at Line 38:INFO:\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "2022-12-18 19:56:14,325: logger.py - info(), at Line 38:INFO:\n",
      ">>> Epoch: [22/30] learning rate: 1.00e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.1587965699130065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 369/369 [02:20<00:00,  2.63it/s]\n",
      "100%|| 157/157 [01:33<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.17354723752402346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-18 20:00:08,435: logger.py - info(), at Line 38:INFO:\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "2022-12-18 20:00:08,436: logger.py - info(), at Line 38:INFO:\n",
      "=================================== 23/30 ===================================\n",
      "2022-12-18 20:00:08,436: logger.py - info(), at Line 38:INFO:\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "2022-12-18 20:00:08,437: logger.py - info(), at Line 38:INFO:\n",
      ">>> Epoch: [23/30] learning rate: 1.00e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save a checkpoint at ../../checkpoints/ttnet/ttnet_2nd_phase_wtt_epoch_22.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 369/369 [02:18<00:00,  2.66it/s]\n",
      "100%|| 157/157 [01:34<00:00,  1.66it/s]\n",
      "2022-12-18 20:04:01,839: logger.py - info(), at Line 38:INFO:\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "2022-12-18 20:04:01,840: logger.py - info(), at Line 38:INFO:\n",
      "=================================== 24/30 ===================================\n",
      "2022-12-18 20:04:01,842: logger.py - info(), at Line 38:INFO:\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "2022-12-18 20:04:01,842: logger.py - info(), at Line 38:INFO:\n",
      ">>> Epoch: [24/30] learning rate: 1.00e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.1554894600744316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 369/369 [02:18<00:00,  2.66it/s]\n",
      "100%|| 157/157 [01:35<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.16766055434005464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-18 20:07:55,830: logger.py - info(), at Line 38:INFO:\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "2022-12-18 20:07:55,831: logger.py - info(), at Line 38:INFO:\n",
      "=================================== 25/30 ===================================\n",
      "2022-12-18 20:07:55,831: logger.py - info(), at Line 38:INFO:\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "2022-12-18 20:07:55,832: logger.py - info(), at Line 38:INFO:\n",
      ">>> Epoch: [25/30] learning rate: 1.00e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save a checkpoint at ../../checkpoints/ttnet/ttnet_2nd_phase_wtt_epoch_24.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 369/369 [02:17<00:00,  2.68it/s]\n",
      "100%|| 157/157 [01:35<00:00,  1.64it/s]\n",
      "2022-12-18 20:11:49,110: logger.py - info(), at Line 38:INFO:\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "2022-12-18 20:11:49,111: logger.py - info(), at Line 38:INFO:\n",
      "=================================== 26/30 ===================================\n",
      "2022-12-18 20:11:49,111: logger.py - info(), at Line 38:INFO:\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "2022-12-18 20:11:49,112: logger.py - info(), at Line 38:INFO:\n",
      ">>> Epoch: [26/30] learning rate: 1.00e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.16796218556613446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 369/369 [02:17<00:00,  2.68it/s]\n",
      "100%|| 157/157 [01:36<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.14875394167040154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-18 20:15:43,746: logger.py - info(), at Line 38:INFO:\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "2022-12-18 20:15:43,747: logger.py - info(), at Line 38:INFO:\n",
      "=================================== 27/30 ===================================\n",
      "2022-12-18 20:15:43,748: logger.py - info(), at Line 38:INFO:\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "2022-12-18 20:15:43,749: logger.py - info(), at Line 38:INFO:\n",
      ">>> Epoch: [27/30] learning rate: 1.00e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save a checkpoint at ../../checkpoints/ttnet/ttnet_2nd_phase_wtt_epoch_26.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 369/369 [02:17<00:00,  2.68it/s]\n",
      "100%|| 157/157 [01:37<00:00,  1.62it/s]\n",
      "2022-12-18 20:19:38,468: logger.py - info(), at Line 38:INFO:\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "2022-12-18 20:19:38,468: logger.py - info(), at Line 38:INFO:\n",
      "=================================== 28/30 ===================================\n",
      "2022-12-18 20:19:38,469: logger.py - info(), at Line 38:INFO:\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "2022-12-18 20:19:38,469: logger.py - info(), at Line 38:INFO:\n",
      ">>> Epoch: [28/30] learning rate: 1.00e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.16616499842356416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 369/369 [02:17<00:00,  2.68it/s]\n",
      "100%|| 157/157 [01:37<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.15679207741102769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-18 20:23:34,295: logger.py - info(), at Line 38:INFO:\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "2022-12-18 20:23:34,296: logger.py - info(), at Line 38:INFO:\n",
      "=================================== 29/30 ===================================\n",
      "2022-12-18 20:23:34,297: logger.py - info(), at Line 38:INFO:\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "2022-12-18 20:23:34,297: logger.py - info(), at Line 38:INFO:\n",
      ">>> Epoch: [29/30] learning rate: 1.00e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save a checkpoint at ../../checkpoints/ttnet/ttnet_2nd_phase_wtt_epoch_28.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 369/369 [02:16<00:00,  2.71it/s]\n",
      "100%|| 157/157 [01:38<00:00,  1.59it/s]\n",
      "2022-12-18 20:27:28,982: logger.py - info(), at Line 38:INFO:\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "2022-12-18 20:27:28,983: logger.py - info(), at Line 38:INFO:\n",
      "=================================== 30/30 ===================================\n",
      "2022-12-18 20:27:28,984: logger.py - info(), at Line 38:INFO:\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "2022-12-18 20:27:28,984: logger.py - info(), at Line 38:INFO:\n",
      ">>> Epoch: [30/30] learning rate: 1.00e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.16005189423873567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 369/369 [02:19<00:00,  2.65it/s]\n",
      "100%|| 157/157 [01:40<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.16488146669190173\n",
      "save a checkpoint at ../../checkpoints/ttnet/ttnet_2nd_phase_wtt_epoch_30.pth\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(configs.start_epoch, configs.num_epochs + 1):\n",
    "        # Get the current learning rate\n",
    "        for param_group in optimizer.param_groups:\n",
    "            lr = param_group['lr']\n",
    "        if logger is not None:\n",
    "            logger.info('{}'.format('*-' * 40))\n",
    "            logger.info('{} {}/{} {}'.format('=' * 35, epoch, configs.num_epochs, '=' * 35))\n",
    "            logger.info('{}'.format('*-' * 40))\n",
    "            logger.info('>>> Epoch: [{}/{}] learning rate: {:.2e}'.format(epoch, configs.num_epochs, lr))\n",
    "            \n",
    "            if configs.distributed:\n",
    "                train_sampler.set_epoch(epoch)\n",
    "            \n",
    "            # train for one epoch\n",
    "            train_loss = train_one_epoch(train_loader, model, optimizer, epoch, configs, logger)\n",
    "            loss_dict = {'train': train_loss}\n",
    "            if not configs.no_val:\n",
    "                val_loss = evaluate_one_epoch(val_loader, model, epoch, configs, logger)\n",
    "                is_best = val_loss <= best_val_loss\n",
    "                best_val_loss = min(val_loss, best_val_loss)\n",
    "                loss_dict['val'] = val_loss\n",
    "\n",
    "            if not configs.no_test:\n",
    "                test_loss = evaluate_one_epoch(test_loader, model, epoch, configs, logger)\n",
    "                loss_dict['test'] = test_loss\n",
    "            # Write tensorboard\n",
    "#             if tb_writer is not None:\n",
    "#                 tb_writer.add_scalars('Loss', loss_dict, epoch)\n",
    "            # Save checkpoint\n",
    "            if (is_best or ((epoch % configs.checkpoint_freq) == 0)):\n",
    "                saved_state = get_saved_state(model, optimizer, lr_scheduler, epoch, configs, best_val_loss,\n",
    "                                              earlystop_count)\n",
    "                save_checkpoint(configs.checkpoints_dir, configs.saved_fn, saved_state, is_best, epoch)\n",
    "            # Check early stop training\n",
    "            if configs.earlystop_patience is not None:\n",
    "                earlystop_count = 0 if is_best else (earlystop_count + 1)\n",
    "                print_string = ' |||\\t earlystop_count: {}'.format(earlystop_count)\n",
    "                if configs.earlystop_patience <= earlystop_count:\n",
    "                    print_string += '\\n\\t--- Early stopping!!!'\n",
    "                    break\n",
    "                else:\n",
    "                    print_string += '\\n\\t--- Continue training..., earlystop_count: {}'.format(earlystop_count)\n",
    "                if logger is not None:\n",
    "                    logger.info(print_string)\n",
    "            # Adjust learning rate\n",
    "            if configs.lr_type == 'plateau':\n",
    "                assert (not configs.no_val), \"Only use plateau when having validation set\"\n",
    "                lr_scheduler.step(val_loss)\n",
    "            else:\n",
    "                lr_scheduler.step()\n",
    "\n",
    "#         if tb_writer is not None:\n",
    "#             tb_writer.close()\n",
    "        if configs.distributed:\n",
    "            cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2986c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7a8a4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26429755",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06e4845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference of 3rd phase- 0\n",
    "# Check global ball detection- all phases - 2\n",
    "# Dataset preparation / automation - 1 \n",
    "# Yolo v7 Inference pipeline       - \n",
    "# Loss / feature extraction        -\n",
    "\n",
    "\n",
    "\n",
    "# autoencoders     \n",
    "# human bbox\n",
    "# clear ml logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4987166",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07af62a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d5b6fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e979edfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
