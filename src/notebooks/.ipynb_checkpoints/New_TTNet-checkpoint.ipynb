{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f86eb28c",
   "metadata": {},
   "source": [
    "# TTNET Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0573fe36",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a84b2f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d228fb7d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5f6a1b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from turbojpeg import TurboJPEG\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from data_process.ttnet_data_utils import load_raw_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7f38e7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "##  Dataset\n",
    "from utils.misc import *\n",
    "from utils.logger import Logger\n",
    "from config.config import parse_configs\n",
    "from utils.train_utils import create_optimizer, create_lr_scheduler,  reduce_tensor, to_python_float, get_saved_state, save_checkpoint\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0317525",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2928a482",
   "metadata": {},
   "source": [
    "### Configs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac6e3ff",
   "metadata": {},
   "source": [
    "##### Phase 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c97da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = parse_configs()\n",
    "configs.distributed = False\n",
    "# configs.multitask_learning = True\n",
    "\n",
    "# Phase 1\n",
    "configs.smooth_labelling = True\n",
    "configs.global_weight    = 5\n",
    "configs.lr_factor        = 0.5\n",
    "configs.saved_fn         = 'ttnet_1st_phase'\n",
    "configs.no_val           = True\n",
    "configs.lr               = 0.001 \n",
    "configs.lr_type          = 'step_lr' \n",
    "configs.lr_step_size     = 10 \n",
    "configs.lr_factor        = 0.1\n",
    "configs.gpu_idx          = 0 \n",
    "configs.global_weight    = 5. \n",
    "configs.no_event         = True\n",
    "configs.no_local         = True\n",
    "configs.print_freq       = 500\n",
    "configs.batch_size       = 24\n",
    "configs.seg_weight       = 1.\n",
    "configs.num_workers      = 8\n",
    "configs.smooth_labelling = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb357e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0817abe0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92e4258e",
   "metadata": {},
   "source": [
    "##### Phase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3e486f",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# # Phase 2\n",
    "# configs = parse_configs()\n",
    "# configs.distributed = False\n",
    "# # configs.multitask_learning = True\n",
    "\n",
    "# #Phase 2\n",
    "# configs.batch_size = 24\n",
    "# configs.num_workers = 8\n",
    "# configs.saved_fn  = 'ttnet_2nd_phase' \n",
    "# configs.no_val = True  \n",
    "# configs.lr = 0.001 \n",
    "# configs.lr_type = 'step_lr' \n",
    "# configs.lr_step_size =  10 \n",
    "# configs.lr_factor = 0.1 \n",
    "# configs.gpu_idx = 0 \n",
    "# configs.global_weight = 0. \n",
    "# configs.event_weight = 2. \n",
    "# configs.local_weight = 1. \n",
    "# configs.pretrained_path = \"../../checkpoints/ttnet/ttnet_1st_phase_epoch_30.pth\"\n",
    "# configs.overwrite_global_2_local  = True\n",
    "# configs.freeze_global  = True\n",
    "# configs.smooth_labelling = True\n",
    "# configs.sigma =  1.0\n",
    "# configs.print_freq =  1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc60649a",
   "metadata": {},
   "source": [
    "##### Phase 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70db7d9",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# # phase 3\n",
    "# configs = parse_configs()\n",
    "# configs.distributed = False\n",
    "# configs.batch_size = 24\n",
    "# configs.num_workers = 8\n",
    "# configs.saved_fn  = 'ttnet_3nd_phase' \n",
    "# configs.no_val = True  \n",
    "# configs.lr = 0.0001 \n",
    "# configs.lr_type = 'step_lr' \n",
    "# configs.lr_step_size =  10 \n",
    "# configs.lr_factor = 0.2 \n",
    "# configs.gpu_idx = 0 \n",
    "# configs.global_weight = 1. \n",
    "# configs.event_weight = 1. \n",
    "# configs.local_weight = 1. \n",
    "# configs.pretrained_path = \"../../checkpoints/ttnet/ttnet_2nd_phase_epoch_30.pth\"\n",
    "# configs.smooth_labelling = True\n",
    "# configs.sigma =  1.0\n",
    "# configs.print_freq =  1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc681181",
   "metadata": {},
   "source": [
    "##### Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f89947d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = Logger(configs.logs_dir, configs.saved_fn)\n",
    "logger.info('>>> Created a new logger')\n",
    "logger.info('>>> configs: {}'.format(configs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7084f0",
   "metadata": {},
   "source": [
    "### Dataloader Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b9f0f5",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### TTNet Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60df1e14",
   "metadata": {
    "code_folding": [
     13,
     16,
     19
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class TTNet_Dataset(Dataset):\n",
    "    def __init__(self, events_infor, org_size, input_size, transform=None, num_samples=None):\n",
    "        self.events_infor = events_infor\n",
    "        self.w_org = org_size[0]\n",
    "        self.h_org = org_size[1]\n",
    "        self.w_input = input_size[0]\n",
    "        self.h_input = input_size[1]\n",
    "        self.w_resize_ratio = self.w_org / self.w_input\n",
    "        self.h_resize_ratio = self.h_org / self.h_input\n",
    "        self.transform = transform\n",
    "        if num_samples is not None:\n",
    "            self.events_infor = self.events_infor[:num_samples]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.events_infor)\n",
    "\n",
    "    def __resize_ball_pos__(self, ball_pos_xy, w_ratio, h_ratio):\n",
    "        return np.array([ball_pos_xy[0] / w_ratio, ball_pos_xy[1] / h_ratio])\n",
    "\n",
    "    def __check_ball_pos__(self, ball_pos_xy, w, h):\n",
    "        if not ((0 < ball_pos_xy[0] < w) and (0 < ball_pos_xy[1] < h)):\n",
    "            ball_pos_xy[0] = -1.\n",
    "            ball_pos_xy[1] = -1.\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path_list, org_ball_pos_xy, target_events, seg_path = self.events_infor[index]\n",
    "        # Load segmentation\n",
    "        seg_img = load_raw_img(seg_path)\n",
    "        self.jpeg_reader = TurboJPEG()  # improve it later (Only initialize it once)\n",
    "        # Load a sequence of images (-4, 4), resize images before stacking them together\n",
    "        # Use TurboJPEG to speed up the loading images' phase\n",
    "        resized_imgs = []\n",
    "        for img_path in img_path_list:\n",
    "            in_file = open(img_path, 'rb')\n",
    "            resized_imgs.append(cv2.resize(self.jpeg_reader.decode(in_file.read(), 0), (self.w_input, self.h_input)))\n",
    "            in_file.close()\n",
    "        resized_imgs = np.dstack(resized_imgs)  # (128, 320, 27)\n",
    "        # Adjust ball pos: full HD --> (320, 128)\n",
    "        global_ball_pos_xy = self.__resize_ball_pos__(org_ball_pos_xy, self.w_resize_ratio, self.h_resize_ratio)\n",
    "\n",
    "        # Apply augmentation\n",
    "        if self.transform:\n",
    "            resized_imgs, global_ball_pos_xy, seg_img = self.transform(resized_imgs, global_ball_pos_xy, seg_img)\n",
    "        # Adjust ball pos: (320, 128) --> full HD\n",
    "        org_ball_pos_xy = self.__resize_ball_pos__(global_ball_pos_xy, 1. / self.w_resize_ratio,\n",
    "                                                   1. / self.h_resize_ratio)\n",
    "        # If the ball position is outside of the resized image, set position to -1, -1 --> No ball (just for safety)\n",
    "        self.__check_ball_pos__(org_ball_pos_xy, self.w_org, self.h_org)\n",
    "        self.__check_ball_pos__(global_ball_pos_xy, self.w_input, self.h_input)\n",
    "\n",
    "        # Transpose (H, W, C) to (C, H, W) --> fit input of Pytorch model\n",
    "        resized_imgs = resized_imgs.transpose(2, 0, 1)\n",
    "        target_seg = seg_img.transpose(2, 0, 1).astype(np.float)\n",
    "        # Segmentation mask should be 0 or 1\n",
    "        target_seg[target_seg < 75] = 0.\n",
    "        target_seg[target_seg >= 75] = 1.\n",
    "\n",
    "        return resized_imgs, org_ball_pos_xy.astype(np.int), global_ball_pos_xy.astype(np.int), target_events, target_seg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecc1e18",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7170c1e7",
   "metadata": {
    "code_folding": [
     1,
     11,
     22,
     33,
     51,
     90,
     117,
     118
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Transformations\n",
    "class Compose(object):\n",
    "    def __init__(self, transforms, p=1.0):\n",
    "        self.transforms = transforms\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, imgs, ball_position_xy, seg_img):\n",
    "        if random.random() <= self.p:\n",
    "            for t in self.transforms:\n",
    "                imgs, ball_position_xy, seg_img = t(imgs, ball_position_xy, seg_img)\n",
    "        return imgs, ball_position_xy, seg_img\n",
    "class Normalize():\n",
    "    def __init__(self, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), num_frames_sequence=9, p=1.0):\n",
    "        self.p = p\n",
    "        self.mean = np.repeat(np.array(mean).reshape(1, 1, 3), repeats=num_frames_sequence, axis=-1)\n",
    "        self.std = np.repeat(np.array(std).reshape(1, 1, 3), repeats=num_frames_sequence, axis=-1)\n",
    "\n",
    "    def __call__(self, imgs, ball_position_xy, seg_img):\n",
    "        if random.random() < self.p:\n",
    "            imgs = ((imgs / 255.) - self.mean) / self.std\n",
    "\n",
    "        return imgs, ball_position_xy, seg_img\n",
    "class Denormalize():\n",
    "    def __init__(self, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), p=1.0):\n",
    "        self.p = p\n",
    "        self.mean = np.array(mean).reshape(1, 1, 3)\n",
    "        self.std = np.array(std).reshape(1, 1, 3)\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img = (img * self.std + self.mean) * 255.\n",
    "        img = img.astype(np.uint8)\n",
    "\n",
    "        return img\n",
    "class Resize(object):\n",
    "    def __init__(self, new_size, p=0.5, interpolation=cv2.INTER_LINEAR):\n",
    "        self.new_size = new_size\n",
    "        self.p = p\n",
    "        self.interpolation = interpolation\n",
    "\n",
    "    def __call__(self, imgs, ball_position_xy, seg_img):\n",
    "        if random.random() <= self.p:\n",
    "            h, w, c = imgs.shape\n",
    "            # Resize a sequence of images\n",
    "            imgs = cv2.resize(imgs, self.new_size, interpolation=self.interpolation)\n",
    "            # Dont need to resize seg_img\n",
    "            # Adjust ball position\n",
    "            w_ratio = w / self.new_size[0]\n",
    "            h_ratio = h / self.new_size[1]\n",
    "            ball_position_xy = np.array([ball_position_xy[0] / w_ratio, ball_position_xy[1] / h_ratio])\n",
    "\n",
    "        return imgs, ball_position_xy, seg_img\n",
    "class Random_Crop(object):\n",
    "    def __init__(self, max_reduction_percent=0.15, p=0.5, interpolation=cv2.INTER_LINEAR):\n",
    "        self.max_reduction_percent = max_reduction_percent\n",
    "        self.p = p\n",
    "        self.interpolation = interpolation\n",
    "\n",
    "    def __call__(self, imgs, ball_position_xy, seg_img):\n",
    "        # imgs are before resizing\n",
    "        if random.random() <= self.p:\n",
    "            h, w, c = imgs.shape\n",
    "            # Calculate min_x, max_x, min_y, max_y\n",
    "            remain_percent = random.uniform(1. - self.max_reduction_percent, 1.)\n",
    "            new_w = remain_percent * w\n",
    "            min_x = int(random.uniform(0, w - new_w))\n",
    "            max_x = int(min_x + new_w)\n",
    "            w_ratio = w / new_w\n",
    "\n",
    "            new_h = remain_percent * h\n",
    "            min_y = int(random.uniform(0, h - new_h))\n",
    "            max_y = int(new_h + min_y)\n",
    "            h_ratio = h / new_h\n",
    "            # crop a sequence of images\n",
    "            imgs = imgs[min_y:max_y, min_x:max_x, :]\n",
    "            imgs = cv2.resize(imgs, (w, h), interpolation=self.interpolation)\n",
    "            # crop seg_img\n",
    "            seg_img_h, seg_img_w, _ = seg_img.shape\n",
    "            # 1. Resize to original\n",
    "            if (seg_img_h != h) or (seg_img_w != w):\n",
    "                seg_img = cv2.resize(seg_img, (w, h), interpolation=self.interpolation)\n",
    "            # 2. Crop\n",
    "            seg_img = seg_img[min_y:max_y, min_x:max_x, :]\n",
    "            # 3. Resize to (128, 320, 3)\n",
    "            seg_img = cv2.resize(seg_img, (seg_img_w, seg_img_h), interpolation=self.interpolation)\n",
    "\n",
    "            # Adjust ball position\n",
    "            ball_position_xy = np.array([(ball_position_xy[0] - min_x) * w_ratio,\n",
    "                                         (ball_position_xy[1] - min_y) * h_ratio])\n",
    "\n",
    "        return imgs, ball_position_xy, seg_img\n",
    "class Random_Rotate(object):\n",
    "    def __init__(self, rotation_angle_limit=15, p=0.5):\n",
    "        self.rotation_angle_limit = rotation_angle_limit\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, imgs, ball_position_xy, seg_img):\n",
    "        if random.random() <= self.p:\n",
    "            random_angle = random.uniform(-self.rotation_angle_limit, self.rotation_angle_limit)\n",
    "            # Rotate a sequence of imgs\n",
    "            h, w, c = imgs.shape\n",
    "            center = (int(w / 2), int(h / 2))\n",
    "            rotate_matrix = cv2.getRotationMatrix2D(center, random_angle, 1.)\n",
    "            imgs = cv2.warpAffine(imgs, rotate_matrix, (w, h), flags=cv2.INTER_LINEAR)\n",
    "\n",
    "            # Adjust ball position, apply the same rotate_matrix for the sequential images\n",
    "            ball_position_xy = rotate_matrix.dot(np.array([ball_position_xy[0], ball_position_xy[1], 1.]).T)\n",
    "\n",
    "            # Rotate seg_img\n",
    "            seg_h, seg_w, seg_c = seg_img.shape\n",
    "            if (seg_h != h) or (seg_w != w):\n",
    "                seg_center = (int(seg_w / 2), int(seg_h / 2))\n",
    "                seg_rotate_matrix = cv2.getRotationMatrix2D(seg_center, random_angle, 1.)\n",
    "            else:\n",
    "                seg_rotate_matrix = rotate_matrix\n",
    "            seg_img = cv2.warpAffine(seg_img, seg_rotate_matrix, (seg_w, seg_h), flags=cv2.INTER_LINEAR)\n",
    "\n",
    "        return imgs, ball_position_xy, seg_img\n",
    "class Random_HFlip(object):\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, imgs, ball_position_xy, seg_img):\n",
    "        if random.random() <= self.p:\n",
    "            h, w, c = imgs.shape\n",
    "            # Horizontal flip a sequence of imgs\n",
    "            imgs = cv2.flip(imgs, 1)\n",
    "            # Horizontal flip seg_img\n",
    "            seg_img = cv2.flip(seg_img, 1)\n",
    "\n",
    "            # Adjust ball position: Same y, new x = w - x\n",
    "            ball_position_xy[0] = w - ball_position_xy[0]\n",
    "\n",
    "        return imgs, ball_position_xy, seg_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd45187",
   "metadata": {},
   "source": [
    "##### TTNet Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2855c86",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def load_raw_img(img_path):\n",
    "    \"\"\"Load raw image based on the path to the image\"\"\"\n",
    "    img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)  # BGR --> RGB\n",
    "    return img\n",
    "def gaussian_1d(pos, muy, sigma):\n",
    "    \"\"\"Create 1D Gaussian distribution based on ball position (muy), and std (sigma)\"\"\"\n",
    "    target = torch.exp(- (((pos - muy) / sigma) ** 2) / 2)\n",
    "    return target\n",
    "def create_target_ball(ball_position_xy, sigma, w, h, thresh_mask, device):\n",
    "    \"\"\"Create target for the ball detection stages\n",
    "\n",
    "    :param ball_position_xy: Position of the ball (x,y)\n",
    "    :param sigma: standard deviation (a hyperparameter)\n",
    "    :param w: width of the resize image\n",
    "    :param h: height of the resize image\n",
    "    :param thresh_mask: if values of 1D Gaussian < thresh_mask --> set to 0 to reduce computation\n",
    "    :param device: cuda() or cpu()\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    w, h = int(w), int(h)\n",
    "    target_ball_position = torch.zeros((w + h,), device=device)\n",
    "    # Only do the next step if the ball is existed\n",
    "    if (w > ball_position_xy[0] > 0) and (h > ball_position_xy[1] > 0):\n",
    "        # For x\n",
    "        x_pos = torch.arange(0, w, device=device)\n",
    "        target_ball_position[:w] = gaussian_1d(x_pos, ball_position_xy[0], sigma=sigma)\n",
    "        # For y\n",
    "        y_pos = torch.arange(0, h, device=device)\n",
    "        target_ball_position[w:] = gaussian_1d(y_pos, ball_position_xy[1], sigma=sigma)\n",
    "\n",
    "        target_ball_position[target_ball_position < thresh_mask] = 0.\n",
    "\n",
    "    return target_ball_position\n",
    "def smooth_event_labelling(event_class, smooth_idx, event_frameidx):\n",
    "    target_events = np.zeros((2,))\n",
    "    if event_class < 2:\n",
    "        n = smooth_idx - event_frameidx\n",
    "        target_events[event_class] = np.cos(n * np.pi / 8)\n",
    "        target_events[target_events < 0.01] = 0.\n",
    "    return target_events\n",
    "def get_events_infor(game_list, configs, dataset_type):\n",
    "    \"\"\"Get information of sequences of images based on events\n",
    "\n",
    "    :param game_list: List of games (video names)\n",
    "    :return:\n",
    "    [\n",
    "        each event: [[img_path_list], ball_position, target_events, segmentation_path]\n",
    "    ]\n",
    "    \"\"\"\n",
    "    # the paper mentioned 25, but used 9 frames only\n",
    "    num_frames_from_event = int((configs.num_frames_sequence - 1) / 2)\n",
    "\n",
    "    annos_dir = os.path.join(configs.dataset_dir, dataset_type, 'annotations')\n",
    "    images_dir = os.path.join(configs.dataset_dir, dataset_type, 'images')\n",
    "    events_infor = []\n",
    "    events_labels = []\n",
    "    for game_name in game_list:\n",
    "        ball_annos_path = os.path.join(annos_dir, game_name, 'ball_markup.json')\n",
    "        events_annos_path = os.path.join(annos_dir, game_name, 'events_markup.json')\n",
    "        # Load ball annotations\n",
    "        json_ball = open(ball_annos_path)\n",
    "        ball_annos = json.load(json_ball)\n",
    "\n",
    "        # Load events annotations\n",
    "        json_events = open(events_annos_path)\n",
    "        events_annos = json.load(json_events)\n",
    "        for event_frameidx, event_name in events_annos.items():\n",
    "            event_frameidx = int(event_frameidx)\n",
    "            smooth_frame_indices = [event_frameidx]  # By default\n",
    "            if (event_name != 'empty_event') and (configs.smooth_labelling):\n",
    "                smooth_frame_indices = [idx for idx in range(event_frameidx - num_frames_from_event,\n",
    "                                                             event_frameidx + num_frames_from_event + 1)]\n",
    "\n",
    "            for smooth_idx in smooth_frame_indices:\n",
    "                sub_smooth_frame_indices = [idx for idx in range(smooth_idx - num_frames_from_event,\n",
    "                                                                 smooth_idx + num_frames_from_event + 1)]\n",
    "                img_path_list = []\n",
    "                for sub_smooth_idx in sub_smooth_frame_indices:\n",
    "                    img_path = os.path.join(images_dir, game_name, 'img_{:06d}.jpg'.format(sub_smooth_idx))\n",
    "                    img_path_list.append(img_path)\n",
    "                last_f_idx = smooth_idx + num_frames_from_event\n",
    "                # Get ball position for the last frame in the sequence\n",
    "                if '{}'.format(last_f_idx) not in ball_annos.keys():\n",
    "                    print('smooth_idx: {} - no ball position for the frame idx {}'.format(smooth_idx, last_f_idx))\n",
    "                    continue\n",
    "                ball_position_xy = ball_annos['{}'.format(last_f_idx)]\n",
    "                ball_position_xy = np.array([ball_position_xy['x'], ball_position_xy['y']], dtype=np.int)\n",
    "                # Ignore the event without ball information\n",
    "                if (ball_position_xy[0] < 0) or (ball_position_xy[1] < 0):\n",
    "                    continue\n",
    "\n",
    "                # Get segmentation path for the last frame in the sequence\n",
    "                seg_path = os.path.join(annos_dir, game_name, 'segmentation_masks', '{}.jpg'.format(last_f_idx))\n",
    "                if not os.path.isfile(seg_path):\n",
    "                    print(\"smooth_idx: {} - The segmentation path {} is invalid\".format(smooth_idx, seg_path))\n",
    "                    continue\n",
    "                event_class = configs.events_dict[event_name]\n",
    "\n",
    "                target_events = smooth_event_labelling(event_class, smooth_idx, event_frameidx)\n",
    "                events_infor.append([img_path_list, ball_position_xy, target_events, seg_path])\n",
    "                # Re-label if the event is neither bounce nor net hit\n",
    "                if (target_events[0] == 0) and (target_events[1] == 0):\n",
    "                    event_class = 2\n",
    "                events_labels.append(event_class)\n",
    "    return events_infor, events_labels\n",
    "def train_val_data_separation(configs):\n",
    "    \"\"\"Seperate data to training and validation sets\"\"\"\n",
    "    dataset_type = 'training'\n",
    "    events_infor, events_labels = get_events_infor(configs.train_game_list, configs, dataset_type)\n",
    "    if configs.no_val:\n",
    "        train_events_infor = events_infor\n",
    "        train_events_labels = events_labels\n",
    "        val_events_infor = None\n",
    "        val_events_labels = None\n",
    "    else:\n",
    "        train_events_infor, val_events_infor, train_events_labels, val_events_labels = train_test_split(events_infor,\n",
    "                                                                                                        events_labels,\n",
    "                                                                                                        shuffle=True,\n",
    "                                                                                                        test_size=configs.val_size,\n",
    "                                                                                                        random_state=configs.seed,\n",
    "                                                                                                        stratify=events_labels)\n",
    "    return train_events_infor, val_events_infor, train_events_labels, val_events_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b15623",
   "metadata": {
    "code_folding": [
     2,
     34
    ]
   },
   "outputs": [],
   "source": [
    "# # Dataloader\n",
    "def create_train_val_dataloader(configs):\n",
    "    \"\"\"Create dataloader for training and validate\"\"\"\n",
    "\n",
    "    train_transform = Compose([\n",
    "        Random_Crop(max_reduction_percent=0.15, p=0.5),\n",
    "        Random_HFlip(p=0.5),\n",
    "        Random_Rotate(rotation_angle_limit=10, p=0.5),\n",
    "    ], p=1.)\n",
    "\n",
    "    train_events_infor, val_events_infor, *_ = train_val_data_separation(configs)\n",
    "    train_dataset = TTNet_Dataset(train_events_infor, configs.org_size, configs.input_size, transform=train_transform,\n",
    "                                  num_samples=configs.num_samples)\n",
    "    train_sampler = None\n",
    "    if configs.distributed:\n",
    "        train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=configs.batch_size, shuffle=(train_sampler is None),\n",
    "                                  pin_memory=configs.pin_memory, num_workers=configs.num_workers, sampler=train_sampler)\n",
    "\n",
    "    val_dataloader = None\n",
    "    if not configs.no_val:\n",
    "        val_transform = None\n",
    "        val_sampler = None\n",
    "        val_dataset = TTNet_Dataset(val_events_infor, configs.org_size, configs.input_size, transform=val_transform,\n",
    "                                    num_samples=configs.num_samples)\n",
    "        if configs.distributed:\n",
    "            val_sampler = torch.utils.data.distributed.DistributedSampler(val_dataset, shuffle=False)\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=configs.batch_size, shuffle=False,\n",
    "                                    pin_memory=configs.pin_memory, num_workers=configs.num_workers, sampler=val_sampler)\n",
    "\n",
    "    return train_dataloader, val_dataloader, train_sampler\n",
    "def create_test_dataloader(configs):\n",
    "    \"\"\"Create dataloader for testing phase\"\"\"\n",
    "\n",
    "    test_transform = None\n",
    "    dataset_type = 'test'\n",
    "    test_events_infor, test_events_labels = get_events_infor(configs.test_game_list, configs, dataset_type)\n",
    "    test_dataset = TTNet_Dataset(test_events_infor, configs.org_size, configs.input_size, transform=test_transform,\n",
    "                                 num_samples=configs.num_samples)\n",
    "    test_sampler = None\n",
    "    if configs.distributed:\n",
    "        test_sampler = torch.utils.data.distributed.DistributedSampler(test_dataset)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=configs.batch_size, shuffle=False,\n",
    "                                 pin_memory=configs.pin_memory, num_workers=configs.num_workers, sampler=test_sampler)\n",
    "\n",
    "    return test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87dca2c",
   "metadata": {
    "code_folding": [
     0,
     33,
     39,
     43,
     51,
     62
    ]
   },
   "outputs": [],
   "source": [
    "def check_dataset():\n",
    "    configs = parse_configs()\n",
    "    game_list    = ['game_1']\n",
    "    dataset_type = 'training'\n",
    "    train_events_infor, val_events_infor, *_ = train_val_data_separation(configs)\n",
    "    print('len(train_events_infor): {}'.format(len(train_events_infor)))\n",
    "    # Test transformation\n",
    "    transform = Compose([\n",
    "        Random_Crop(max_reduction_percent=0.15, p=1.),\n",
    "        Random_HFlip(p=1.),\n",
    "        Random_Rotate(rotation_angle_limit=15, p=1.)\n",
    "    ], p=1.)\n",
    "    ttnet_dataset = TTNet_Dataset(train_events_infor, configs.org_size, configs.input_size, transform=transform)\n",
    "    print('len(ttnet_dataset): {}'.format(len(ttnet_dataset)))\n",
    "    example_index = 100\n",
    "#     resized_imgs, org_ball_pos_xy, global_ball_pos_xy, target_event = ttnet_dataset.__getitem__(example_index)\n",
    "    resized_imgs, org_ball_pos_xy, global_ball_pos_xy, target_event, target_seg = ttnet_dataset.__getitem__(example_index)\n",
    "    if 1:\n",
    "        # Test F.interpolate, we can simply use cv2.resize() to get origin_imgs from resized_imgs\n",
    "        # Achieve better quality of images and faster\n",
    "        origin_imgs = F.interpolate(torch.from_numpy(resized_imgs).unsqueeze(0).float(), (1080, 1920))\n",
    "        origin_imgs = origin_imgs.squeeze().numpy().transpose(1, 2, 0).astype(np.uint8)\n",
    "        print('F.interpolate - origin_imgs shape: {}'.format(origin_imgs.shape))\n",
    "        resized_imgs = resized_imgs.transpose(1, 2, 0)\n",
    "        print('resized_imgs shape: {}'.format(resized_imgs.shape))\n",
    "    else:\n",
    "        # Test cv2.resize\n",
    "        resized_imgs = resized_imgs.transpose(1, 2, 0)\n",
    "        print('resized_imgs shape: {}'.format(resized_imgs.shape))\n",
    "        origin_imgs = cv2.resize(resized_imgs, (1920, 1080))\n",
    "        print('cv2.resize - origin_imgs shape: {}'.format(origin_imgs.shape))\n",
    "        \n",
    "    out_images_dir = os.path.join(configs.results_dir, 'debug', 'ttnet_dataset')\n",
    "    if not os.path.isdir(out_images_dir):\n",
    "        os.makedirs(out_images_dir)\n",
    "        \n",
    "    fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(20, 20))\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    for i in range(configs.num_frames_sequence):\n",
    "        img = origin_imgs[:, :, (i * 3): (i + 1) * 3]\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title('image {}'.format(i))\n",
    "    fig.suptitle(\n",
    "        'Event: is bounce {}, is net: {}, ball_position_xy: (x= {}, y= {})'.format(target_event[0], target_event[1],\n",
    "                                                                                   org_ball_pos_xy[0],\n",
    "                                                                                   org_ball_pos_xy[1]),\n",
    "        fontsize=16)\n",
    "    plt.savefig(os.path.join(out_images_dir, 'org_all_imgs_{}.jpg'.format(example_index)))\n",
    "\n",
    "\n",
    "    for i in range(configs.num_frames_sequence):\n",
    "        img = resized_imgs[:, :, (i * 3): (i + 1) * 3]\n",
    "        if (i == (configs.num_frames_sequence - 1)):\n",
    "            img = cv2.resize(img, (img.shape[1], img.shape[0]))\n",
    "            ball_img = cv2.circle(img, tuple(global_ball_pos_xy), radius=5, color=(255, 0, 0), thickness=2)\n",
    "            ball_img = cv2.cvtColor(ball_img, cv2.COLOR_RGB2BGR)\n",
    "            cv2.imwrite(os.path.join(out_images_dir, 'augment_img_{}.jpg'.format(example_index)),\n",
    "                        ball_img)\n",
    "\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title('image {}'.format(i))\n",
    "    fig.suptitle(\n",
    "        'Event: is bounce {}, is net: {}, ball_position_xy: (x= {}, y= {})'.format(target_event[0], target_event[1],\n",
    "                                                                                   global_ball_pos_xy[0],\n",
    "                                                                                   global_ball_pos_xy[1]),\n",
    "        fontsize=16)\n",
    "    plt.savefig(os.path.join(out_images_dir, 'augment_all_imgs_{}.jpg'.format(example_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a730e76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912f4a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dataloader():\n",
    "    configs = parse_configs()\n",
    "    configs.distributed = False  # For testing\n",
    "    train_dataloader, val_dataloader, train_sampler = create_train_val_dataloader(configs)\n",
    "    print('len train_dataloader: {}, val_dataloader: {}'.format(len(train_dataloader), len(val_dataloader)))\n",
    "    return train_dataloader, val_dataloader, train_sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eef7064",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataloader, val_dataloader, train_sampler = check_dataloader()\n",
    "for data in train_dataloader:\n",
    "    for i in range(len(data)):\n",
    "        print(data[i].shape)\n",
    "    break\n",
    "# data[0] : stack of images ( 9 frames per batch )\n",
    "# data[1] : original coordinated of ball\n",
    "# data[2] : scaled coordinates of ball\n",
    "# data[3] : event labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e796e19f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d10238e5",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### Video Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4800a635",
   "metadata": {
    "code_folding": [
     1
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Video loader\n",
    "class TTNet_Video_Loader:\n",
    "    \"\"\"The loader for demo with a video input\"\"\"\n",
    "\n",
    "    def __init__(self, video_path, input_size=(320, 128), num_frames_sequence=9):\n",
    "        assert os.path.isfile(video_path), \"No video at {}\".format(video_path)\n",
    "        self.cap = cv2.VideoCapture(video_path)\n",
    "        self.video_fps = int(round(self.cap.get(cv2.CAP_PROP_FPS)))\n",
    "        self.video_w = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        self.video_h = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        self.video_num_frames = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "        self.width = input_size[0]\n",
    "        self.height = input_size[1]\n",
    "        self.count = 0\n",
    "        self.num_frames_sequence = num_frames_sequence\n",
    "        print('Length of the video: {:d} frames'.format(self.video_num_frames))\n",
    "\n",
    "        self.images_sequence = deque(maxlen=num_frames_sequence)\n",
    "        self.get_first_images_sequence()\n",
    "\n",
    "    def get_first_images_sequence(self):\n",
    "        # Load (self.num_frames_sequence - 1) images\n",
    "        while (self.count < self.num_frames_sequence):\n",
    "            self.count += 1\n",
    "            ret, frame = self.cap.read()  # BGR\n",
    "            assert ret, 'Failed to load frame {:d}'.format(self.count)\n",
    "            self.images_sequence.append(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB), (self.width, self.height)))\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.count = -1\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        self.count += 1\n",
    "        if self.count == len(self):\n",
    "            raise StopIteration\n",
    "        # Read image\n",
    "\n",
    "        ret, frame = self.cap.read()  # BGR\n",
    "        assert ret, 'Failed to load frame {:d}'.format(self.count)\n",
    "        self.images_sequence.append(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB), (self.width, self.height)))\n",
    "        resized_imgs = np.dstack(self.images_sequence)  # (128, 320, 27)\n",
    "        # Transpose (H, W, C) to (C, H, W) --> fit input of TTNet model\n",
    "        resized_imgs = resized_imgs.transpose(2, 0, 1)  # (27, 128, 320)\n",
    "\n",
    "        return self.count, resized_imgs\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.video_num_frames - self.num_frames_sequence + 1  # number of sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ffd7f0",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### Data Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6095b31",
   "metadata": {
    "code_folding": [
     2,
     12,
     37,
     45,
     49,
     110
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Data utils\n",
    "\n",
    "def load_raw_img(img_path):\n",
    "    \"\"\"Load raw image based on the path to the image\"\"\"\n",
    "    img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)  # BGR --> RGB\n",
    "    return img\n",
    "\n",
    "def gaussian_1d(pos, muy, sigma):\n",
    "    \"\"\"Create 1D Gaussian distribution based on ball position (muy), and std (sigma)\"\"\"\n",
    "    target = torch.exp(- (((pos - muy) / sigma) ** 2) / 2)\n",
    "    return target\n",
    "\n",
    "def create_target_ball(ball_position_xy, sigma, w, h, thresh_mask, device):\n",
    "    \"\"\"Create target for the ball detection stages\n",
    "    :param ball_position_xy: Position of the ball (x,y)\n",
    "    :param sigma: standard deviation (a hyperparameter)\n",
    "    :param w: width of the resize image\n",
    "    :param h: height of the resize image\n",
    "    :param thresh_mask: if values of 1D Gaussian < thresh_mask --> set to 0 to reduce computation\n",
    "    :param device: cuda() or cpu()\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    w, h = int(w), int(h)\n",
    "    target_ball_position = torch.zeros((w + h,), device=device)\n",
    "    # Only do the next step if the ball is existed\n",
    "    if (w > ball_position_xy[0] > 0) and (h > ball_position_xy[1] > 0):\n",
    "        # For x\n",
    "        x_pos = torch.arange(0, w, device=device)\n",
    "        target_ball_position[:w] = gaussian_1d(x_pos, ball_position_xy[0], sigma=sigma)\n",
    "        # For y\n",
    "        y_pos = torch.arange(0, h, device=device)\n",
    "        target_ball_position[w:] = gaussian_1d(y_pos, ball_position_xy[1], sigma=sigma)\n",
    "\n",
    "        target_ball_position[target_ball_position < thresh_mask] = 0.\n",
    "\n",
    "    return target_ball_position\n",
    "\n",
    "def smooth_event_labelling(event_class, smooth_idx, event_frameidx):\n",
    "    target_events = np.zeros((2,))\n",
    "    if event_class < 2:\n",
    "        n = smooth_idx - event_frameidx\n",
    "        target_events[event_class] = np.cos(n * np.pi / 8)\n",
    "        target_events[target_events < 0.01] = 0.\n",
    "    return target_events\n",
    "\n",
    "def get_events_infor(game_list, configs, dataset_type):\n",
    "    \"\"\"Get information of sequences of images based on events\n",
    "    :param game_list: List of games (video names)\n",
    "    :return:\n",
    "    [\n",
    "        each event: [[img_path_list], ball_position, target_events, segmentation_path]\n",
    "    ]\n",
    "    \"\"\"\n",
    "    # the paper mentioned 25, but used 9 frames only\n",
    "    num_frames_from_event = int((configs.num_frames_sequence - 1) / 2)\n",
    "\n",
    "    annos_dir = os.path.join(configs.dataset_dir, dataset_type, 'annotations')\n",
    "    images_dir = os.path.join(configs.dataset_dir, dataset_type, 'images')\n",
    "    events_infor = []\n",
    "    events_labels = []\n",
    "    for game_name in game_list:\n",
    "        ball_annos_path = os.path.join(annos_dir, game_name, 'ball_markup.json')\n",
    "        events_annos_path = os.path.join(annos_dir, game_name, 'events_markup.json')\n",
    "        # Load ball annotations\n",
    "        json_ball = open(ball_annos_path)\n",
    "        ball_annos = json.load(json_ball)\n",
    "\n",
    "        # Load events annotations\n",
    "        json_events = open(events_annos_path)\n",
    "        events_annos = json.load(json_events)\n",
    "        for event_frameidx, event_name in events_annos.items():\n",
    "            event_frameidx = int(event_frameidx)\n",
    "            smooth_frame_indices = [event_frameidx]  # By default\n",
    "            if (event_name != 'empty_event') and (configs.smooth_labelling):\n",
    "                smooth_frame_indices = [idx for idx in range(event_frameidx - num_frames_from_event,\n",
    "                                                             event_frameidx + num_frames_from_event + 1)]\n",
    "\n",
    "            for smooth_idx in smooth_frame_indices:\n",
    "                sub_smooth_frame_indices = [idx for idx in range(smooth_idx - num_frames_from_event,\n",
    "                                                                 smooth_idx + num_frames_from_event + 1)]\n",
    "                img_path_list = []\n",
    "                for sub_smooth_idx in sub_smooth_frame_indices:\n",
    "                    img_path = os.path.join(images_dir, game_name, 'img_{:06d}.jpg'.format(sub_smooth_idx))\n",
    "                    img_path_list.append(img_path)\n",
    "                last_f_idx = smooth_idx + num_frames_from_event\n",
    "                # Get ball position for the last frame in the sequence\n",
    "                if '{}'.format(last_f_idx) not in ball_annos.keys():\n",
    "                    print('smooth_idx: {} - no ball position for the frame idx {}'.format(smooth_idx, last_f_idx))\n",
    "                    continue\n",
    "                ball_position_xy = ball_annos['{}'.format(last_f_idx)]\n",
    "                ball_position_xy = np.array([ball_position_xy['x'], ball_position_xy['y']], dtype=np.int)\n",
    "                # Ignore the event without ball information\n",
    "                if (ball_position_xy[0] < 0) or (ball_position_xy[1] < 0):\n",
    "                    continue\n",
    "\n",
    "                # Get segmentation path for the last frame in the sequence\n",
    "                seg_path = os.path.join(annos_dir, game_name, 'segmentation_masks', '{}.png'.format(last_f_idx))\n",
    "                if not os.path.isfile(seg_path):\n",
    "                    print(\"smooth_idx: {} - The segmentation path {} is invalid\".format(smooth_idx, seg_path))\n",
    "                    continue\n",
    "                event_class = configs.events_dict[event_name]\n",
    "\n",
    "                target_events = smooth_event_labelling(event_class, smooth_idx, event_frameidx)\n",
    "                events_infor.append([img_path_list, ball_position_xy, target_events, seg_path])\n",
    "                # Re-label if the event is neither bounce nor net hit\n",
    "                if (target_events[0] == 0) and (target_events[1] == 0):\n",
    "                    event_class = 2\n",
    "                events_labels.append(event_class)\n",
    "    return events_infor, events_labels\n",
    "\n",
    "def train_val_data_separation(configs):\n",
    "    \"\"\"Seperate data to training and validation sets\"\"\"\n",
    "    dataset_type = 'training'\n",
    "    events_infor, events_labels = get_events_infor(configs.train_game_list, configs, dataset_type)\n",
    "    if configs.no_val:\n",
    "        train_events_infor = events_infor\n",
    "        train_events_labels = events_labels\n",
    "        val_events_infor = None\n",
    "        val_events_labels = None\n",
    "    else:\n",
    "        train_events_infor, val_events_infor, train_events_labels, val_events_labels = train_test_split(events_infor,\n",
    "                                                                                                        events_labels,\n",
    "                                                                                                        shuffle=True,\n",
    "                                                                                                        test_size=configs.val_size,\n",
    "                                                                                                        random_state=configs.seed,\n",
    "                                                                                                        stratify=events_labels)\n",
    "    return train_events_infor, val_events_infor, train_events_labels, val_events_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6530f82a",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Model Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dd1a4b",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### TTNet Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bd01cc",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"artifacts/network.png\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a2a726",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### ConvBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9781acf1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.maxpool(self.relu(self.batchnorm(self.conv(x))))\n",
    "        return x\n",
    "class ConvBlock_without_Pooling(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvBlock_without_Pooling, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.batchnorm(self.conv(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345c9afb",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### DeconvBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407549b8",
   "metadata": {
    "code_folding": [
     0,
     19,
     56,
     82,
     113
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class DeconvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DeconvBlock, self).__init__()\n",
    "        middle_channels = int(in_channels / 4)\n",
    "        self.conv1 = nn.Conv2d(in_channels, middle_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(middle_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.batchnorm_tconv = nn.BatchNorm2d(middle_channels)\n",
    "        self.tconv = nn.ConvTranspose2d(middle_channels, middle_channels, kernel_size=3, stride=2, padding=1,\n",
    "                                        output_padding=1)\n",
    "        self.conv2 = nn.Conv2d(middle_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.batchnorm1(self.conv1(x)))\n",
    "        x = self.relu(self.batchnorm_tconv(self.tconv(x)))\n",
    "        x = self.relu(self.batchnorm2(self.conv2(x)))\n",
    "\n",
    "        return x\n",
    "class BallDetection(nn.Module):\n",
    "    def __init__(self, num_frames_sequence, dropout_p):\n",
    "        super(BallDetection, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_frames_sequence * 3, 64, kernel_size=1, stride=1, padding=0)\n",
    "        self.batchnorm = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.convblock1 = ConvBlock(in_channels=64, out_channels=64)\n",
    "        self.convblock2 = ConvBlock(in_channels=64, out_channels=64)\n",
    "        self.dropout2d = nn.Dropout2d(p=dropout_p)\n",
    "        self.convblock3 = ConvBlock(in_channels=64, out_channels=128)\n",
    "        self.convblock4 = ConvBlock(in_channels=128, out_channels=128)\n",
    "        self.convblock5 = ConvBlock(in_channels=128, out_channels=256)\n",
    "        self.convblock6 = ConvBlock(in_channels=256, out_channels=256)\n",
    "        self.fc1 = nn.Linear(in_features=2560, out_features=1792)\n",
    "        self.fc2 = nn.Linear(in_features=1792, out_features=896)\n",
    "        self.fc3 = nn.Linear(in_features=896, out_features=448)\n",
    "        self.dropout1d = nn.Dropout(p=dropout_p)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.batchnorm(self.conv1(x)))\n",
    "        out_block2 = self.convblock2(self.convblock1(x))\n",
    "        x = self.dropout2d(out_block2)\n",
    "        out_block3 = self.convblock3(x)\n",
    "        out_block4 = self.convblock4(out_block3)\n",
    "        x = self.dropout2d(out_block4)\n",
    "        out_block5 = self.convblock5(out_block4)\n",
    "        features = self.convblock6(out_block5)\n",
    "\n",
    "        x = self.dropout2d(features)\n",
    "        x = x.contiguous().view(x.size(0), -1)\n",
    "\n",
    "        x = self.dropout1d(self.relu(self.fc1(x)))\n",
    "        x = self.dropout1d(self.relu(self.fc2(x)))\n",
    "        out = self.sigmoid(self.fc3(x))\n",
    "\n",
    "        return out, features, out_block2, out_block3, out_block4, out_block5\n",
    "class EventsSpotting(nn.Module):\n",
    "    def __init__(self, dropout_p):\n",
    "        super(EventsSpotting, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(512, 64, kernel_size=1, stride=1, padding=0)\n",
    "        self.batchnorm = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout2d = nn.Dropout2d(p=dropout_p)\n",
    "        self.convblock = ConvBlock_without_Pooling(in_channels=64, out_channels=64)\n",
    "        self.fc1 = nn.Linear(in_features=640, out_features=512)\n",
    "        self.fc2 = nn.Linear(in_features=512, out_features=2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, global_features, local_features):\n",
    "        input_eventspotting = torch.cat((global_features, local_features), dim=1)\n",
    "        x = self.relu(self.batchnorm(self.conv1(input_eventspotting)))\n",
    "        x = self.dropout2d(x)\n",
    "        x = self.convblock(x)\n",
    "        x = self.dropout2d(x)\n",
    "        x = self.convblock(x)\n",
    "        x = self.dropout2d(x)\n",
    "\n",
    "        x = x.contiguous().view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        out = self.sigmoid(self.fc2(x))\n",
    "\n",
    "        return out\n",
    "class Segmentation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Segmentation, self).__init__()\n",
    "        self.deconvblock5 = DeconvBlock(in_channels=256, out_channels=128)\n",
    "        self.deconvblock4 = DeconvBlock(in_channels=128, out_channels=128)\n",
    "        self.deconvblock3 = DeconvBlock(in_channels=128, out_channels=64)\n",
    "        self.deconvblock2 = DeconvBlock(in_channels=64, out_channels=64)\n",
    "        self.tconv = nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=3, stride=2, padding=0,\n",
    "                                        output_padding=0)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv1 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=0)\n",
    "        self.conv2 = nn.Conv2d(32, 3, kernel_size=2, stride=1, padding=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, out_block2, out_block3, out_block4, out_block5):\n",
    "        x = self.deconvblock5(out_block5)\n",
    "        x = x + out_block4\n",
    "        x = self.deconvblock4(x)\n",
    "        x = x + out_block3\n",
    "        x = self.deconvblock3(x)\n",
    "\n",
    "        x = x + out_block2\n",
    "        x = self.deconvblock2(x)\n",
    "\n",
    "        x = self.relu(self.tconv(x))\n",
    "\n",
    "        x = self.relu(self.conv1(x))\n",
    "\n",
    "        out = self.sigmoid(self.conv2(x))\n",
    "\n",
    "        return out\n",
    "class TTNet(nn.Module):\n",
    "    def __init__(self, dropout_p, tasks, input_size, thresh_ball_pos_mask, num_frames_sequence,\n",
    "                 mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n",
    "        super(TTNet, self).__init__()\n",
    "        self.tasks = tasks\n",
    "        self.ball_local_stage, self.events_spotting, self.segmentation = None, None, None\n",
    "        self.ball_global_stage = BallDetection(num_frames_sequence=num_frames_sequence, dropout_p=dropout_p)\n",
    "        if 'local' in tasks:\n",
    "            self.ball_local_stage = BallDetection(num_frames_sequence=num_frames_sequence, dropout_p=dropout_p)\n",
    "        if 'event' in tasks:\n",
    "            self.events_spotting = EventsSpotting(dropout_p=dropout_p)\n",
    "        if 'seg' in tasks:\n",
    "            self.segmentation = Segmentation()\n",
    "        self.w_resize = input_size[0]\n",
    "        self.h_resize = input_size[1]\n",
    "        self.thresh_ball_pos_mask = thresh_ball_pos_mask\n",
    "        self.mean = torch.repeat_interleave(torch.tensor(mean).view(1, 3, 1, 1), repeats=9, dim=1)\n",
    "        self.std = torch.repeat_interleave(torch.tensor(std).view(1, 3, 1, 1), repeats=9, dim=1)\n",
    "\n",
    "    def forward(self, resize_batch_input, org_ball_pos_xy):\n",
    "        \"\"\"Forward propagation\n",
    "        :param resize_batch_input: (batch_size, 27, 128, 320)\n",
    "        :param org_ball_pos_xy: (batch_size, 2) --> Use it to get ground-truth for the local stage\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        pred_ball_local, pred_events, pred_seg, local_ball_pos_xy = None, None, None, None\n",
    "\n",
    "        # Normalize the input before compute forward propagation\n",
    "        pred_ball_global, global_features, out_block2, out_block3, out_block4, out_block5 = self.ball_global_stage(\n",
    "            self.__normalize__(resize_batch_input))\n",
    "        if self.ball_local_stage is not None:\n",
    "            # Based on the prediction of the global stage, crop the original images\n",
    "            input_ball_local, cropped_params = self.__crop_original_batch__(resize_batch_input, pred_ball_global)\n",
    "            # Get the ground truth of the ball for the local stage\n",
    "            local_ball_pos_xy = self.__get_groundtruth_local_ball_pos__(org_ball_pos_xy, cropped_params)\n",
    "            # Normalize the input before compute forward propagation\n",
    "            pred_ball_local, local_features, *_ = self.ball_local_stage(self.__normalize__(input_ball_local))\n",
    "            # Only consider the events spotting if the model has the local stage for ball detection\n",
    "            if self.events_spotting is not None:\n",
    "                pred_events = self.events_spotting(global_features, local_features)\n",
    "        if self.segmentation is not None:\n",
    "            pred_seg = self.segmentation(out_block2, out_block3, out_block4, out_block5)\n",
    "\n",
    "        return pred_ball_global, pred_ball_local, pred_events, pred_seg, local_ball_pos_xy\n",
    "\n",
    "    def run_demo(self, resize_batch_input):\n",
    "        \"\"\"Only for full 4 stages/modules in TTNet\"\"\"\n",
    "\n",
    "        # Normalize the input before compute forward propagation\n",
    "        pred_ball_global, global_features, out_block2, out_block3, out_block4, out_block5 = self.ball_global_stage(\n",
    "            self.__normalize__(resize_batch_input))\n",
    "        input_ball_local, cropped_params = self.__crop_original_batch__(resize_batch_input, pred_ball_global)\n",
    "        # Normalize the input before compute forward propagation\n",
    "        pred_ball_local, local_features, *_ = self.ball_local_stage(self.__normalize__(input_ball_local))\n",
    "        pred_events = self.events_spotting(global_features, local_features)\n",
    "        pred_seg = self.segmentation(out_block2, out_block3, out_block4, out_block5)\n",
    "\n",
    "        return pred_ball_global, pred_ball_local, pred_events, pred_seg\n",
    "\n",
    "    def __normalize__(self, x):\n",
    "        if not self.mean.is_cuda:\n",
    "            self.mean = self.mean.cuda()\n",
    "            self.std = self.std.cuda()\n",
    "\n",
    "        return (x / 255. - self.mean) / self.std\n",
    "\n",
    "    def __get_groundtruth_local_ball_pos__(self, org_ball_pos_xy, cropped_params):\n",
    "        local_ball_pos_xy = torch.zeros_like(org_ball_pos_xy)  # no grad for torch.zeros_like output\n",
    "\n",
    "        for idx, params in enumerate(cropped_params):\n",
    "            is_ball_detected, x_min, x_max, y_min, y_max, x_pad, y_pad = params\n",
    "\n",
    "            if is_ball_detected:\n",
    "                # Get the local ball position based on the crop image informaion\n",
    "                local_ball_pos_xy[idx, 0] = max(org_ball_pos_xy[idx, 0] - x_min + x_pad, -1)\n",
    "                local_ball_pos_xy[idx, 1] = max(org_ball_pos_xy[idx, 1] - y_min + y_pad, -1)\n",
    "                # If the ball is outside of the cropped image --> set position to -1, -1 --> No ball\n",
    "                if (local_ball_pos_xy[idx, 0] >= self.w_resize) or (local_ball_pos_xy[idx, 1] >= self.h_resize) or (\n",
    "                        local_ball_pos_xy[idx, 0] < 0) or (local_ball_pos_xy[idx, 1] < 0):\n",
    "                    local_ball_pos_xy[idx, 0] = -1\n",
    "                    local_ball_pos_xy[idx, 1] = -1\n",
    "            else:\n",
    "                local_ball_pos_xy[idx, 0] = -1\n",
    "                local_ball_pos_xy[idx, 1] = -1\n",
    "        return local_ball_pos_xy\n",
    "\n",
    "    def __crop_original_batch__(self, resize_batch_input, pred_ball_global):\n",
    "        \"\"\"Get input of the local stage by cropping the original images based on the predicted ball position\n",
    "            of the global stage\n",
    "        :param resize_batch_input: (batch_size, 27, 128, 320)\n",
    "        :param pred_ball_global: (batch_size, 448)\n",
    "        :param org_ball_pos_xy: (batch_size, 2)\n",
    "        :return: input_ball_local (batch_size, 27, 128, 320)\n",
    "        \"\"\"\n",
    "        # Process input for local stage based on output of the global one\n",
    "\n",
    "        batch_size = resize_batch_input.size(0)\n",
    "        h_original, w_original = 1080, 1920\n",
    "        h_ratio = h_original / self.h_resize\n",
    "        w_ratio = w_original / self.w_resize\n",
    "        pred_ball_global_mask = pred_ball_global.clone().detach()\n",
    "        pred_ball_global_mask[pred_ball_global_mask < self.thresh_ball_pos_mask] = 0.\n",
    "\n",
    "        # Crop the original images\n",
    "        input_ball_local = torch.zeros_like(resize_batch_input)  # same shape with resize_batch_input, no grad\n",
    "        original_batch_input = F.interpolate(resize_batch_input, (h_original, w_original))  # On GPU\n",
    "        cropped_params = []\n",
    "        for idx in range(batch_size):\n",
    "            pred_ball_pos_x = pred_ball_global_mask[idx, :self.w_resize]\n",
    "            pred_ball_pos_y = pred_ball_global_mask[idx, self.w_resize:]\n",
    "            # If the ball is not detected, we crop the center of the images, set ball_poss to [-1, -1]\n",
    "            if (torch.sum(pred_ball_pos_x) == 0.) or (torch.sum(pred_ball_pos_y) == 0.):\n",
    "                # Assume the ball is in the center image\n",
    "                x_center = int(self.w_resize / 2)\n",
    "                y_center = int(self.h_resize / 2)\n",
    "                is_ball_detected = False\n",
    "            else:\n",
    "                x_center = torch.argmax(pred_ball_pos_x)  # Upper part\n",
    "                y_center = torch.argmax(pred_ball_pos_y)  # Lower part\n",
    "                is_ball_detected = True\n",
    "\n",
    "            # Adjust ball position to the original size\n",
    "            x_center = int(x_center * w_ratio)\n",
    "            y_center = int(y_center * h_ratio)\n",
    "\n",
    "            x_min, x_max, y_min, y_max = self.__get_crop_params__(x_center, y_center, self.w_resize, self.h_resize,\n",
    "                                                                  w_original, h_original)\n",
    "            # Put image to the center\n",
    "            h_crop = y_max - y_min\n",
    "            w_crop = x_max - x_min\n",
    "            x_pad = 0\n",
    "            y_pad = 0\n",
    "            if (h_crop != self.h_resize) or (w_crop != self.w_resize):\n",
    "                x_pad = int((self.w_resize - w_crop) / 2)\n",
    "                y_pad = int((self.h_resize - h_crop) / 2)\n",
    "                input_ball_local[idx, :, y_pad:(y_pad + h_crop), x_pad:(x_pad + w_crop)] = original_batch_input[idx, :,\n",
    "                                                                                           y_min:y_max, x_min: x_max]\n",
    "            else:\n",
    "                input_ball_local[idx, :, :, :] = original_batch_input[idx, :, y_min:y_max, x_min: x_max]\n",
    "            cropped_params.append([is_ball_detected, x_min, x_max, y_min, y_max, x_pad, y_pad])\n",
    "\n",
    "        return input_ball_local, cropped_params\n",
    "\n",
    "    def __get_crop_params__(self, x_center, y_center, w_resize, h_resize, w_original, h_original):\n",
    "        x_min = max(0, x_center - int(w_resize / 2))\n",
    "        y_min = max(0, y_center - int(h_resize / 2))\n",
    "\n",
    "        x_max = min(w_original, x_min + w_resize)\n",
    "        y_max = min(h_original, y_min + h_resize)\n",
    "\n",
    "        return x_min, x_max, y_min, y_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5072fdff",
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def check_ttnet():\n",
    "    tasks = ['global', 'local', 'event']\n",
    "    ttnet = TTNet(dropout_p=0.5, tasks=tasks, input_size=(320, 128), thresh_ball_pos_mask=0.01,\n",
    "                  num_frames_sequence=9).cuda()\n",
    "    resize_batch_input = torch.rand((1, 27, 128, 320)).cuda()\n",
    "    org_ball_pos_xy = torch.rand((1, 2)).cuda()\n",
    "    start = time.time()\n",
    "    \n",
    "#     pred_ball_global, pred_ball_local, pred_events, local_ball_pos_xy = ttnet(resize_batch_input, org_ball_pos_xy)\n",
    "    pred_ball_global, pred_ball_local, pred_events, pred_seg, local_ball_pos_xy = ttnet(resize_batch_input, org_ball_pos_xy)\n",
    "    \n",
    "#     print(\"DEBUG Unbalaced loss: \", pred_ball_global.shape, pred_ball_local.shape, pred_events.shape, local_ball_pos_xy.shape)    \n",
    "        \n",
    "    if pred_ball_global is not None:\n",
    "        print('pred_ball_global: {}'.format(pred_ball_global.size()))\n",
    "    if pred_ball_local is not None:\n",
    "        print('pred_ball_local: {}'.format(pred_ball_local.size()))\n",
    "    if pred_events is not None:\n",
    "        print('pred_events: {}'.format(pred_events.size()))\n",
    "    print('local_ball_pos_xy: {}'.format(local_ball_pos_xy.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22e3c06",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "check_ttnet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90505b50",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "610138d4",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159518e5",
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Ball_Detection_Loss(nn.Module):\n",
    "    def __init__(self, w, h, epsilon=1e-9):\n",
    "        super(Ball_Detection_Loss, self).__init__()\n",
    "        self.w = w\n",
    "        self.h = h\n",
    "        self.epsilon = epsilon\n",
    "        self.criterion = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "\n",
    "    def forward(self, pred_ball_position, target_ball_position):\n",
    "        x_pred = pred_ball_position[:, :self.w]\n",
    "        y_pred = pred_ball_position[:, self.w:]\n",
    "\n",
    "        x_target = target_ball_position[:, :self.w]\n",
    "        y_target = target_ball_position[:, self.w:]\n",
    "\n",
    "        loss_ball_x = - torch.mean(x_target * torch.log(x_pred + self.epsilon) + (1 - x_target) * torch.log(1 - x_pred + self.epsilon))\n",
    "        loss_ball_y = - torch.mean(y_target * torch.log(y_pred + self.epsilon) + (1 - y_target) * torch.log(1 - y_pred + self.epsilon))\n",
    "#         loss_ball_x = self.criterion(x_target.log(), x_pred)\n",
    "#         loss_ball_y = self.criterion(y_target.log(), y_pred)\n",
    "        \n",
    "        return loss_ball_x + loss_ball_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afebcbe",
   "metadata": {
    "code_folding": [
     0,
     12,
     20,
     28
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Events_Spotting_Loss(nn.Module):\n",
    "    def __init__(self, weights=(1, 3), num_events=2, epsilon=1e-9):\n",
    "        super(Events_Spotting_Loss, self).__init__()\n",
    "        self.weights = torch.tensor(weights).view(1, 2)\n",
    "        self.weights = self.weights / self.weights.sum()\n",
    "        self.num_events = num_events\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, pred_events, target_events):\n",
    "        self.weights = self.weights.cuda()\n",
    "        return - torch.mean(self.weights * (target_events * torch.log(pred_events + self.epsilon) + (1. - target_events) * torch.log(1 - pred_events + self.epsilon)))\n",
    "\n",
    "class DICE_Smotth_Loss(nn.Module):\n",
    "    def __init__(self, epsilon=1e-9):\n",
    "        super(DICE_Smotth_Loss, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, pred_seg, target_seg):\n",
    "        return 1. - ((torch.sum(2 * pred_seg * target_seg) + self.epsilon) / (torch.sum(pred_seg) + torch.sum(target_seg) + self.epsilon))\n",
    "\n",
    "class BCE_Loss(nn.Module):\n",
    "    def __init__(self, epsilon=1e-9):\n",
    "        super(BCE_Loss, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, pred_seg, target_seg):\n",
    "        return - torch.mean(target_seg * torch.log(pred_seg + self.epsilon) + (1 - target_seg) * torch.log(1 - pred_seg + self.epsilon))\n",
    "\n",
    "class Segmentation_Loss(nn.Module):\n",
    "    def __init__(self, bce_weight=0.5):\n",
    "        super(Segmentation_Loss, self).__init__()\n",
    "        self.bce_criterion = BCE_Loss(epsilon=1e-9)\n",
    "        self.dice_criterion = DICE_Smotth_Loss(epsilon=1e-9)\n",
    "        self.bce_weight = bce_weight\n",
    "\n",
    "    def forward(self, pred_seg, target_seg):\n",
    "        target_seg = target_seg.float()\n",
    "        loss_bce = self.bce_criterion(pred_seg, target_seg)\n",
    "        loss_dice = self.dice_criterion(pred_seg, target_seg)\n",
    "        loss_seg = (1 - self.bce_weight) * loss_dice + self.bce_weight * loss_bce\n",
    "        return loss_seg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a966691",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### DICE Smotth Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0f0452",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class DICE_Smotth_Loss(nn.Module):\n",
    "    def __init__(self, epsilon=1e-9):\n",
    "        super(DICE_Smotth_Loss, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, pred_seg, target_seg):\n",
    "        return 1. - ((torch.sum(2 * pred_seg * target_seg) + self.epsilon) / (torch.sum(pred_seg) + torch.sum(target_seg) + self.epsilon))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0536a4a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### BCE Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e175791c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class BCE_Loss(nn.Module):\n",
    "    def __init__(self, epsilon=1e-9):\n",
    "        super(BCE_Loss, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, pred_seg, target_seg):\n",
    "        return - torch.mean(target_seg * torch.log(pred_seg + self.epsilon) + (1 - target_seg) * torch.log(1 - pred_seg + self.epsilon))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b14245b",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Segmentation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8771437e",
   "metadata": {
    "code_folding": [
     1,
     7
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Segmentation_Loss(nn.Module):\n",
    "    def __init__(self, bce_weight=0.5):\n",
    "        super(Segmentation_Loss, self).__init__()\n",
    "        self.bce_criterion = BCE_Loss(epsilon=1e-9)\n",
    "        self.dice_criterion = DICE_Smotth_Loss(epsilon=1e-9)\n",
    "        self.bce_weight = bce_weight\n",
    "\n",
    "    def forward(self, pred_seg, target_seg):\n",
    "        target_seg = target_seg.float()\n",
    "        loss_bce = self.bce_criterion(pred_seg, target_seg)\n",
    "        loss_dice = self.dice_criterion(pred_seg, target_seg)\n",
    "        loss_seg = (1 - self.bce_weight) * loss_dice + self.bce_weight * loss_bce\n",
    "        return loss_seg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d674a56e",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb85d713",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Unbalanced Loss Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08206a2c",
   "metadata": {
    "code_folding": [
     0,
     1,
     16
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Unbalance_Loss_Model(nn.Module):\n",
    "    def __init__(self, model, tasks_loss_weight, weights_events, input_size, sigma, thresh_ball_pos_mask, device):\n",
    "        super(Unbalance_Loss_Model, self).__init__()\n",
    "        self.model = model\n",
    "        self.tasks_loss_weight = torch.tensor(tasks_loss_weight)\n",
    "        self.tasks_loss_weight = self.tasks_loss_weight / self.tasks_loss_weight.sum()\n",
    "        self.num_events = len(tasks_loss_weight)\n",
    "        self.w = input_size[0]\n",
    "        self.h = input_size[1]\n",
    "        self.sigma = sigma\n",
    "        self.thresh_ball_pos_mask = thresh_ball_pos_mask\n",
    "        self.device = device\n",
    "        self.ball_loss_criterion = Ball_Detection_Loss(self.w, self.h)\n",
    "        self.event_loss_criterion = Events_Spotting_Loss(weights=weights_events, num_events=self.num_events)\n",
    "        self.seg_loss_criterion = Segmentation_Loss()\n",
    "\n",
    "    def forward(self, resize_batch_input, org_ball_pos_xy, global_ball_pos_xy, target_events, target_seg):\n",
    "        pred_ball_global, pred_ball_local, pred_events, pred_seg, local_ball_pos_xy = self.model(resize_batch_input,\n",
    "                                                                                                 org_ball_pos_xy)\n",
    "        # Create target for events spotting and ball position (local and global)\n",
    "        batch_size = pred_ball_global.size(0)\n",
    "        target_ball_global = torch.zeros_like(pred_ball_global)\n",
    "        task_idx = 0\n",
    "        for sample_idx in range(batch_size):\n",
    "            target_ball_global[sample_idx] = create_target_ball(global_ball_pos_xy[sample_idx], sigma=self.sigma,\n",
    "                                                                w=self.w, h=self.h,\n",
    "                                                                thresh_mask=self.thresh_ball_pos_mask,\n",
    "                                                                device=self.device)\n",
    "        global_ball_loss = self.ball_loss_criterion(pred_ball_global, target_ball_global)\n",
    "        total_loss = global_ball_loss * self.tasks_loss_weight[task_idx]\n",
    "\n",
    "        if pred_ball_local is not None:\n",
    "            task_idx += 1\n",
    "            target_ball_local = torch.zeros_like(pred_ball_local)\n",
    "            for sample_idx in range(batch_size):\n",
    "                target_ball_local[sample_idx] = create_target_ball(local_ball_pos_xy[sample_idx], sigma=self.sigma,\n",
    "                                                                   w=self.w, h=self.h,\n",
    "                                                                   thresh_mask=self.thresh_ball_pos_mask,\n",
    "                                                                   device=self.device)\n",
    "            local_ball_loss = self.ball_loss_criterion(pred_ball_local, target_ball_local)\n",
    "            total_loss += local_ball_loss * self.tasks_loss_weight[task_idx]\n",
    "\n",
    "        if pred_events is not None:\n",
    "            task_idx += 1\n",
    "            target_events = target_events.to(device=self.device)\n",
    "            event_loss = self.event_loss_criterion(pred_events, target_events)\n",
    "            total_loss += event_loss * self.tasks_loss_weight[task_idx]\n",
    "\n",
    "        if pred_seg is not None:\n",
    "            task_idx += 1\n",
    "            seg_loss = self.seg_loss_criterion(pred_seg, target_seg)\n",
    "            total_loss += seg_loss * self.tasks_loss_weight[task_idx]\n",
    "\n",
    "        return pred_ball_global, pred_ball_local, pred_events, pred_seg, local_ball_pos_xy, total_loss, None\n",
    "\n",
    "    def run_demo(self, resize_batch_input):\n",
    "        pred_ball_global, pred_ball_local, pred_events, pred_seg = self.model.run_demo(resize_batch_input)\n",
    "        return pred_ball_global, pred_ball_local, pred_events, pred_seg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dc0a4c",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Multi-task Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7b989a",
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Multi_Task_Learning_Model(nn.Module):\n",
    "    \"\"\"\n",
    "    Original paper: \"Multi-task learning using uncertainty to weigh losses for scene geometry and semantics\" - CVPR 2018\n",
    "    url: https://arxiv.org/pdf/1705.07115.pdf\n",
    "    refer code: https://github.com/Hui-Li/multi-task-learning-example-PyTorch\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, tasks, num_events, weights_events, input_size, sigma, thresh_ball_pos_mask, device):\n",
    "        super(Multi_Task_Learning_Model, self).__init__()\n",
    "        self.model = model\n",
    "        self.tasks = tasks\n",
    "        self.num_tasks = len(tasks)\n",
    "        self.log_vars = nn.Parameter(torch.zeros((self.num_tasks)))\n",
    "        self.w = input_size[0]\n",
    "        self.h = input_size[1]\n",
    "        self.sigma = sigma\n",
    "        self.thresh_ball_pos_mask = thresh_ball_pos_mask\n",
    "        self.device = device\n",
    "        self.ball_loss_criterion = Ball_Detection_Loss(self.w, self.h)\n",
    "        self.event_loss_criterion = Events_Spotting_Loss(weights=weights_events, num_events=num_events)\n",
    "        self.seg_loss_criterion = Segmentation_Loss()\n",
    "\n",
    "    def forward(self, resize_batch_input, org_ball_pos_xy, global_ball_pos_xy, target_events, target_seg):\n",
    "        log_vars_idx = 0\n",
    "        pred_ball_global, pred_ball_local, pred_events, pred_seg, local_ball_pos_xy = self.model(resize_batch_input,\n",
    "                                                                                                 org_ball_pos_xy)\n",
    "        # Create target for events spotting and ball position (local and global)\n",
    "        batch_size = pred_ball_global.size(0)\n",
    "        target_ball_global = torch.zeros_like(pred_ball_global)\n",
    "        for sample_idx in range(batch_size):\n",
    "            target_ball_global[sample_idx] = create_target_ball(global_ball_pos_xy[sample_idx], sigma=self.sigma,\n",
    "                                                                w=self.w, h=self.h,\n",
    "                                                                thresh_mask=self.thresh_ball_pos_mask,\n",
    "                                                                device=self.device)\n",
    "        global_ball_loss = self.ball_loss_criterion(pred_ball_global, target_ball_global)\n",
    "        total_loss = global_ball_loss / (torch.exp(2 * self.log_vars[log_vars_idx])) + self.log_vars[log_vars_idx]\n",
    "\n",
    "        if pred_ball_local is not None:\n",
    "            log_vars_idx += 1\n",
    "            target_ball_local = torch.zeros_like(pred_ball_local)\n",
    "            for sample_idx in range(batch_size):\n",
    "                target_ball_local[sample_idx] = create_target_ball(local_ball_pos_xy[sample_idx], sigma=self.sigma,\n",
    "                                                                   w=self.w, h=self.h,\n",
    "                                                                   thresh_mask=self.thresh_ball_pos_mask,\n",
    "                                                                   device=self.device)\n",
    "            local_ball_loss = self.ball_loss_criterion(pred_ball_local, target_ball_local)\n",
    "            total_loss += local_ball_loss / (torch.exp(2 * self.log_vars[log_vars_idx])) + self.log_vars[log_vars_idx]\n",
    "\n",
    "        if pred_events is not None:\n",
    "            log_vars_idx += 1\n",
    "            target_events = target_events.to(device=self.device)\n",
    "            event_loss = self.event_loss_criterion(pred_events, target_events)\n",
    "            total_loss += event_loss / (2 * torch.exp(self.log_vars[log_vars_idx])) + self.log_vars[log_vars_idx]\n",
    "\n",
    "        if pred_seg is not None:\n",
    "            log_vars_idx += 1\n",
    "            seg_loss = self.seg_loss_criterion(pred_seg, target_seg)\n",
    "            total_loss += seg_loss / (2 * torch.exp(self.log_vars[log_vars_idx])) + self.log_vars[log_vars_idx]\n",
    "\n",
    "        # Final weights: [math.exp(log_var) ** 0.5 for log_var in log_vars]\n",
    "\n",
    "        return pred_ball_global, pred_ball_local, pred_events, pred_seg, local_ball_pos_xy, total_loss, self.log_vars.data.tolist()\n",
    "\n",
    "    def run_demo(self, resize_batch_input):\n",
    "        pred_ball_global, pred_ball_local, pred_events, pred_seg = self.model.run_demo(resize_batch_input)\n",
    "        return pred_ball_global, pred_ball_local, pred_events, pred_seg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb1d4d8",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Model Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049f246d",
   "metadata": {
    "code_folding": [
     0,
     21,
     29,
     39,
     51,
     85,
     98
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def create_model(configs):\n",
    "    \"\"\"Create model based on architecture name\"\"\"\n",
    "    if configs.arch == 'ttnet':\n",
    "        ttnet_model = TTNet(dropout_p=configs.dropout_p, tasks=configs.tasks, input_size=configs.input_size,\n",
    "                            thresh_ball_pos_mask=configs.thresh_ball_pos_mask,\n",
    "                            num_frames_sequence=configs.num_frames_sequence)\n",
    "    else:\n",
    "        assert False, 'Undefined model backbone'\n",
    "\n",
    "    if configs.multitask_learning == True:\n",
    "        model = Multi_Task_Learning_Model(ttnet_model, tasks=configs.tasks, num_events=configs.num_events,\n",
    "                                          weights_events=configs.events_weights_loss,\n",
    "                                          input_size=configs.input_size, sigma=configs.sigma,\n",
    "                                          thresh_ball_pos_mask=configs.thresh_ball_pos_mask, device=configs.device)\n",
    "    else:\n",
    "        model = Unbalance_Loss_Model(ttnet_model, tasks_loss_weight=configs.tasks_loss_weight,\n",
    "                                     weights_events=configs.events_weights_loss, input_size=configs.input_size,\n",
    "                                     sigma=configs.sigma, thresh_ball_pos_mask=configs.thresh_ball_pos_mask,\n",
    "                                     device=configs.device)\n",
    "\n",
    "    return model\n",
    "def get_num_parameters(model):\n",
    "    \"\"\"Count number of trained parameters of the model\"\"\"\n",
    "    if hasattr(model, 'module'):\n",
    "        num_parameters = sum(p.numel() for p in model.module.parameters() if p.requires_grad)\n",
    "    else:\n",
    "        num_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    return num_parameters\n",
    "def freeze_model(model, freeze_modules_list):\n",
    "    \"\"\"Freeze modules of the model based on the configuration\"\"\"\n",
    "    for layer_name, p in model.named_parameters():\n",
    "        p.requires_grad = True\n",
    "        for freeze_module in freeze_modules_list:\n",
    "            if freeze_module in layer_name:\n",
    "                p.requires_grad = False\n",
    "                break\n",
    "\n",
    "    return model\n",
    "def load_weights_local_stage(pretrained_dict):\n",
    "    \"\"\"Overwrite the weights of the global stage to the local stage\"\"\"\n",
    "    local_weights_dict = {}\n",
    "    for layer_name, v in pretrained_dict.items():\n",
    "        if 'ball_global_stage' in layer_name:\n",
    "            layer_name_parts = layer_name.split('.')\n",
    "            layer_name_parts[1] = 'ball_local_stage'\n",
    "            local_name = '.'.join(layer_name_parts)\n",
    "            local_weights_dict[local_name] = v\n",
    "\n",
    "    return {**pretrained_dict, **local_weights_dict}\n",
    "\n",
    "def load_pretrained_model(model, pretrained_path, gpu_idx, overwrite_global_2_local):\n",
    "    \"\"\"Load weights from the pretrained model\"\"\"\n",
    "    assert os.path.isfile(pretrained_path), \"=> no checkpoint found at '{}'\".format(pretrained_path)\n",
    "    if gpu_idx is None:\n",
    "        checkpoint = torch.load(pretrained_path, map_location='cpu')\n",
    "    else:\n",
    "        # Map model to be loaded to specified single gpu.\n",
    "        loc = 'cuda:{}'.format(gpu_idx)\n",
    "        checkpoint = torch.load(pretrained_path, map_location=loc)\n",
    "    pretrained_dict = checkpoint['state_dict']\n",
    "    if hasattr(model, 'module'):\n",
    "        model_state_dict = model.module.state_dict()\n",
    "        # 1. filter out unnecessary keys\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_state_dict}\n",
    "        # Load global to local stage\n",
    "        if overwrite_global_2_local:\n",
    "            pretrained_dict = load_weights_local_stage(pretrained_dict)\n",
    "        # 2. overwrite entries in the existing state dict\n",
    "        model_state_dict.update(pretrained_dict)\n",
    "        # 3. load the new state dict\n",
    "        model.module.load_state_dict(model_state_dict)\n",
    "    else:\n",
    "        model_state_dict = model.state_dict()\n",
    "        # 1. filter out unnecessary keys\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_state_dict}\n",
    "        # Load global to local stage\n",
    "        if overwrite_global_2_local:\n",
    "            pretrained_dict = load_weights_local_stage(pretrained_dict)\n",
    "        # 2. overwrite entries in the existing state dict\n",
    "        model_state_dict.update(pretrained_dict)\n",
    "        # 3. load the new state dict\n",
    "        model.load_state_dict(model_state_dict)\n",
    "    return model\n",
    "\n",
    "def resume_model(resume_path, arch, gpu_idx):\n",
    "    \"\"\"Resume training model from the previous trained checkpoint\"\"\"\n",
    "    assert os.path.isfile(resume_path), \"=> no checkpoint found at '{}'\".format(resume_path)\n",
    "    if gpu_idx is None:\n",
    "        checkpoint = torch.load(resume_path, map_location='cpu')\n",
    "    else:\n",
    "        # Map model to be loaded to specified single gpu.\n",
    "        loc = 'cuda:{}'.format(gpu_idx)\n",
    "        checkpoint = torch.load(resume_path, map_location=loc)\n",
    "    assert arch == checkpoint['configs'].arch, \"Load the different arch...\"\n",
    "    print(\"=> loaded checkpoint '{}' (epoch {})\".format(resume_path, checkpoint['epoch']))\n",
    "\n",
    "    return checkpoint\n",
    "def make_data_parallel(model, configs):\n",
    "    if configs.distributed:\n",
    "        # For multiprocessing distributed, DistributedDataParallel constructor\n",
    "        # should always set the single device scope, otherwise,\n",
    "        # DistributedDataParallel will use all available devices.\n",
    "        if configs.gpu_idx is not None:\n",
    "            torch.cuda.set_device(configs.gpu_idx)\n",
    "            model.cuda(configs.gpu_idx)\n",
    "            # When using a single GPU per process and per\n",
    "            # DistributedDataParallel, we need to divide the batch size\n",
    "            # ourselves based on the total number of GPUs we have\n",
    "            configs.batch_size = int(configs.batch_size / configs.ngpus_per_node)\n",
    "            configs.num_workers = int((configs.num_workers + configs.ngpus_per_node - 1) / configs.ngpus_per_node)\n",
    "            model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[configs.gpu_idx],\n",
    "                                                              find_unused_parameters=True)\n",
    "        else:\n",
    "            model.cuda()\n",
    "            # DistributedDataParallel will divide and allocate batch_size to all\n",
    "            # available GPUs if device_ids are not set\n",
    "            model = torch.nn.parallel.DistributedDataParallel(model)\n",
    "    elif configs.gpu_idx is not None:\n",
    "        torch.cuda.set_device(configs.gpu_idx)\n",
    "        model = model.cuda(configs.gpu_idx)\n",
    "    else:\n",
    "        # DataParallel will divide and allocate batch_size to all available GPUs\n",
    "        model = torch.nn.DataParallel(model).cuda()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f7bc4f",
   "metadata": {},
   "source": [
    "### Training Debugger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185839b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(configs)\n",
    "# model = make_data_parallel(model, configs)\n",
    "model = freeze_model(model, configs.freeze_modules_list)\n",
    "print(\"Total Parameters: \", get_num_parameters(model) / 1000000 , \"M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a346e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer       = create_optimizer(configs, model)\n",
    "lr_scheduler    = create_lr_scheduler(optimizer, configs)\n",
    "best_val_loss   = np.inf\n",
    "earlystop_count = 0\n",
    "is_best         = False\n",
    "print(configs.pretrained_path, configs.resume_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d928a2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # optionally load weight from a checkpoint\n",
    "# if configs.pretrained_path is not None:\n",
    "#     model = load_pretrained_model(model, configs.pretrained_path, gpu_idx, configs.overwrite_global_2_local)\n",
    "#     if logger is not None:\n",
    "#         logger.info('loaded pretrained model at {}'.format(configs.pretrained_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f6bd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # optionally resume from a checkpoint\n",
    "# if configs.resume_path is not None:\n",
    "#     checkpoint = resume_model(configs.resume_path, configs.arch, configs.gpu_idx)\n",
    "#     if hasattr(model, 'module'):\n",
    "#         model.module.load_state_dict(checkpoint['state_dict'])\n",
    "#     else:\n",
    "#         model.load_state_dict(checkpoint['state_dict'])\n",
    "#     optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "#     lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "#     best_val_loss = checkpoint['best_val_loss']\n",
    "#     earlystop_count = checkpoint['earlystop_count']\n",
    "#     configs.start_epoch = checkpoint['epoch'] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c69a021",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for i in range(3):\n",
    "#     configs['train_game_list'].pop()\n",
    "# configs['train_game_list'].pop(0)\n",
    "configs['train_game_list'] = ['wtt_1']\n",
    "print(configs['train_game_list'])\n",
    "# for i in enumerate(train_loader):\n",
    "#     print(len(i[1]))\n",
    "print(configs['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9e945b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloader\n",
    "train_loader, val_loader, train_sampler = check_dataloader()\n",
    "# train_loader, val_loader, train_sampler = create_train_val_dataloader(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1ad3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"../../dataset/training/annotations/wtt_1/segmentation_masks/3775.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb3df79",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8ca2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f4e69d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e89f461",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b6e096",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd9d1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# test_loader = create_test_dataloader(configs)\n",
    "if logger is not None:\n",
    "    logger.info('number of batches in train set: {}'.format(len(train_loader)))\n",
    "    if val_loader is not None:\n",
    "        logger.info('number of batches in val set: {}'.format(len(val_loader)))\n",
    "    logger.info('number of batches in test set: {}'.format(len(test_loader)))\n",
    "\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03daa4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls ../../dataset/training/annotations/wtt_1/segmentation_masks/66.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cf76a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d444d2",
   "metadata": {
    "code_folding": [
     0,
     48
    ]
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(train_loader, model, optimizer, epoch, configs, logger):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    data_time = AverageMeter('Data', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "\n",
    "    progress = ProgressMeter(len(train_loader), [batch_time, data_time, losses],\n",
    "                             prefix=\"Train - Epoch: [{}/{}]\".format(epoch, configs.num_epochs))\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    for batch_idx, (resized_imgs, org_ball_pos_xy, global_ball_pos_xy, target_events, target_seg) in enumerate(\n",
    "            tqdm(train_loader)):\n",
    "        data_time.update(time.time() - start_time)\n",
    "        batch_size = resized_imgs.size(0)\n",
    "        target_seg = target_seg.to(configs.device, non_blocking=True)\n",
    "        resized_imgs = resized_imgs.to(configs.device, non_blocking=True).float()\n",
    "        pred_ball_global, pred_ball_local, pred_events, pred_seg, local_ball_pos_xy, total_loss, _ = model(\n",
    "            resized_imgs, org_ball_pos_xy, global_ball_pos_xy, target_events, target_seg)\n",
    "        # For torch.nn.DataParallel case\n",
    "        if (not configs.distributed) and (configs.gpu_idx is None):\n",
    "            total_loss = torch.mean(total_loss)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # compute gradient and perform backpropagation\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if configs.distributed:\n",
    "            reduced_loss = reduce_tensor(total_loss.data, configs.world_size)\n",
    "        else:\n",
    "            reduced_loss = total_loss.data\n",
    "        losses.update(to_python_float(reduced_loss), batch_size)\n",
    "        # measure elapsed time\n",
    "        torch.cuda.synchronize()\n",
    "        batch_time.update(time.time() - start_time)\n",
    "\n",
    "        # Log message\n",
    "        if logger is not None:\n",
    "            if ((batch_idx + 1) % configs.print_freq) == 0:\n",
    "                logger.info(progress.get_message(batch_idx))\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "    return losses.avg\n",
    "\n",
    "def evaluate_one_epoch(val_loader, model, epoch, configs, logger):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    data_time = AverageMeter('Data', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "\n",
    "    progress = ProgressMeter(len(val_loader), [batch_time, data_time, losses],\n",
    "                             prefix=\"Evaluate - Epoch: [{}/{}]\".format(epoch, configs.num_epochs))\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        start_time = time.time()\n",
    "        for batch_idx, (resized_imgs, org_ball_pos_xy, global_ball_pos_xy, target_events, target_seg) in enumerate(\n",
    "                tqdm(val_loader)):\n",
    "            data_time.update(time.time() - start_time)\n",
    "            batch_size = resized_imgs.size(0)\n",
    "            target_seg = target_seg.to(configs.device, non_blocking=True)\n",
    "            resized_imgs = resized_imgs.to(configs.device, non_blocking=True).float()\n",
    "            pred_ball_global, pred_ball_local, pred_events, pred_seg, local_ball_pos_xy, total_loss, _ = model(\n",
    "                resized_imgs, org_ball_pos_xy, global_ball_pos_xy, target_events, target_seg)\n",
    "\n",
    "            # For torch.nn.DataParallel case\n",
    "            if (not configs.distributed) and (configs.gpu_idx is None):\n",
    "                total_loss = torch.mean(total_loss)\n",
    "\n",
    "            if configs.distributed:\n",
    "                reduced_loss = reduce_tensor(total_loss.data, configs.world_size)\n",
    "            else:\n",
    "                reduced_loss = total_loss.data\n",
    "            losses.update(to_python_float(reduced_loss), batch_size)\n",
    "            # measure elapsed time\n",
    "            torch.cuda.synchronize()\n",
    "            batch_time.update(time.time() - start_time)\n",
    "\n",
    "            # Log message\n",
    "            if logger is not None:\n",
    "                if ((batch_idx + 1) % configs.print_freq) == 0:\n",
    "                    logger.info(progress.get_message(batch_idx))\n",
    "\n",
    "            start_time = time.time()\n",
    "    print(\"Loss: \", losses.avg)\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf64fa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(configs.device)\n",
    "configs.batch_size = 64\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75af42e1",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "for epoch in range(configs.start_epoch, configs.num_epochs + 1):\n",
    "        # Get the current learning rate\n",
    "        for param_group in optimizer.param_groups:\n",
    "            lr = param_group['lr']\n",
    "        if logger is not None:\n",
    "            logger.info('{}'.format('*-' * 40))\n",
    "            logger.info('{} {}/{} {}'.format('=' * 35, epoch, configs.num_epochs, '=' * 35))\n",
    "            logger.info('{}'.format('*-' * 40))\n",
    "            logger.info('>>> Epoch: [{}/{}] learning rate: {:.2e}'.format(epoch, configs.num_epochs, lr))\n",
    "            \n",
    "            if configs.distributed:\n",
    "                train_sampler.set_epoch(epoch)\n",
    "            \n",
    "            # train for one epoch\n",
    "            train_loss = train_one_epoch(train_loader, model, optimizer, epoch, configs, logger)\n",
    "            loss_dict = {'train': train_loss}\n",
    "            if not configs.no_val:\n",
    "                val_loss = evaluate_one_epoch(val_loader, model, epoch, configs, logger)\n",
    "                is_best = val_loss <= best_val_loss\n",
    "                best_val_loss = min(val_loss, best_val_loss)\n",
    "                loss_dict['val'] = val_loss\n",
    "\n",
    "            if not configs.no_test:\n",
    "                test_loss = evaluate_one_epoch(test_loader, model, epoch, configs, logger)\n",
    "                loss_dict['test'] = test_loss\n",
    "            # Write tensorboard\n",
    "#             if tb_writer is not None:\n",
    "#                 tb_writer.add_scalars('Loss', loss_dict, epoch)\n",
    "            # Save checkpoint\n",
    "            if (is_best or ((epoch % configs.checkpoint_freq) == 0)):\n",
    "                saved_state = get_saved_state(model, optimizer, lr_scheduler, epoch, configs, best_val_loss,\n",
    "                                              earlystop_count)\n",
    "                save_checkpoint(configs.checkpoints_dir, configs.saved_fn, saved_state, is_best, epoch)\n",
    "            # Check early stop training\n",
    "            if configs.earlystop_patience is not None:\n",
    "                earlystop_count = 0 if is_best else (earlystop_count + 1)\n",
    "                print_string = ' |||\\t earlystop_count: {}'.format(earlystop_count)\n",
    "                if configs.earlystop_patience <= earlystop_count:\n",
    "                    print_string += '\\n\\t--- Early stopping!!!'\n",
    "                    break\n",
    "                else:\n",
    "                    print_string += '\\n\\t--- Continue training..., earlystop_count: {}'.format(earlystop_count)\n",
    "                if logger is not None:\n",
    "                    logger.info(print_string)\n",
    "            # Adjust learning rate\n",
    "            if configs.lr_type == 'plateau':\n",
    "                assert (not configs.no_val), \"Only use plateau when having validation set\"\n",
    "                lr_scheduler.step(val_loss)\n",
    "            else:\n",
    "                lr_scheduler.step()\n",
    "\n",
    "#         if tb_writer is not None:\n",
    "#             tb_writer.close()\n",
    "        if configs.distributed:\n",
    "            cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2986c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7a8a4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26429755",
   "metadata": {},
   "source": [
    "### Training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
