{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e117e23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44992a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29298111",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from turbojpeg import TurboJPEG\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from data_process.ttnet_data_utils import load_raw_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1fb462",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Dataset\n",
    "from utils.misc import *\n",
    "from utils.logger import Logger\n",
    "from config.config import parse_configs\n",
    "from utils.train_utils import create_optimizer, create_lr_scheduler,  reduce_tensor, to_python_float, get_saved_state, save_checkpoint\n",
    "from data_process.ttnet_data_utils import train_val_data_separation, get_events_infor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f4a10f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "985d4b2f",
   "metadata": {},
   "source": [
    "### Phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5879ec33",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = parse_configs()\n",
    "configs.distributed = False\n",
    "# configs.multitask_learning = True\n",
    "\n",
    "# Phase 1\n",
    "configs.smooth_labelling = True\n",
    "configs.global_weight  = 5\n",
    "configs.lr_factor =  0.5\n",
    "configs.saved_fn = 'ttnet_1st_phase'\n",
    "configs.no_val = True\n",
    "configs.lr = 0.001 \n",
    "configs.lr_type ='step_lr' \n",
    "configs.lr_step_size = 10 \n",
    "configs.lr_factor = 0.1\n",
    "configs.gpu_idx = 0 \n",
    "configs.global_weight = 5. \n",
    "configs.no_event = True\n",
    "configs.no_local = True\n",
    "configs.print_freq =  500\n",
    "configs.batch_size = 24\n",
    "# configs.sigma =  1.0\n",
    "\n",
    "#Phase 2\n",
    "# configs.saved_fn  = 'ttnet_2nd_phase' \n",
    "# configs.no-val = True  \n",
    "# configs.lr = 0.001 \n",
    "# configs.lr_type = 'step_lr' \n",
    "# configs.lr_step_size =  10 \n",
    "# configs.lr_factor = 0.1 \n",
    "# configs.gpu_idx = 0 \n",
    "# configs.global_weight = 0. \n",
    "# configs.event_weight = 2. \n",
    "# configs.local_weight = 1. \n",
    "# configs.pretrained_path ../checkpoints/ttnet_1st_phase/ttnet_1st_phase_epoch_30.pth \n",
    "# configs.overwrite_global_2_local  = True\n",
    "# configs.freeze_global  = True\n",
    "# configs.smooth-labelling = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7324fabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = Logger(configs.logs_dir, configs.saved_fn)\n",
    "logger.info('>>> Created a new logger')\n",
    "logger.info('>>> configs: {}'.format(configs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cc6397",
   "metadata": {},
   "source": [
    "#### Dataloader Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f65a52",
   "metadata": {
    "code_folding": [
     1,
     2
    ]
   },
   "outputs": [],
   "source": [
    "## Dataset Class\n",
    "class TTNet_Dataset(Dataset):\n",
    "    def __init__(self, events_infor, org_size, input_size, transform=None, num_samples=None):\n",
    "        self.events_infor = events_infor\n",
    "        self.w_org = org_size[0]\n",
    "        self.h_org = org_size[1]\n",
    "        self.w_input = input_size[0]\n",
    "        self.h_input = input_size[1]\n",
    "        self.w_resize_ratio = self.w_org / self.w_input\n",
    "        self.h_resize_ratio = self.h_org / self.h_input\n",
    "        self.transform = transform\n",
    "        if num_samples is not None:\n",
    "            self.events_infor = self.events_infor[:num_samples]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.events_infor)\n",
    "\n",
    "    def __resize_ball_pos__(self, ball_pos_xy, w_ratio, h_ratio):\n",
    "        return np.array([ball_pos_xy[0] / w_ratio, ball_pos_xy[1] / h_ratio])\n",
    "\n",
    "    def __check_ball_pos__(self, ball_pos_xy, w, h):\n",
    "        if not ((0 < ball_pos_xy[0] < w) and (0 < ball_pos_xy[1] < h)):\n",
    "            ball_pos_xy[0] = -1.\n",
    "            ball_pos_xy[1] = -1.\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path_list, org_ball_pos_xy, target_events = self.events_infor[index]\n",
    "        self.jpeg_reader = TurboJPEG()  # improve it later (Only initialize it once)\n",
    "        # Load a sequence of images (-4, 4), resize images before stacking them together\n",
    "        # Use TurboJPEG to speed up the loading images' phase\n",
    "        resized_imgs = []\n",
    "        for img_path in img_path_list:\n",
    "            in_file = open(img_path, 'rb')\n",
    "            resized_imgs.append(cv2.resize(self.jpeg_reader.decode(in_file.read(), 0), (self.w_input, self.h_input)))\n",
    "            in_file.close()\n",
    "        resized_imgs = np.dstack(resized_imgs)  # (128, 320, 27)\n",
    "        # Adjust ball pos: full HD --> (320, 128)\n",
    "        global_ball_pos_xy = self.__resize_ball_pos__(org_ball_pos_xy, self.w_resize_ratio, self.h_resize_ratio)\n",
    "\n",
    "        # Apply augmentation\n",
    "        if self.transform:\n",
    "            resized_imgs, global_ball_pos_xy = self.transform(resized_imgs, global_ball_pos_xy)\n",
    "        # Adjust ball pos: (320, 128) --> full HD\n",
    "        org_ball_pos_xy = self.__resize_ball_pos__(global_ball_pos_xy, 1. / self.w_resize_ratio,\n",
    "                                                   1. / self.h_resize_ratio)\n",
    "        # If the ball position is outside of the resized image, set position to -1, -1 --> No ball (just for safety)\n",
    "        self.__check_ball_pos__(org_ball_pos_xy, self.w_org, self.h_org)\n",
    "        self.__check_ball_pos__(global_ball_pos_xy, self.w_input, self.h_input)\n",
    "\n",
    "        # Transpose (H, W, C) to (C, H, W) --> fit input of Pytorch model\n",
    "        resized_imgs = resized_imgs.transpose(2, 0, 1)\n",
    "\n",
    "        return resized_imgs, org_ball_pos_xy.astype(np.int), global_ball_pos_xy.astype(np.int), target_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce80f22",
   "metadata": {
    "code_folding": [
     1,
     11,
     22,
     33,
     51,
     81,
     100
    ]
   },
   "outputs": [],
   "source": [
    "## Transformations \n",
    "class Compose(object):\n",
    "    def __init__(self, transforms, p=1.0):\n",
    "        self.transforms = transforms\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, imgs, ball_position_xy):\n",
    "        if random.random() <= self.p:\n",
    "            for t in self.transforms:\n",
    "                imgs, ball_position_xy = t(imgs, ball_position_xy)\n",
    "        return imgs, ball_position_xy\n",
    "class Normalize():\n",
    "    def __init__(self, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), num_frames_sequence=9, p=1.0):\n",
    "        self.p = p\n",
    "        self.mean = np.repeat(np.array(mean).reshape(1, 1, 3), repeats=num_frames_sequence, axis=-1)\n",
    "        self.std = np.repeat(np.array(std).reshape(1, 1, 3), repeats=num_frames_sequence, axis=-1)\n",
    "\n",
    "    def __call__(self, imgs, ball_position_xy, seg_img):\n",
    "        if random.random() < self.p:\n",
    "            imgs = ((imgs / 255.) - self.mean) / self.std\n",
    "\n",
    "        return imgs, ball_position_xy, seg_img\n",
    "class Denormalize():\n",
    "    def __init__(self, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), p=1.0):\n",
    "        self.p = p\n",
    "        self.mean = np.array(mean).reshape(1, 1, 3)\n",
    "        self.std = np.array(std).reshape(1, 1, 3)\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img = (img * self.std + self.mean) * 255.\n",
    "        img = img.astype(np.uint8)\n",
    "\n",
    "        return img\n",
    "class Resize(object):\n",
    "    def __init__(self, new_size, p=0.5, interpolation=cv2.INTER_LINEAR):\n",
    "        self.new_size = new_size\n",
    "        self.p = p\n",
    "        self.interpolation = interpolation\n",
    "\n",
    "    def __call__(self, imgs, ball_position_xy):\n",
    "        if random.random() <= self.p:\n",
    "            h, w, c = imgs.shape\n",
    "            # Resize a sequence of images\n",
    "            imgs = cv2.resize(imgs, self.new_size, interpolation=self.interpolation)\n",
    "            # Dont need to resize seg_img\n",
    "            # Adjust ball position\n",
    "            w_ratio = w / self.new_size[0]\n",
    "            h_ratio = h / self.new_size[1]\n",
    "            ball_position_xy = np.array([ball_position_xy[0] / w_ratio, ball_position_xy[1] / h_ratio])\n",
    "\n",
    "        return imgs, ball_position_xy\n",
    "class Random_Crop(object):\n",
    "    def __init__(self, max_reduction_percent=0.15, p=0.5, interpolation=cv2.INTER_LINEAR):\n",
    "        self.max_reduction_percent = max_reduction_percent\n",
    "        self.p = p\n",
    "        self.interpolation = interpolation\n",
    "\n",
    "    def __call__(self, imgs, ball_position_xy):\n",
    "        # imgs are before resizing\n",
    "        if random.random() <= self.p:\n",
    "            h, w, c = imgs.shape\n",
    "            # Calculate min_x, max_x, min_y, max_y\n",
    "            remain_percent = random.uniform(1. - self.max_reduction_percent, 1.)\n",
    "            new_w = remain_percent * w\n",
    "            min_x = int(random.uniform(0, w - new_w))\n",
    "            max_x = int(min_x + new_w)\n",
    "            w_ratio = w / new_w\n",
    "\n",
    "            new_h = remain_percent * h\n",
    "            min_y = int(random.uniform(0, h - new_h))\n",
    "            max_y = int(new_h + min_y)\n",
    "            h_ratio = h / new_h\n",
    "            # crop a sequence of images\n",
    "            imgs = imgs[min_y:max_y, min_x:max_x, :]\n",
    "            imgs = cv2.resize(imgs, (w, h), interpolation=self.interpolation)\n",
    "\n",
    "            # Adjust ball position\n",
    "            ball_position_xy = np.array([(ball_position_xy[0] - min_x) * w_ratio,\n",
    "                                         (ball_position_xy[1] - min_y) * h_ratio])\n",
    "\n",
    "        return imgs, ball_position_xy\n",
    "class Random_Rotate(object):\n",
    "    def __init__(self, rotation_angle_limit=15, p=0.5):\n",
    "        self.rotation_angle_limit = rotation_angle_limit\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, imgs, ball_position_xy):\n",
    "        if random.random() <= self.p:\n",
    "            random_angle = random.uniform(-self.rotation_angle_limit, self.rotation_angle_limit)\n",
    "            # Rotate a sequence of imgs\n",
    "            h, w, c = imgs.shape\n",
    "            center = (int(w / 2), int(h / 2))\n",
    "            rotate_matrix = cv2.getRotationMatrix2D(center, random_angle, 1.)\n",
    "            imgs = cv2.warpAffine(imgs, rotate_matrix, (w, h), flags=cv2.INTER_LINEAR)\n",
    "\n",
    "            # Adjust ball position, apply the same rotate_matrix for the sequential images\n",
    "            ball_position_xy = rotate_matrix.dot(np.array([ball_position_xy[0], ball_position_xy[1], 1.]).T)\n",
    "\n",
    "\n",
    "        return imgs, ball_position_xy\n",
    "class Random_HFlip(object):\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, imgs, ball_position_xy):\n",
    "        if random.random() <= self.p:\n",
    "            h, w, c = imgs.shape\n",
    "            # Horizontal flip a sequence of imgs\n",
    "            imgs = cv2.flip(imgs, 1)\n",
    "            # Adjust ball position: Same y, new x = w - x\n",
    "            ball_position_xy[0] = w - ball_position_xy[0]\n",
    "\n",
    "        return imgs, ball_position_xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d8f574",
   "metadata": {
    "code_folding": [
     1,
     31
    ]
   },
   "outputs": [],
   "source": [
    "#Dataloader\n",
    "def create_train_val_dataloader(configs):\n",
    "    \"\"\"Create dataloader for training and validate\"\"\"\n",
    "\n",
    "    train_transform = Compose([\n",
    "        Random_Crop(max_reduction_percent=0.15, p=0.5),\n",
    "        Random_HFlip(p=0.5),\n",
    "        Random_Rotate(rotation_angle_limit=10, p=0.5),\n",
    "    ], p=1.)\n",
    "\n",
    "    train_events_infor, val_events_infor, *_ = train_val_data_separation(configs)\n",
    "    train_dataset = TTNet_Dataset(train_events_infor, configs.org_size, configs.input_size, transform=train_transform,\n",
    "                                  num_samples=configs.num_samples)\n",
    "    train_sampler = None\n",
    "    if configs.distributed:\n",
    "        train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=configs.batch_size, shuffle=(train_sampler is None),\n",
    "                                  pin_memory=configs.pin_memory, num_workers=configs.num_workers, sampler=train_sampler)\n",
    "\n",
    "    val_dataloader = None\n",
    "    if not configs.no_val:\n",
    "        val_transform = None\n",
    "        val_sampler = None\n",
    "        val_dataset = TTNet_Dataset(val_events_infor, configs.org_size, configs.input_size, transform=val_transform,\n",
    "                                    num_samples=configs.num_samples)\n",
    "        if configs.distributed:\n",
    "            val_sampler = torch.utils.data.distributed.DistributedSampler(val_dataset, shuffle=False)\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=configs.batch_size, shuffle=False,\n",
    "                                    pin_memory=configs.pin_memory, num_workers=configs.num_workers, sampler=val_sampler)\n",
    "\n",
    "    return train_dataloader, val_dataloader, train_sampler\n",
    "def create_test_dataloader(configs):\n",
    "    \"\"\"Create dataloader for testing phase\"\"\"\n",
    "\n",
    "    test_transform = None\n",
    "    dataset_type = 'test'\n",
    "    test_events_infor, test_events_labels = get_events_infor(configs.test_game_list, configs, dataset_type)\n",
    "    test_dataset = TTNet_Dataset(test_events_infor, configs.org_size, configs.input_size, transform=test_transform,\n",
    "                                 num_samples=configs.num_samples)\n",
    "    test_sampler = None\n",
    "    if configs.distributed:\n",
    "        test_sampler = torch.utils.data.distributed.DistributedSampler(test_dataset)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=configs.batch_size, shuffle=False,\n",
    "                                 pin_memory=configs.pin_memory, num_workers=configs.num_workers, sampler=test_sampler)\n",
    "\n",
    "    return test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8055ea",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def check_dataset():\n",
    "    configs = parse_configs()\n",
    "    game_list    = ['game_1']\n",
    "    dataset_type = 'training'\n",
    "    train_events_infor, val_events_infor, *_ = train_val_data_separation(configs)\n",
    "    print('len(train_events_infor): {}'.format(len(train_events_infor)))\n",
    "    # Test transformation\n",
    "    transform = Compose([\n",
    "        Random_Crop(max_reduction_percent=0.15, p=1.),\n",
    "        Random_HFlip(p=1.),\n",
    "        Random_Rotate(rotation_angle_limit=15, p=1.)\n",
    "    ], p=1.)\n",
    "    ttnet_dataset = TTNet_Dataset(train_events_infor, configs.org_size, configs.input_size, transform=transform)\n",
    "    print('len(ttnet_dataset): {}'.format(len(ttnet_dataset)))\n",
    "    example_index = 100\n",
    "    resized_imgs, org_ball_pos_xy, global_ball_pos_xy, target_event = ttnet_dataset.__getitem__(example_index)\n",
    "    if 1:\n",
    "        # Test F.interpolate, we can simply use cv2.resize() to get origin_imgs from resized_imgs\n",
    "        # Achieve better quality of images and faster\n",
    "        origin_imgs = F.interpolate(torch.from_numpy(resized_imgs).unsqueeze(0).float(), (1080, 1920))\n",
    "        origin_imgs = origin_imgs.squeeze().numpy().transpose(1, 2, 0).astype(np.uint8)\n",
    "        print('F.interpolate - origin_imgs shape: {}'.format(origin_imgs.shape))\n",
    "        resized_imgs = resized_imgs.transpose(1, 2, 0)\n",
    "        print('resized_imgs shape: {}'.format(resized_imgs.shape))\n",
    "    else:\n",
    "        # Test cv2.resize\n",
    "        resized_imgs = resized_imgs.transpose(1, 2, 0)\n",
    "        print('resized_imgs shape: {}'.format(resized_imgs.shape))\n",
    "        origin_imgs = cv2.resize(resized_imgs, (1920, 1080))\n",
    "        print('cv2.resize - origin_imgs shape: {}'.format(origin_imgs.shape))\n",
    "        \n",
    "    out_images_dir = os.path.join(configs.results_dir, 'debug', 'ttnet_dataset')\n",
    "    if not os.path.isdir(out_images_dir):\n",
    "        os.makedirs(out_images_dir)\n",
    "        \n",
    "    fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(20, 20))\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    for i in range(configs.num_frames_sequence):\n",
    "        img = origin_imgs[:, :, (i * 3): (i + 1) * 3]\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title('image {}'.format(i))\n",
    "    fig.suptitle(\n",
    "        'Event: is bounce {}, is net: {}, ball_position_xy: (x= {}, y= {})'.format(target_event[0], target_event[1],\n",
    "                                                                                   org_ball_pos_xy[0],\n",
    "                                                                                   org_ball_pos_xy[1]),\n",
    "        fontsize=16)\n",
    "    plt.savefig(os.path.join(out_images_dir, 'org_all_imgs_{}.jpg'.format(example_index)))\n",
    "\n",
    "\n",
    "    for i in range(configs.num_frames_sequence):\n",
    "        img = resized_imgs[:, :, (i * 3): (i + 1) * 3]\n",
    "        if (i == (configs.num_frames_sequence - 1)):\n",
    "            img = cv2.resize(img, (img.shape[1], img.shape[0]))\n",
    "            ball_img = cv2.circle(img, tuple(global_ball_pos_xy), radius=5, color=(255, 0, 0), thickness=2)\n",
    "            ball_img = cv2.cvtColor(ball_img, cv2.COLOR_RGB2BGR)\n",
    "            cv2.imwrite(os.path.join(out_images_dir, 'augment_img_{}.jpg'.format(example_index)),\n",
    "                        ball_img)\n",
    "\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title('image {}'.format(i))\n",
    "    fig.suptitle(\n",
    "        'Event: is bounce {}, is net: {}, ball_position_xy: (x= {}, y= {})'.format(target_event[0], target_event[1],\n",
    "                                                                                   global_ball_pos_xy[0],\n",
    "                                                                                   global_ball_pos_xy[1]),\n",
    "        fontsize=16)\n",
    "    plt.savefig(os.path.join(out_images_dir, 'augment_all_imgs_{}.jpg'.format(example_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab60492",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# def check_dataloader():\n",
    "#     configs = parse_configs()\n",
    "#     configs.distributed = False  # For testing\n",
    "#     train_dataloader, val_dataloader, train_sampler = create_train_val_dataloader(configs)\n",
    "#     print('len train_dataloader: {}, val_dataloader: {}'.format(len(train_dataloader), len(val_dataloader)))\n",
    "#     return train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef37785d",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# train_dataloader = check_dataloader()\n",
    "# for data in train_dataloader:\n",
    "#     print(len(data))\n",
    "#     break\n",
    "# data[0] : stack of images ( 9 frames per batch )\n",
    "# data[1] : original coordinated of ball\n",
    "# data[2] : scaled coordinates of ball\n",
    "# data[3] : event labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a17f82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(4):\n",
    "#     print(data[i].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eac300e",
   "metadata": {},
   "source": [
    "#### Model Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74eadecf",
   "metadata": {},
   "source": [
    "#####  TTNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17534a48",
   "metadata": {},
   "source": [
    "<img src=\"artifacts/network.png\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ec251f",
   "metadata": {
    "code_folding": [
     0,
     11,
     21,
     58,
     84,
     91,
     93,
     112,
     125
    ]
   },
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.maxpool(self.relu(self.batchnorm(self.conv(x))))\n",
    "        return x\n",
    "class ConvBlock_without_Pooling(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvBlock_without_Pooling, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.batchnorm(self.conv(x)))\n",
    "        return x\n",
    "class BallDetection(nn.Module):\n",
    "    def __init__(self, num_frames_sequence, dropout_p):\n",
    "        super(BallDetection, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_frames_sequence * 3, 64, kernel_size=1, stride=1, padding=0)\n",
    "        self.batchnorm = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.convblock1 = ConvBlock(in_channels=64, out_channels=64)\n",
    "        self.convblock2 = ConvBlock(in_channels=64, out_channels=64)\n",
    "        self.dropout2d = nn.Dropout2d(p=dropout_p)\n",
    "        self.convblock3 = ConvBlock(in_channels=64, out_channels=128)\n",
    "        self.convblock4 = ConvBlock(in_channels=128, out_channels=128)\n",
    "        self.convblock5 = ConvBlock(in_channels=128, out_channels=256)\n",
    "        self.convblock6 = ConvBlock(in_channels=256, out_channels=256)\n",
    "        self.fc1 = nn.Linear(in_features=2560, out_features=1792)\n",
    "        self.fc2 = nn.Linear(in_features=1792, out_features=896)\n",
    "        self.fc3 = nn.Linear(in_features=896, out_features=448)\n",
    "        self.dropout1d = nn.Dropout(p=dropout_p)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.batchnorm(self.conv1(x)))\n",
    "        out_block2 = self.convblock2(self.convblock1(x))\n",
    "        x = self.dropout2d(out_block2)\n",
    "        out_block3 = self.convblock3(x)\n",
    "        out_block4 = self.convblock4(out_block3)\n",
    "        x = self.dropout2d(out_block4)\n",
    "        out_block5 = self.convblock5(out_block4)\n",
    "        features = self.convblock6(out_block5)\n",
    "\n",
    "        x = self.dropout2d(features)\n",
    "        x = x.contiguous().view(x.size(0), -1)\n",
    "\n",
    "        x = self.dropout1d(self.relu(self.fc1(x)))\n",
    "        x = self.dropout1d(self.relu(self.fc2(x)))\n",
    "        out = self.sigmoid(self.fc3(x))\n",
    "\n",
    "        return out, features#, out_block2, out_block3, out_block4, out_block5\n",
    "class EventsSpotting(nn.Module):\n",
    "    def __init__(self, dropout_p):\n",
    "        super(EventsSpotting, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(512, 64, kernel_size=1, stride=1, padding=0)\n",
    "        self.batchnorm = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout2d = nn.Dropout2d(p=dropout_p)\n",
    "        self.convblock = ConvBlock_without_Pooling(in_channels=64, out_channels=64)\n",
    "        self.fc1 = nn.Linear(in_features=640, out_features=512)\n",
    "        self.fc2 = nn.Linear(in_features=512, out_features=2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, global_features, local_features):\n",
    "        input_eventspotting = torch.cat((global_features, local_features), dim=1)\n",
    "        x = self.relu(self.batchnorm(self.conv1(input_eventspotting)))\n",
    "        x = self.dropout2d(x)\n",
    "        x = self.convblock(x)\n",
    "        x = self.dropout2d(x)\n",
    "        x = self.convblock(x)\n",
    "        x = self.dropout2d(x)\n",
    "\n",
    "        x = x.contiguous().view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        out = self.sigmoid(self.fc2(x))\n",
    "\n",
    "        return out\n",
    "class TTNet(nn.Module):\n",
    "    def __init__(self, dropout_p, tasks, input_size, thresh_ball_pos_mask, num_frames_sequence,\n",
    "                 mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n",
    "        super(TTNet, self).__init__()\n",
    "        self.tasks = tasks\n",
    "        self.ball_local_stage, self.events_spotting = None, None, \n",
    "        self.ball_global_stage = BallDetection(num_frames_sequence=num_frames_sequence, dropout_p=dropout_p)\n",
    "        if 'local' in tasks:\n",
    "            self.ball_local_stage = BallDetection(num_frames_sequence=num_frames_sequence, dropout_p=dropout_p)\n",
    "        if 'event' in tasks:\n",
    "            self.events_spotting = EventsSpotting(dropout_p=dropout_p)\n",
    "        self.w_resize = input_size[0]\n",
    "        self.h_resize = input_size[1]\n",
    "        self.thresh_ball_pos_mask = thresh_ball_pos_mask\n",
    "        self.mean = torch.repeat_interleave(torch.tensor(mean).view(1, 3, 1, 1), repeats=9, dim=1)\n",
    "        self.std = torch.repeat_interleave(torch.tensor(std).view(1, 3, 1, 1), repeats=9, dim=1)\n",
    "\n",
    "    def forward(self, resize_batch_input, org_ball_pos_xy):\n",
    "        \"\"\"Forward propagation\n",
    "        :param resize_batch_input: (batch_size, 27, 128, 320)\n",
    "        :param org_ball_pos_xy: (batch_size, 2) --> Use it to get ground-truth for the local stage\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        pred_ball_local, pred_events, local_ball_pos_xy = None, None, None\n",
    "\n",
    "        # Normalize the input before compute forward propagation\n",
    "        pred_ball_global, global_features = self.ball_global_stage(\n",
    "            self.__normalize__(resize_batch_input))\n",
    "        if self.ball_local_stage is not None:\n",
    "            # Based on the prediction of the global stage, crop the original images\n",
    "            input_ball_local, cropped_params = self.__crop_original_batch__(resize_batch_input, pred_ball_global)\n",
    "            # Get the ground truth of the ball for the local stage\n",
    "            local_ball_pos_xy = self.__get_groundtruth_local_ball_pos__(org_ball_pos_xy, cropped_params)\n",
    "            # Normalize the input before compute forward propagation\n",
    "            pred_ball_local, local_features  = self.ball_local_stage(self.__normalize__(input_ball_local))\n",
    "            # Only consider the events spotting if the model has the local stage for ball detection\n",
    "            if self.events_spotting is not None:\n",
    "                pred_events = self.events_spotting(global_features, local_features)\n",
    "\n",
    "        return pred_ball_global, pred_ball_local, pred_events, local_ball_pos_xy\n",
    "\n",
    "    def run_demo(self, resize_batch_input):\n",
    "        \"\"\"Only for full 4 stages/modules in TTNet\"\"\"\n",
    "\n",
    "        # Normalize the input before compute forward propagation\n",
    "        pred_ball_global, global_features, out_block2, out_block3, out_block4, out_block5 = self.ball_global_stage(\n",
    "            self.__normalize__(resize_batch_input))\n",
    "        input_ball_local, cropped_params = self.__crop_original_batch__(resize_batch_input, pred_ball_global)\n",
    "        # Normalize the input before compute forward propagation\n",
    "        pred_ball_local, local_features, *_ = self.ball_local_stage(self.__normalize__(input_ball_local))\n",
    "        pred_events = self.events_spotting(global_features, local_features)\n",
    "        pred_seg = self.segmentation(out_block2, out_block3, out_block4, out_block5)\n",
    "\n",
    "        return pred_ball_global, pred_ball_local, pred_events, pred_seg\n",
    "\n",
    "    def __normalize__(self, x):\n",
    "        if not self.mean.is_cuda:\n",
    "            self.mean = self.mean.cuda()\n",
    "            self.std = self.std.cuda()\n",
    "\n",
    "        return (x / 255. - self.mean) / self.std\n",
    "\n",
    "    def __get_groundtruth_local_ball_pos__(self, org_ball_pos_xy, cropped_params):\n",
    "        local_ball_pos_xy = torch.zeros_like(org_ball_pos_xy)  # no grad for torch.zeros_like output\n",
    "\n",
    "        for idx, params in enumerate(cropped_params):\n",
    "            is_ball_detected, x_min, x_max, y_min, y_max, x_pad, y_pad = params\n",
    "\n",
    "            if is_ball_detected:\n",
    "                # Get the local ball position based on the crop image informaion\n",
    "                local_ball_pos_xy[idx, 0] = max(org_ball_pos_xy[idx, 0] - x_min + x_pad, -1)\n",
    "                local_ball_pos_xy[idx, 1] = max(org_ball_pos_xy[idx, 1] - y_min + y_pad, -1)\n",
    "                # If the ball is outside of the cropped image --> set position to -1, -1 --> No ball\n",
    "                if (local_ball_pos_xy[idx, 0] >= self.w_resize) or (local_ball_pos_xy[idx, 1] >= self.h_resize) or (\n",
    "                        local_ball_pos_xy[idx, 0] < 0) or (local_ball_pos_xy[idx, 1] < 0):\n",
    "                    local_ball_pos_xy[idx, 0] = -1\n",
    "                    local_ball_pos_xy[idx, 1] = -1\n",
    "            else:\n",
    "                local_ball_pos_xy[idx, 0] = -1\n",
    "                local_ball_pos_xy[idx, 1] = -1\n",
    "        return local_ball_pos_xy\n",
    "\n",
    "    def __crop_original_batch__(self, resize_batch_input, pred_ball_global):\n",
    "        \"\"\"Get input of the local stage by cropping the original images based on the predicted ball position\n",
    "            of the global stage\n",
    "        :param resize_batch_input: (batch_size, 27, 128, 320)\n",
    "        :param pred_ball_global: (batch_size, 448)\n",
    "        :param org_ball_pos_xy: (batch_size, 2)\n",
    "        :return: input_ball_local (batch_size, 27, 128, 320)\n",
    "        \"\"\"\n",
    "        # Process input for local stage based on output of the global one\n",
    "\n",
    "        batch_size = resize_batch_input.size(0)\n",
    "        h_original, w_original = 1080, 1920\n",
    "        h_ratio = h_original / self.h_resize\n",
    "        w_ratio = w_original / self.w_resize\n",
    "        pred_ball_global_mask = pred_ball_global.clone().detach()\n",
    "        pred_ball_global_mask[pred_ball_global_mask < self.thresh_ball_pos_mask] = 0.\n",
    "\n",
    "        # Crop the original images\n",
    "        input_ball_local = torch.zeros_like(resize_batch_input)  # same shape with resize_batch_input, no grad\n",
    "        original_batch_input = F.interpolate(resize_batch_input, (h_original, w_original))  # On GPU\n",
    "        cropped_params = []\n",
    "        for idx in range(batch_size):\n",
    "            pred_ball_pos_x = pred_ball_global_mask[idx, :self.w_resize]\n",
    "            pred_ball_pos_y = pred_ball_global_mask[idx, self.w_resize:]\n",
    "            # If the ball is not detected, we crop the center of the images, set ball_poss to [-1, -1]\n",
    "            if (torch.sum(pred_ball_pos_x) == 0.) or (torch.sum(pred_ball_pos_y) == 0.):\n",
    "                # Assume the ball is in the center image\n",
    "                x_center = int(self.w_resize / 2)\n",
    "                y_center = int(self.h_resize / 2)\n",
    "                is_ball_detected = False\n",
    "            else:\n",
    "                x_center = torch.argmax(pred_ball_pos_x)  # Upper part\n",
    "                y_center = torch.argmax(pred_ball_pos_y)  # Lower part\n",
    "                is_ball_detected = True\n",
    "\n",
    "            # Adjust ball position to the original size\n",
    "            x_center = int(x_center * w_ratio)\n",
    "            y_center = int(y_center * h_ratio)\n",
    "\n",
    "            x_min, x_max, y_min, y_max = self.__get_crop_params__(x_center, y_center, self.w_resize, self.h_resize,\n",
    "                                                                  w_original, h_original)\n",
    "            # Put image to the center\n",
    "            h_crop = y_max - y_min\n",
    "            w_crop = x_max - x_min\n",
    "            x_pad = 0\n",
    "            y_pad = 0\n",
    "            if (h_crop != self.h_resize) or (w_crop != self.w_resize):\n",
    "                x_pad = int((self.w_resize - w_crop) / 2)\n",
    "                y_pad = int((self.h_resize - h_crop) / 2)\n",
    "                input_ball_local[idx, :, y_pad:(y_pad + h_crop), x_pad:(x_pad + w_crop)] = original_batch_input[idx, :,\n",
    "                                                                                           y_min:y_max, x_min: x_max]\n",
    "            else:\n",
    "                input_ball_local[idx, :, :, :] = original_batch_input[idx, :, y_min:y_max, x_min: x_max]\n",
    "            cropped_params.append([is_ball_detected, x_min, x_max, y_min, y_max, x_pad, y_pad])\n",
    "\n",
    "        return input_ball_local, cropped_params\n",
    "\n",
    "    def __get_crop_params__(self, x_center, y_center, w_resize, h_resize, w_original, h_original):\n",
    "        x_min = max(0, x_center - int(w_resize / 2))\n",
    "        y_min = max(0, y_center - int(h_resize / 2))\n",
    "\n",
    "        x_max = min(w_original, x_min + w_resize)\n",
    "        y_max = min(h_original, y_min + h_resize)\n",
    "\n",
    "        return x_min, x_max, y_min, y_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0badb2",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# def check_ttnet():\n",
    "#     tasks = ['global', 'local', 'event']\n",
    "#     ttnet = TTNet(dropout_p=0.5, tasks=tasks, input_size=(320, 128), thresh_ball_pos_mask=0.01,\n",
    "#                   num_frames_sequence=9).cuda()\n",
    "#     resize_batch_input = torch.rand((1, 27, 128, 320)).cuda()\n",
    "#     org_ball_pos_xy = torch.rand((1, 2)).cuda()\n",
    "#     start = time.time()\n",
    "    \n",
    "#     pred_ball_global, pred_ball_local, pred_events, local_ball_pos_xy = ttnet(resize_batch_input, org_ball_pos_xy)\n",
    "# #     print(\"DEBUG Unbalaced loss: \", pred_ball_global.shape, pred_ball_local.shape, pred_events.shape, local_ball_pos_xy.shape)    \n",
    "        \n",
    "#     if pred_ball_global is not None:\n",
    "#         print('pred_ball_global: {}'.format(pred_ball_global.size()))\n",
    "#     if pred_ball_local is not None:\n",
    "#         print('pred_ball_local: {}'.format(pred_ball_local.size()))\n",
    "#     if pred_events is not None:\n",
    "#         print('pred_events: {}'.format(pred_events.size()))\n",
    "#     print('local_ball_pos_xy: {}'.format(local_ball_pos_xy.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ec0f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_ttnet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e94f6ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e516cfa0",
   "metadata": {},
   "source": [
    "#### Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05792241",
   "metadata": {
    "code_folding": [
     0,
     25
    ]
   },
   "outputs": [],
   "source": [
    "def create_target_ball(ball_position_xy, sigma, w, h, thresh_mask, device):\n",
    "    \"\"\"Create target for the ball detection stages\n",
    "\n",
    "    :param ball_position_xy: Position of the ball (x,y)\n",
    "    :param sigma: standard deviation (a hyperparameter)\n",
    "    :param w: width of the resize image\n",
    "    :param h: height of the resize image\n",
    "    :param thresh_mask: if values of 1D Gaussian < thresh_mask --> set to 0 to reduce computation\n",
    "    :param device: cuda() or cpu()\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    w, h = int(w), int(h)\n",
    "    target_ball_position = torch.zeros((w + h,), device=device)\n",
    "    # Only do the next step if the ball is existed\n",
    "    if (w > ball_position_xy[0] > 0) and (h > ball_position_xy[1] > 0):\n",
    "        # For x\n",
    "        x_pos = torch.arange(0, w, device=device)\n",
    "        target_ball_position[:w] = gaussian_1d(x_pos, ball_position_xy[0], sigma=sigma)\n",
    "        # For y\n",
    "        y_pos = torch.arange(0, h, device=device)\n",
    "        target_ball_position[w:] = gaussian_1d(y_pos, ball_position_xy[1], sigma=sigma)\n",
    "\n",
    "        target_ball_position[target_ball_position < thresh_mask] = 0.\n",
    "\n",
    "    return target_ball_position\n",
    "def gaussian_1d(pos, muy, sigma):\n",
    "    \"\"\"Create 1D Gaussian distribution based on ball position (muy), and std (sigma)\"\"\"\n",
    "    target = torch.exp(- (((pos - muy) / sigma) ** 2) / 2)\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6eccbd",
   "metadata": {
    "code_folding": [
     0,
     1,
     18,
     19
    ]
   },
   "outputs": [],
   "source": [
    "class Ball_Detection_Loss(nn.Module):\n",
    "    def __init__(self, w, h, epsilon=1e-9):\n",
    "        super(Ball_Detection_Loss, self).__init__()\n",
    "        self.w = w\n",
    "        self.h = h\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, pred_ball_position, target_ball_position):\n",
    "        x_pred = pred_ball_position[:, :self.w]\n",
    "        y_pred = pred_ball_position[:, self.w:]\n",
    "\n",
    "        x_target = target_ball_position[:, :self.w]\n",
    "        y_target = target_ball_position[:, self.w:]\n",
    "\n",
    "        loss_ball_x = - torch.mean(x_target * torch.log(x_pred + self.epsilon) + (1 - x_target) * torch.log(1 - x_pred + self.epsilon))\n",
    "        loss_ball_y = - torch.mean(y_target * torch.log(y_pred + self.epsilon) + (1 - y_target) * torch.log(1 - y_pred + self.epsilon))\n",
    "\n",
    "        return loss_ball_x + loss_ball_y\n",
    "class Events_Spotting_Loss(nn.Module):\n",
    "    def __init__(self, weights=(1, 3), num_events=2, epsilon=1e-9):\n",
    "        super(Events_Spotting_Loss, self).__init__()\n",
    "        self.weights = torch.tensor(weights).view(1, 2)\n",
    "        self.weights = self.weights / self.weights.sum()\n",
    "        self.num_events = num_events\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, pred_events, target_events):\n",
    "        self.weights = self.weights.cuda()\n",
    "        return - torch.mean(self.weights * (target_events * torch.log(pred_events + self.epsilon) + (1. - target_events) * torch.log(1 - pred_events + self.epsilon)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab434c7",
   "metadata": {
    "code_folding": [
     0,
     15,
     52,
     59
    ]
   },
   "outputs": [],
   "source": [
    "class Unbalance_Loss_Model(nn.Module):\n",
    "    def __init__(self, model, tasks_loss_weight, weights_events, input_size, sigma, thresh_ball_pos_mask, device):\n",
    "        super(Unbalance_Loss_Model, self).__init__()\n",
    "        self.model = model\n",
    "        self.tasks_loss_weight = torch.tensor(tasks_loss_weight)\n",
    "        self.tasks_loss_weight = self.tasks_loss_weight / self.tasks_loss_weight.sum()\n",
    "        self.num_events = len(tasks_loss_weight)\n",
    "        self.w = input_size[0]\n",
    "        self.h = input_size[1]\n",
    "        self.sigma = sigma\n",
    "        self.thresh_ball_pos_mask = thresh_ball_pos_mask\n",
    "        self.device = device\n",
    "        self.ball_loss_criterion = Ball_Detection_Loss(self.w, self.h)\n",
    "        self.event_loss_criterion = Events_Spotting_Loss(weights=weights_events, num_events=self.num_events)\n",
    "\n",
    "    def forward(self, resize_batch_input, org_ball_pos_xy, global_ball_pos_xy, target_events):\n",
    "        pred_ball_global, pred_ball_local, pred_events, local_ball_pos_xy = self.model(resize_batch_input, org_ball_pos_xy)\n",
    "        # Create target for events spotting and ball position (local and global)\n",
    "        batch_size = pred_ball_global.size(0)\n",
    "        target_ball_global = torch.zeros_like(pred_ball_global)\n",
    "        task_idx = 0\n",
    "        for sample_idx in range(batch_size):\n",
    "            target_ball_global[sample_idx] = create_target_ball(global_ball_pos_xy[sample_idx], sigma=self.sigma,\n",
    "                                                                w=self.w, h=self.h,\n",
    "                                                                thresh_mask=self.thresh_ball_pos_mask,\n",
    "                                                                device=self.device)\n",
    "        global_ball_loss = self.ball_loss_criterion(pred_ball_global, target_ball_global)\n",
    "        total_loss = global_ball_loss * self.tasks_loss_weight[task_idx]\n",
    "\n",
    "        if pred_ball_local is not None:\n",
    "            task_idx += 1\n",
    "            target_ball_local = torch.zeros_like(pred_ball_local)\n",
    "            for sample_idx in range(batch_size):\n",
    "                target_ball_local[sample_idx] = create_target_ball(local_ball_pos_xy[sample_idx], sigma=self.sigma,\n",
    "                                                                   w=self.w, h=self.h,\n",
    "                                                                   thresh_mask=self.thresh_ball_pos_mask,\n",
    "                                                                   device=self.device)\n",
    "            local_ball_loss = self.ball_loss_criterion(pred_ball_local, target_ball_local)\n",
    "            total_loss += local_ball_loss * self.tasks_loss_weight[task_idx]\n",
    "\n",
    "        if pred_events is not None:\n",
    "            task_idx += 1\n",
    "            target_events = target_events.to(device=self.device)\n",
    "            event_loss = self.event_loss_criterion(pred_events, target_events)\n",
    "            total_loss += event_loss * self.tasks_loss_weight[task_idx]\n",
    "\n",
    "\n",
    "        return pred_ball_global, pred_ball_local, pred_events, local_ball_pos_xy, total_loss, None\n",
    "\n",
    "    def run_demo(self, resize_batch_input):\n",
    "        pred_ball_global, pred_ball_local, pred_events = self.model.run_demo(resize_batch_input)\n",
    "        return pred_ball_global, pred_ball_local, pred_events\n",
    "class Multi_Task_Learning_Model(nn.Module):\n",
    "    \"\"\"\n",
    "    Original paper: \"Multi-task learning using uncertainty to weigh losses for scene geometry and semantics\" - CVPR 2018\n",
    "    url: https://arxiv.org/pdf/1705.07115.pdf\n",
    "    refer code: https://github.com/Hui-Li/multi-task-learning-example-PyTorch\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, tasks, num_events, weights_events, input_size, sigma, thresh_ball_pos_mask, device):\n",
    "        super(Multi_Task_Learning_Model, self).__init__()\n",
    "        self.model = model\n",
    "        self.tasks = tasks\n",
    "        self.num_tasks = len(tasks)\n",
    "        self.log_vars = nn.Parameter(torch.zeros((self.num_tasks)))\n",
    "        self.w = input_size[0]\n",
    "        self.h = input_size[1]\n",
    "        self.sigma = sigma\n",
    "        self.thresh_ball_pos_mask = thresh_ball_pos_mask\n",
    "        self.device = device\n",
    "        self.ball_loss_criterion = Ball_Detection_Loss(self.w, self.h)\n",
    "        self.event_loss_criterion = Events_Spotting_Loss(weights=weights_events, num_events=num_events)\n",
    "        print(\"Learnable Multitask model\")\n",
    "        # self.seg_loss_criterion = Segmentation_Loss()\n",
    "\n",
    "    def forward(self, resize_batch_input, org_ball_pos_xy, global_ball_pos_xy, target_events):#, target_seg):\n",
    "        log_vars_idx = 0\n",
    "        # pred_ball_global, pred_ball_local, pred_events, pred_seg, local_ball_pos_xy = self.model(resize_batch_input,org_ball_pos_xy)\n",
    "        pred_ball_global, pred_ball_local, pred_events, local_ball_pos_xy = self.model(resize_batch_input,org_ball_pos_xy)\n",
    "        \n",
    "        # Create target for events spotting and ball position (local and global)\n",
    "        batch_size = pred_ball_global.size(0)\n",
    "        target_ball_global = torch.zeros_like(pred_ball_global)\n",
    "        for sample_idx in range(batch_size):\n",
    "            target_ball_global[sample_idx] = create_target_ball(global_ball_pos_xy[sample_idx], sigma=self.sigma,\n",
    "                                                                w=self.w, h=self.h,\n",
    "                                                                thresh_mask=self.thresh_ball_pos_mask,\n",
    "                                                                device=self.device)\n",
    "        global_ball_loss = self.ball_loss_criterion(pred_ball_global, target_ball_global)\n",
    "        total_loss = global_ball_loss / (torch.exp(2 * self.log_vars[log_vars_idx])) + self.log_vars[log_vars_idx]\n",
    "\n",
    "        if pred_ball_local is not None:\n",
    "            log_vars_idx += 1\n",
    "            target_ball_local = torch.zeros_like(pred_ball_local)\n",
    "            for sample_idx in range(batch_size):\n",
    "                target_ball_local[sample_idx] = create_target_ball(local_ball_pos_xy[sample_idx], sigma=self.sigma,\n",
    "                                                                   w=self.w, h=self.h,\n",
    "                                                                   thresh_mask=self.thresh_ball_pos_mask,\n",
    "                                                                   device=self.device)\n",
    "            local_ball_loss = self.ball_loss_criterion(pred_ball_local, target_ball_local)\n",
    "            total_loss += local_ball_loss / (torch.exp(2 * self.log_vars[log_vars_idx])) + self.log_vars[log_vars_idx]\n",
    "\n",
    "        if pred_events is not None:\n",
    "            log_vars_idx += 1\n",
    "            target_events = target_events.to(device=self.device)\n",
    "            event_loss = self.event_loss_criterion(pred_events, target_events)\n",
    "            total_loss += event_loss / (2 * torch.exp(self.log_vars[log_vars_idx])) + self.log_vars[log_vars_idx]\n",
    "\n",
    "        # if pred_seg is not None:\n",
    "        #     log_vars_idx += 1\n",
    "        #     seg_loss = self.seg_loss_criterion(pred_seg, target_seg)\n",
    "        #     total_loss += seg_loss / (2 * torch.exp(self.log_vars[log_vars_idx])) + self.log_vars[log_vars_idx]\n",
    "\n",
    "        # Final weights: [math.exp(log_var) ** 0.5 for log_var in log_vars]\n",
    "\n",
    "        #return pred_ball_global, pred_ball_local, pred_events, pred_seg, local_ball_pos_xy, total_loss, self.log_vars.data.tolist()\n",
    "        return pred_ball_global, pred_ball_local, pred_events, local_ball_pos_xy, total_loss, self.log_vars.data.tolist()\n",
    "\n",
    "    def run_demo(self, resize_batch_input):\n",
    "        # pred_ball_global, pred_ball_local, pred_events, pred_seg = self.model.run_demo(resize_batch_input)\n",
    "        # return pred_ball_global, pred_ball_local, pred_events, pred_seg\n",
    "        pred_ball_global, pred_ball_local, pred_events = self.model.run_demo(resize_batch_input)\n",
    "        return pred_ball_global, pred_ball_local, pred_events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5af973f",
   "metadata": {},
   "source": [
    "##### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9525c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configs.thresh_ball_pos_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b7092b",
   "metadata": {
    "code_folding": [
     1,
     23,
     32,
     43,
     55,
     89,
     103
    ]
   },
   "outputs": [],
   "source": [
    "## Model defination\n",
    "def create_model(configs):\n",
    "    \"\"\"Create model based on architecture name\"\"\"\n",
    "    if configs.arch == 'ttnet':\n",
    "        ttnet_model = TTNet(dropout_p=configs.dropout_p, tasks=configs.tasks, input_size=configs.input_size,\n",
    "                            thresh_ball_pos_mask=configs.thresh_ball_pos_mask,\n",
    "                            num_frames_sequence=configs.num_frames_sequence)\n",
    "    else:\n",
    "        assert False, 'Undefined model backbone'\n",
    "\n",
    "    if configs.multitask_learning == True:\n",
    "        model = Multi_Task_Learning_Model(ttnet_model, tasks=configs.tasks, num_events=configs.num_events,\n",
    "                                          weights_events=configs.events_weights_loss,\n",
    "                                          input_size=configs.input_size, sigma=configs.sigma,\n",
    "                                          thresh_ball_pos_mask=configs.thresh_ball_pos_mask, device=configs.device)\n",
    "    else:\n",
    "        model = Unbalance_Loss_Model(ttnet_model, tasks_loss_weight=configs.tasks_loss_weight,\n",
    "                                     weights_events=configs.events_weights_loss, input_size=configs.input_size,\n",
    "                                     sigma=configs.sigma, thresh_ball_pos_mask=configs.thresh_ball_pos_mask,\n",
    "                                     device=configs.device)\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_num_parameters(model):\n",
    "    \"\"\"Count number of trained parameters of the model\"\"\"\n",
    "    if hasattr(model, 'module'):\n",
    "        num_parameters = sum(p.numel() for p in model.module.parameters() if p.requires_grad)\n",
    "    else:\n",
    "        num_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    return num_parameters\n",
    "\n",
    "def freeze_model(model, freeze_modules_list):\n",
    "    \"\"\"Freeze modules of the model based on the configuration\"\"\"\n",
    "    for layer_name, p in model.named_parameters():\n",
    "        p.requires_grad = True\n",
    "        for freeze_module in freeze_modules_list:\n",
    "            if freeze_module in layer_name:\n",
    "                p.requires_grad = False\n",
    "                break\n",
    "\n",
    "    return model\n",
    "\n",
    "def load_weights_local_stage(pretrained_dict):\n",
    "    \"\"\"Overwrite the weights of the global stage to the local stage\"\"\"\n",
    "    local_weights_dict = {}\n",
    "    for layer_name, v in pretrained_dict.items():\n",
    "        if 'ball_global_stage' in layer_name:\n",
    "            layer_name_parts = layer_name.split('.')\n",
    "            layer_name_parts[1] = 'ball_local_stage'\n",
    "            local_name = '.'.join(layer_name_parts)\n",
    "            local_weights_dict[local_name] = v\n",
    "\n",
    "    return {**pretrained_dict, **local_weights_dict}\n",
    "\n",
    "def load_pretrained_model(model, pretrained_path, gpu_idx, overwrite_global_2_local):\n",
    "    \"\"\"Load weights from the pretrained model\"\"\"\n",
    "    assert os.path.isfile(pretrained_path), \"=> no checkpoint found at '{}'\".format(pretrained_path)\n",
    "    if gpu_idx is None:\n",
    "        checkpoint = torch.load(pretrained_path, map_location='cpu')\n",
    "    else:\n",
    "        # Map model to be loaded to specified single gpu.\n",
    "        loc = 'cuda:{}'.format(gpu_idx)\n",
    "        checkpoint = torch.load(pretrained_path, map_location=loc)\n",
    "    pretrained_dict = checkpoint['state_dict']\n",
    "    if hasattr(model, 'module'):\n",
    "        model_state_dict = model.module.state_dict()\n",
    "        # 1. filter out unnecessary keys\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_state_dict}\n",
    "        # Load global to local stage\n",
    "        if overwrite_global_2_local:\n",
    "            pretrained_dict = load_weights_local_stage(pretrained_dict)\n",
    "        # 2. overwrite entries in the existing state dict\n",
    "        model_state_dict.update(pretrained_dict)\n",
    "        # 3. load the new state dict\n",
    "        model.module.load_state_dict(model_state_dict)\n",
    "    else:\n",
    "        model_state_dict = model.state_dict()\n",
    "        # 1. filter out unnecessary keys\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_state_dict}\n",
    "        # Load global to local stage\n",
    "        if overwrite_global_2_local:\n",
    "            pretrained_dict = load_weights_local_stage(pretrained_dict)\n",
    "        # 2. overwrite entries in the existing state dict\n",
    "        model_state_dict.update(pretrained_dict)\n",
    "        # 3. load the new state dict\n",
    "        model.load_state_dict(model_state_dict)\n",
    "    return model\n",
    "\n",
    "def resume_model(resume_path, arch, gpu_idx):\n",
    "    \"\"\"Resume training model from the previous trained checkpoint\"\"\"\n",
    "    assert os.path.isfile(resume_path), \"=> no checkpoint found at '{}'\".format(resume_path)\n",
    "    if gpu_idx is None:\n",
    "        checkpoint = torch.load(resume_path, map_location='cpu')\n",
    "    else:\n",
    "        # Map model to be loaded to specified single gpu.\n",
    "        loc = 'cuda:{}'.format(gpu_idx)\n",
    "        checkpoint = torch.load(resume_path, map_location=loc)\n",
    "    assert arch == checkpoint['configs'].arch, \"Load the different arch...\"\n",
    "    print(\"=> loaded checkpoint '{}' (epoch {})\".format(resume_path, checkpoint['epoch']))\n",
    "\n",
    "    return checkpoint\n",
    "\n",
    "def make_data_parallel(model, configs):\n",
    "    if configs.distributed:\n",
    "        # For multiprocessing distributed, DistributedDataParallel constructor\n",
    "        # should always set the single device scope, otherwise,\n",
    "        # DistributedDataParallel will use all available devices.\n",
    "        if configs.gpu_idx is not None:\n",
    "            torch.cuda.set_device(configs.gpu_idx)\n",
    "            model.cuda(configs.gpu_idx)\n",
    "            # When using a single GPU per process and per\n",
    "            # DistributedDataParallel, we need to divide the batch size\n",
    "            # ourselves based on the total number of GPUs we have\n",
    "            configs.batch_size = int(configs.batch_size / configs.ngpus_per_node)\n",
    "            configs.num_workers = int((configs.num_workers + configs.ngpus_per_node - 1) / configs.ngpus_per_node)\n",
    "            model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[configs.gpu_idx],\n",
    "                                                              find_unused_parameters=True)\n",
    "        else:\n",
    "            model.cuda()\n",
    "            # DistributedDataParallel will divide and allocate batch_size to all\n",
    "            # available GPUs if device_ids are not set\n",
    "            model = torch.nn.parallel.DistributedDataParallel(model)\n",
    "    elif configs.gpu_idx is not None:\n",
    "        torch.cuda.set_device(configs.gpu_idx)\n",
    "        model = model.cuda(configs.gpu_idx)\n",
    "    else:\n",
    "        # DataParallel will divide and allocate batch_size to all available GPUs\n",
    "        model = torch.nn.DataParallel(model).cuda()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d575a372",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def check_model(configs):\n",
    "    model = create_model(configs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e92454",
   "metadata": {
    "code_folding": [
     0,
     28
    ]
   },
   "outputs": [],
   "source": [
    "def make_data_parallel(model, configs):\n",
    "#     if configs.distributed:\n",
    "#         # For multiprocessing distributed, DistributedDataParallel constructor\n",
    "#         # should always set the single device scope, otherwise,\n",
    "#         # DistributedDataParallel will use all available devices.\n",
    "#         if configs.gpu_idx is not None:\n",
    "#             torch.cuda.set_device(configs.gpu_idx)\n",
    "#             model.cuda(configs.gpu_idx)\n",
    "#             # When using a single GPU per process and per\n",
    "#             # DistributedDataParallel, we need to divide the batch size\n",
    "#             # ourselves based on the total number of GPUs we have\n",
    "#             configs.batch_size = int(configs.batch_size / configs.ngpus_per_node)\n",
    "#             configs.num_workers = int((configs.num_workers + configs.ngpus_per_node - 1) / configs.ngpus_per_node)\n",
    "#             model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[configs.gpu_idx],\n",
    "#                                                               find_unused_parameters=True)\n",
    "#         else:\n",
    "#             model.cuda()\n",
    "#             # DistributedDataParallel will divide and allocate batch_size to all\n",
    "#             # available GPUs if device_ids are not set\n",
    "#             model = torch.nn.parallel.DistributedDataParallel(model)\n",
    "    if configs.gpu_idx is not None:\n",
    "        torch.cuda.set_device(configs.gpu_idx)\n",
    "        model = model.cuda(configs.gpu_idx)\n",
    "    else:\n",
    "        # DataParallel will divide and allocate batch_size to all available GPUs\n",
    "        model = torch.nn.DataParallel(model).cuda()\n",
    "\n",
    "    return model\n",
    "def freeze_model(model, freeze_modules_list):\n",
    "    \"\"\"Freeze modules of the model based on the configuration\"\"\"\n",
    "    for layer_name, p in model.named_parameters():\n",
    "        p.requires_grad = True\n",
    "        for freeze_module in freeze_modules_list:\n",
    "            if freeze_module in layer_name:\n",
    "                p.requires_grad = False\n",
    "                break\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd42a555",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def evaluate_one_epoch(val_loader, model, epoch, configs, logger=None):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    data_time = AverageMeter('Data', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "\n",
    "    progress = ProgressMeter(len(val_loader), [batch_time, data_time, losses],\n",
    "                             prefix=\"Evaluate - Epoch: [{}/{}]\".format(epoch, configs.num_epochs))\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        start_time = time.time()\n",
    "        for batch_idx, (resized_imgs, org_ball_pos_xy, global_ball_pos_xy, target_events) in enumerate(\n",
    "                tqdm(val_loader)):\n",
    "            data_time.update(time.time() - start_time)\n",
    "            batch_size = resized_imgs.size(0)\n",
    "#             target_seg = target_seg.to(configs.device, non_blocking=True)\n",
    "            resized_imgs = resized_imgs.to(configs.device, non_blocking=True).float()\n",
    "            pred_ball_global, pred_ball_local, pred_events, local_ball_pos_xy, total_loss, _ = model(\n",
    "                resized_imgs, org_ball_pos_xy, global_ball_pos_xy, target_events)\n",
    "\n",
    "            # For torch.nn.DataParallel case\n",
    "            if (not configs.distributed) and (configs.gpu_idx is None):\n",
    "                total_loss = torch.mean(total_loss)\n",
    "\n",
    "            if configs.distributed:\n",
    "                reduced_loss = reduce_tensor(total_loss.data, configs.world_size)\n",
    "            else:\n",
    "                reduced_loss = total_loss.data\n",
    "            losses.update(to_python_float(reduced_loss), batch_size)\n",
    "            # measure elapsed time\n",
    "            torch.cuda.synchronize()\n",
    "            batch_time.update(time.time() - start_time)\n",
    "\n",
    "            # Log message\n",
    "            if logger is not None:\n",
    "                if ((batch_idx + 1) % configs.print_freq) == 0:\n",
    "                    logger.info(progress.get_message(batch_idx))\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2a4e67",
   "metadata": {},
   "source": [
    "##### Training Debugger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a443b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(configs)\n",
    "model = make_data_parallel(model, configs)\n",
    "model = freeze_model(model, configs.freeze_modules_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec2139b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total Parameters: \", get_num_parameters(model) / 1000000 , \"M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e382de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer       = create_optimizer(configs, model)\n",
    "lr_scheduler    = create_lr_scheduler(optimizer, configs)\n",
    "best_val_loss   = np.inf\n",
    "earlystop_count = 0\n",
    "is_best         = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90996898",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(configs.pretrained_path, configs.resume_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285ba2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs.resume_path = \"../../checkpoints/ttnet/ttnet_1st_phase_epoch_6.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0be488",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "if configs.pretrained_path is not None:\n",
    "    model = load_pretrained_model(model, configs.pretrained_path, gpu_idx, configs.overwrite_global_2_local)\n",
    "    if logger is not None:\n",
    "        logger.info('loaded pretrained model at {}'.format(configs.pretrained_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cf29b4",
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# optionally resume from a checkpoint\n",
    "if configs.resume_path is not None:\n",
    "    checkpoint = resume_model(configs.resume_path, configs.arch, configs.gpu_idx)\n",
    "    if hasattr(model, 'module'):\n",
    "        model.module.load_state_dict(checkpoint['state_dict'])\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "    best_val_loss = checkpoint['best_val_loss']\n",
    "    earlystop_count = checkpoint['earlystop_count']\n",
    "    configs.start_epoch = checkpoint['epoch'] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b445ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "if logger is not None:\n",
    "    logger.info(\">>> Loading dataset & getting dataloader...\")\n",
    "train_loader, val_loader, train_sampler = create_train_val_dataloader(configs)\n",
    "test_loader = create_test_dataloader(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb13dcd",
   "metadata": {
    "code_folding": [
     0,
     5
    ]
   },
   "outputs": [],
   "source": [
    "if logger is not None:\n",
    "    logger.info('number of batches in train set: {}'.format(len(train_loader)))\n",
    "    if val_loader is not None:\n",
    "        logger.info('number of batches in val set: {}'.format(len(val_loader)))\n",
    "    logger.info('number of batches in test set: {}'.format(len(test_loader)))\n",
    "if configs.evaluate:\n",
    "    assert val_loader is not None, \"The validation should not be None\"\n",
    "    val_loss = evaluate_one_epoch(val_loader, model, configs.start_epoch - 1, configs, logger)\n",
    "    print('Evaluate, val_loss: {}'.format(val_loss))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d16fce",
   "metadata": {},
   "source": [
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a75368",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(train_loader, model, optimizer, epoch, configs, logger):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    data_time = AverageMeter('Data', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "\n",
    "    progress = ProgressMeter(len(train_loader), [batch_time, data_time, losses],\n",
    "                             prefix=\"Train - Epoch: [{}/{}]\".format(epoch, configs.num_epochs))\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    for batch_idx, (resized_imgs, org_ball_pos_xy, global_ball_pos_xy, target_events) in enumerate(\n",
    "            tqdm(train_loader)):\n",
    "        data_time.update(time.time() - start_time)\n",
    "        batch_size = resized_imgs.size(0)\n",
    "        resized_imgs = resized_imgs.to(configs.device, non_blocking=True).float()\n",
    "        pred_ball_global, pred_ball_local, pred_events, local_ball_pos_xy, total_loss, _ = model(resized_imgs, org_ball_pos_xy, global_ball_pos_xy, target_events)\n",
    "        # For torch.nn.DataParallel case\n",
    "        if (not configs.distributed) and (configs.gpu_idx is None):\n",
    "            total_loss = torch.mean(total_loss)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # compute gradient and perform backpropagation\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if configs.distributed:\n",
    "            reduced_loss = reduce_tensor(total_loss.data, configs.world_size)\n",
    "        else:\n",
    "            reduced_loss = total_loss.data\n",
    "        losses.update(to_python_float(reduced_loss), batch_size)\n",
    "        # measure elapsed time\n",
    "        torch.cuda.synchronize()\n",
    "        batch_time.update(time.time() - start_time)\n",
    "\n",
    "        # Log message\n",
    "        if logger is not None:\n",
    "            if ((batch_idx + 1) % configs.print_freq) == 0:\n",
    "                logger.info(progress.get_message(batch_idx))\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f57bbf",
   "metadata": {
    "code_folding": [
     0,
     4
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(configs.start_epoch, configs.num_epochs + 1):\n",
    "#     Get the current learning rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        lr = param_group['lr']\n",
    "    if logger is not None:\n",
    "        logger.info('{}'.format('*-' * 40))\n",
    "        logger.info('{} {}/{} {}'.format('=' * 35, epoch, configs.num_epochs, '=' * 35))\n",
    "        logger.info('{}'.format('*-' * 40))\n",
    "        logger.info('>>> Epoch: [{}/{}] learning rate: {:.2e}'.format(epoch, configs.num_epochs, lr))\n",
    "\n",
    "    if configs.distributed:\n",
    "        train_sampler.set_epoch(epoch)\n",
    "    # train for one epoch\n",
    "    train_loss = train_one_epoch(train_loader, model, optimizer, epoch, configs, logger)\n",
    "    loss_dict = {'train': train_loss}\n",
    "    if not configs.no_val:\n",
    "        val_loss = evaluate_one_epoch(val_loader, model, epoch, configs, logger)\n",
    "        is_best = val_loss <= best_val_loss\n",
    "        best_val_loss = min(val_loss, best_val_loss)\n",
    "        loss_dict['val'] = val_loss\n",
    "\n",
    "    if not configs.no_test:\n",
    "        test_loss = evaluate_one_epoch(test_loader, model, epoch, configs, logger)\n",
    "        loss_dict['test'] = test_loss\n",
    "#     Write tensorboard\n",
    "#     Save checkpoint\n",
    "    if (is_best or ((epoch % configs.checkpoint_freq) == 0)):\n",
    "        saved_state = get_saved_state(model, optimizer, lr_scheduler, epoch, configs, best_val_loss,\n",
    "                                      earlystop_count)\n",
    "        save_checkpoint(configs.checkpoints_dir, configs.saved_fn, saved_state, is_best, epoch)\n",
    "    # Check early stop training\n",
    "    if configs.earlystop_patience is not None:\n",
    "        earlystop_count = 0 if is_best else (earlystop_count + 1)\n",
    "        print_string = ' |||\\t earlystop_count: {}'.format(earlystop_count)\n",
    "        if configs.earlystop_patience <= earlystop_count:\n",
    "            print_string += '\\n\\t--- Early stopping!!!'\n",
    "            break\n",
    "        else:\n",
    "            print_string += '\\n\\t--- Continue training..., earlystop_count: {}'.format(earlystop_count)\n",
    "        if logger is not None:\n",
    "            logger.info(print_string)\n",
    "    # Adjust learning rate\n",
    "    if configs.lr_type == 'plateau':\n",
    "        assert (not configs.no_val), \"Only use plateau when having validation set\"\n",
    "        lr_scheduler.step(val_loss)\n",
    "    else:\n",
    "        lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ade0a8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a17be1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335b3c86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00247a18",
   "metadata": {},
   "source": [
    "### Phase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae46c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = parse_configs()\n",
    "configs.distributed = False\n",
    "# configs.multitask_learning = True\n",
    "\n",
    "#Phase 2\n",
    "configs.saved_fn  = 'ttnet_2nd_phase' \n",
    "configs.no-val = True  \n",
    "configs.lr = 0.001 \n",
    "configs.lr_type = 'step_lr' \n",
    "configs.lr_step_size =  10 \n",
    "configs.lr_factor = 0.1 \n",
    "configs.gpu_idx = 0 \n",
    "configs.global_weight = 0. \n",
    "configs.event_weight = 2. \n",
    "configs.local_weight = 1. \n",
    "configs.pretrained_path = \"../checkpoints/ttnet_1st_phase/ttnet_1st_phase_epoch_30.pth\"\n",
    "configs.overwrite_global_2_local  = True\n",
    "configs.freeze_global  = True\n",
    "configs.smooth-labelling = True\n",
    "configs.sigma =  1.0\n",
    "configs.print_freq =  1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689cc5da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f2af86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7801d14e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
